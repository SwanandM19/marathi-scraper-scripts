# import asyncio
# import json
# from datetime import datetime
# from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode
# from bs4 import BeautifulSoup
# from openai import OpenAI
# import re
# import os

# # Initialize Perplexity client with environment variable
# perplexity_client = OpenAI(
#     api_key=os.environ.get("PERPLEXITY_API_KEY"),  # Read from GitHub Secrets
#     base_url="https://api.perplexity.ai"
# )

# # Track token usage (CORRECTED: $1 per 1M tokens)
# total_tokens_used = 0
# total_cost = 0.0

# async def scrape_marathi_news_final():
#     """
#     Scraper that collects articles from all three sites
#     """
    
#     news_sites = [
#         {
#             "name": "TV9 Marathi",
#             "url": "https://www.tv9marathi.com/latest-news",
#             "article_selector": "article, div.story-card, div.news-item",
#             "link_pattern": "tv9marathi.com"
#         },
#         {
#             "name": "ABP Majha",
#             "url": "https://marathi.abplive.com/news",
#             "article_selector": "article, div.story-box, div.news-card",
#             "link_pattern": "abplive.com"
#         },
#         {
#             "name": "Lokmat",
#             "url": "https://www.lokmat.com/latestnews/",
#             "article_selector": "article, div.story-card, div.card-body",
#             "link_pattern": "lokmat.com"
#         }
#     ]
    
#     all_news = []
    
#     async with AsyncWebCrawler(verbose=True) as crawler:
        
#         for site in news_sites:
#             print(f"\nüîç Scraping {site['name']}...")
            
#             try:
#                 # Step 1: Fetch homepage with JavaScript rendering
#                 config = CrawlerRunConfig(
#                     cache_mode=CacheMode.BYPASS,
#                     wait_for="body",
#                     word_count_threshold=10,
#                     page_timeout=30000,
#                     js_code="""
#                     // Wait for content to load
#                     await new Promise(r => setTimeout(r, 2000));
#                     """
#                 )
                
#                 result = await crawler.arun(site['url'], config=config)
                
#                 if result.success:
#                     soup = BeautifulSoup(result.html, 'html.parser')
                    
#                     raw_articles = []
                    
#                     # Strategy: Find all links with Marathi text
#                     all_links = soup.find_all('a', href=True)
                    
#                     for link_tag in all_links:
#                         href = link_tag.get('href', '')
#                         title = link_tag.get_text(strip=True)
                        
#                         # Filter valid news links
#                         if (len(title) > 15 and len(title) < 300 and
#                             site['link_pattern'] in href and
#                             not any(x in href for x in [
#                                 'javascript:', 'mailto:', '#', 
#                                 '/category/', '/tag/', '/author/',
#                                 'facebook.com', 'twitter.com', 'instagram.com',
#                                 'youtube.com', 'whatsapp.com', '/myaccount/',
#                                 '/install_app', '/advertisement', '/epaper',
#                                 'web-stories', 'photo-gallery', '/videos/',
#                                 '/sakhi/', '/astro/', '/bhakti/', '/games/',
#                                 '/jokes/', '/terms-and-conditions', '/utility-news',
#                                 '/spiritual-adhyatmik', '/rashi-bhavishya', 
#                                 '/topic/', '/elections/', '/career/'
#                             ])):
                            
#                             # Make absolute URL
#                             if href.startswith('/'):
#                                 base_url = site['url'].split('/')[0] + '//' + site['url'].split('/')[2]
#                                 href = base_url + href
                            
#                             if href.startswith('http'):
#                                 raw_articles.append({
#                                     'title': title,
#                                     'link': href
#                                 })
                    
#                     # Remove duplicates by link
#                     seen_links = set()
#                     unique_articles = []
#                     for article in raw_articles:
#                         if article['link'] not in seen_links:
#                             unique_articles.append(article)
#                             seen_links.add(article['link'])
                    
#                     print(f"üìã Found {len(unique_articles)} unique articles from {site['name']}")
                    
#                     if len(unique_articles) > 0:
#                         # Get top 12 articles per site
#                         print(f"üìÑ Fetching detailed content from top {min(12, len(unique_articles))} articles...")
                        
#                         articles_with_content = []
#                         for article in unique_articles[:12]:  # Top 12 per site
#                             try:
#                                 article_result = await crawler.arun(
#                                     article['link'],
#                                     config=CrawlerRunConfig(
#                                         cache_mode=CacheMode.BYPASS,
#                                         word_count_threshold=50,
#                                         page_timeout=15000
#                                     )
#                                 )
                                
#                                 if article_result.success and len(article_result.markdown) > 100:
#                                     articles_with_content.append({
#                                         'title': article['title'],
#                                         'link': article['link'],
#                                         'content': article_result.markdown[:2500]
#                                     })
#                                     print(f"   ‚úì {article['title'][:60]}...")
                                    
#                             except Exception as e:
#                                 continue
                        
#                         print(f"‚úÖ Fetched content for {len(articles_with_content)} articles")
                        
#                         # Step 3: AI analysis
#                         if articles_with_content:
#                             filtered_news = await smart_analyze_with_detailed_summary(
#                                 articles_with_content, 
#                                 site['name']
#                             )
#                             all_news.extend(filtered_news)
#                             print(f"‚úÖ Extracted {len(filtered_news)} important articles with detailed summaries")
#                     else:
#                         print(f"‚ö†Ô∏è No articles found from {site['name']}")
                
#                 else:
#                     print(f"‚ùå Failed to fetch {site['name']}: {result.error_message}")
                    
#             except Exception as e:
#                 print(f"‚ùå Error scraping {site['name']}: {e}")
#                 import traceback
#                 traceback.print_exc()
    
#     return all_news


# async def smart_analyze_with_detailed_summary(articles, source_name):
#     """
#     AI analysis with CORRECTED token tracking ($1 per 1M tokens)
#     """
#     global total_tokens_used, total_cost
    
#     print(f"\nüß† Using AI for detailed analysis of {source_name} articles...")
    
#     all_filtered = []
    
#     # Process in batches of 5
#     for i in range(0, len(articles), 5):
#         batch = articles[i:i+5]
        
#         articles_text = ""
#         for idx, article in enumerate(batch, i+1):
#             articles_text += f"""
# ‡§¨‡§æ‡§§‡§Æ‡•Ä #{idx}:
# ‡§∂‡•Ä‡§∞‡•ç‡§∑‡§ï: {article['title']}
# Link: {article['link']}
# Content: {article['content'][:1200]}

# ---
# """
        
#         prompt = f"""
# ‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä ‡§è‡§ï ‡§§‡§ú‡•ç‡§û ‡§Æ‡§∞‡§æ‡§†‡•Ä ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§ï ‡§Ü‡§π‡§æ‡§§. ‡§ñ‡§æ‡§≤‡•Ä‡§≤ ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ‡§Ç‡§ö‡•á ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£ ‡§ï‡§∞‡§æ.

# **‡§´‡§ï‡•ç‡§§ ‡§π‡•á ‡§™‡•ç‡§∞‡§ï‡§æ‡§∞ ‡§®‡§ø‡§µ‡§°‡§æ:**
# 1. ‡§ó‡•Å‡§®‡•ç‡§π‡•á‡§ó‡§æ‡§∞‡•Ä ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ (Crime) - ‡§π‡§§‡•ç‡§Ø‡§æ, ‡§¶‡§∞‡•ã‡§°‡§æ, ‡§Ö‡§™‡§ò‡§æ‡§§, ‡§Ö‡§ü‡§ï, ‡§≤‡§æ‡§ö
# 2. ‡§∞‡§æ‡§ú‡§ï‡•Ä‡§Ø ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ (Political) - ‡§®‡§ø‡§µ‡§°‡§£‡•Å‡§ï‡§æ, ‡§∏‡§∞‡§ï‡§æ‡§∞, ‡§Æ‡§π‡§æ‡§™‡§æ‡§≤‡§ø‡§ï‡§æ, ‡§∞‡§æ‡§ú‡§ï‡•Ä‡§Ø ‡§ò‡§°‡§æ‡§Æ‡•ã‡§°‡•Ä
# 3. ‡§Æ‡§π‡§§‡•ç‡§§‡•ç‡§µ‡§æ‡§ö‡•ç‡§Ø‡§æ ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ (Important General) - ‡§∂‡§æ‡§∏‡§ï‡•Ä‡§Ø ‡§®‡§ø‡§∞‡•ç‡§£‡§Ø, ‡§∏‡§æ‡§Æ‡§æ‡§ú‡§ø‡§ï ‡§Æ‡•Å‡§¶‡•ç‡§¶‡•á

# **‡§ü‡§æ‡§≥‡§æ‡§µ‡•á:** ‡§Æ‡§®‡•ã‡§∞‡§Ç‡§ú‡§® gossip, ‡§ú‡•ç‡§Ø‡•ã‡§§‡§ø‡§∑, ‡§´‡•Ö‡§∂‡§®, lifestyle, ‡§ñ‡•á‡§≥‡§æ‡§ö‡•Ä ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§¨‡§æ‡§§‡§Æ‡•Ä, job posts, ‡§ß‡§æ‡§∞‡•ç‡§Æ‡§ø‡§ï ‡§ï‡§•‡§æ, Bigg Boss, ‡§¨‡•â‡§≤‡•Ä‡§µ‡•Ç‡§° gossip

# **JSON format (‡§´‡§ï‡•ç‡§§ array ‡§™‡§∞‡§§ ‡§ï‡§∞‡§æ, ‡§á‡§§‡§∞ ‡§ï‡§æ‡§π‡•Ä ‡§®‡§æ‡§π‡•Ä):**
# [
#   {{
#     "title": "‡§Æ‡•Ç‡§≥ ‡§∂‡•Ä‡§∞‡•ç‡§∑‡§ï",
#     "category": "crime/politics/general",
#     "detailed_summary": "‡§∏‡§Ç‡§™‡•Ç‡§∞‡•ç‡§£ ‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§ ‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂ 150-250 ‡§∂‡§¨‡•ç‡§¶‡§æ‡§Ç‡§§ ‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§. ‡§ï‡§æ‡§Ø ‡§ò‡§°‡§≤‡§Ç? ‡§ï‡•Å‡§†‡•á? ‡§ï‡§ß‡•Ä? ‡§ï‡•ã‡§£ ‡§Ü‡§π‡•á‡§§? ‡§ï‡•ã‡§£‡§§‡•Ä ‡§ï‡§æ‡§∞‡§µ‡§æ‡§à? ‡§ï‡§æ‡§Ø ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ? ‡§á‡§§‡§∞ ‡§§‡§™‡§∂‡•Ä‡§≤ ‡§∏‡§Æ‡§æ‡§µ‡§ø‡§∑‡•ç‡§ü ‡§ï‡§∞‡§æ",
#     "importance": "high/medium/low",
#     "link": "URL",
#     "article_number": number,
#     "key_points": ["‡§Æ‡•Å‡§¶‡•ç‡§¶‡§æ 1", "‡§Æ‡•Å‡§¶‡•ç‡§¶‡§æ 2", "‡§Æ‡•Å‡§¶‡•ç‡§¶‡§æ 3"]
#   }}
# ]

# {articles_text}
# """
        
#         try:
#             response = perplexity_client.chat.completions.create(
#                 model="sonar-pro",
#                 messages=[
#                     {
#                         "role": "system",
#                         "content": "You are an expert Marathi news analyst. Return ONLY valid JSON array. No markdown, no explanation, no extra text."
#                     },
#                     {
#                         "role": "user",
#                         "content": prompt
#                     }
#                 ],
#                 temperature=0.3,
#                 max_tokens=4000
#             )
            
#             # Track tokens (CORRECTED: $1 per 1M tokens)
#             if hasattr(response, 'usage'):
#                 batch_tokens = response.usage.total_tokens
#                 total_tokens_used += batch_tokens
                
#                 # CORRECT pricing: $1 per 1M tokens
#                 batch_cost = (batch_tokens / 1_000_000) * 1.0
#                 total_cost += batch_cost
                
#                 print(f"   üìä Batch tokens: {batch_tokens:,} | Cost: ${batch_cost:.4f}")
            
#             content = response.choices[0].message.content
            
#             # Extract JSON
#             json_match = re.search(r'\[.*\]', content, re.DOTALL)
            
#             if json_match:
#                 batch_articles = json.loads(json_match.group())
#                 all_filtered.extend(batch_articles)
#                 print(f"   ‚úÖ Extracted {len(batch_articles)} articles from this batch")
#             else:
#                 print(f"   ‚ö†Ô∏è No valid JSON in AI response")
            
#         except json.JSONDecodeError as e:
#             print(f"   ‚ùå JSON parsing error: {e}")
#         except Exception as e:
#             print(f"   ‚ùå AI analysis error: {e}")
    
#     # Add source and timestamp
#     for article in all_filtered:
#         article['source'] = source_name
#         article['scraped_at'] = datetime.now().isoformat()
    
#     return all_filtered


# async def main():
#     global total_tokens_used, total_cost
    
#     print("üöÄ Starting Smart Marathi News Scraper with DETAILED Summaries")
#     print("üìç Focus: Criminal, Political & Important General News")
#     print("üìù Feature: Detailed 150-250 word summaries")
#     print("üí∞ Token tracking enabled (Correct pricing: $1/1M tokens)")
#     print("üéØ Strategy: Get top 10 news from ALL THREE SITES COMBINED\n")
    
#     start_time = datetime.now()
    
#     # Scrape all sites
#     all_articles = await scrape_marathi_news_final()
    
#     # Remove duplicates by title
#     unique_articles = []
#     seen_titles = set()
    
#     for article in all_articles:
#         title_lower = article['title'].lower()
#         if title_lower not in seen_titles:
#             unique_articles.append(article)
#             seen_titles.add(title_lower)
    
#     # Sort ALL articles by importance FIRST
#     priority_order = {'high': 1, 'medium': 2, 'low': 3}
#     unique_articles.sort(key=lambda x: priority_order.get(x.get('importance', 'medium'), 2))
    
#     # Save ALL articles to JSON (fixed filename)
#     output_file = "latest_news.json"
#     with open(output_file, 'w', encoding='utf-8') as f:
#         json.dump(unique_articles, f, ensure_ascii=False, indent=2)
    
#     # Save TOP 10 to separate file (fixed filename)
#     top_10_articles = unique_articles[:10]
#     top_10_file = "top_10_latest.json"
#     with open(top_10_file, 'w', encoding='utf-8') as f:
#         json.dump(top_10_articles, f, ensure_ascii=False, indent=2)
    
#     end_time = datetime.now()
#     duration = (end_time - start_time).total_seconds()
    
#     # Final summary
#     print("\n" + "="*80)
#     print("üìä SCRAPING SUMMARY")
#     print("="*80)
#     print(f"   Total articles scraped: {len(unique_articles)}")
#     print(f"   High importance: {len([a for a in unique_articles if a.get('importance') == 'high'])}")
#     print(f"   Crime news: {len([a for a in unique_articles if a.get('category') == 'crime'])}")
#     print(f"   Political news: {len([a for a in unique_articles if a.get('category') == 'politics'])}")
#     print(f"   General news: {len([a for a in unique_articles if a.get('category') == 'general'])}")
#     print(f"\n   By source:")
#     for source in ['TV9 Marathi', 'ABP Majha', 'Lokmat']:
#         count = len([a for a in unique_articles if a['source'] == source])
#         count_top10 = len([a for a in top_10_articles if a['source'] == source])
#         print(f"      ‚Ä¢ {source}: {count} total articles | {count_top10} in TOP 10")
#     print(f"\nüíæ All articles saved to: {output_file}")
#     print(f"üèÜ TOP 10 articles saved to: {top_10_file}")
#     print(f"\n‚è±Ô∏è  Total time: {duration:.2f} seconds")
#     print(f"üî¢ Total tokens used: {total_tokens_used:,}")
#     print(f"üí∞ Estimated cost: ${total_cost:.4f} (@ $1.00 per 1M tokens)")
#     if len(unique_articles) > 0:
#         print(f"üìà Average tokens per article: {total_tokens_used // len(unique_articles):,}")
#     print("="*80 + "\n")
    
#     # Display TOP 10 from ALL sites combined
#     if len(top_10_articles) > 0:
#         print("üèÜ TOP 10 IMPORTANT NEWS FROM ALL THREE SITES COMBINED")
#         print("="*80 + "\n")
        
#         for i, article in enumerate(top_10_articles, 1):
#             importance_emoji = "üî•" if article.get('importance') == 'high' else "üìå"
#             category_emoji = {
#                 'crime': 'üö®',
#                 'politics': 'üèõÔ∏è',
#                 'general': 'üì∞'
#             }.get(article.get('category', 'general'), 'üì∞')
            
#             print(f"{i}. {importance_emoji} {category_emoji} [{article['source']}]")
#             print(f"\n   üìã ‡§∂‡•Ä‡§∞‡•ç‡§∑‡§ï: {article['title']}")
#             print(f"\n   üìù ‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§ ‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂:")
#             print(f"   {article.get('detailed_summary', 'N/A')}")
            
#             if article.get('key_points'):
#                 print(f"\n   üîë ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§Æ‡•Å‡§¶‡•ç‡§¶‡•á:")
#                 for point in article['key_points']:
#                     print(f"      ‚Ä¢ {point}")
            
#             print(f"\n   üîó {article['link']}")
#             print(f"   ‚ö° ‡§Æ‡§π‡§§‡•ç‡§§‡•ç‡§µ: {article.get('importance', 'N/A').upper()}")
#             print("\n" + "-"*80 + "\n")
    
#     print("‚úÖ Smart scraping complete! Top 10 news from all sites extracted.\n")


# if __name__ == "__main__":
#     asyncio.run(main())
import asyncio
import json
from datetime import datetime
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode
from bs4 import BeautifulSoup
from openai import OpenAI
import re
import gspread
from google.oauth2.service_account import Credentials
import os

# Initialize Perplexity client

perplexity_client = OpenAI(
    api_key=os.environ.get("PERPLEXITY_API_KEY"),
    base_url="https://api.perplexity.ai"
)


# Google Sheets Configuration
GOOGLE_SHEETS_CREDENTIALS_FILE = "credentials.json"
GOOGLE_SHEET_NAME = "Instagram Scripts"
GOOGLE_WORKSHEET_NAME = "Scripts"


# Track token usage (CORRECTED: $1 per 1M tokens)
total_tokens_used = 0
total_cost = 0.0


def setup_google_sheets():
    """
    Initialize Google Sheets connection
    """
    try:
        # Define the scope
        scope = [
            'https://spreadsheets.google.com/feeds',
            'https://www.googleapis.com/auth/drive'
        ]
        
        # Load credentials
        creds = Credentials.from_service_account_file(
            GOOGLE_SHEETS_CREDENTIALS_FILE, 
            scopes=scope
        )
        
        # Authorize and connect
        client = gspread.authorize(creds)
        
        # Open or create spreadsheet
        try:
            sheet = client.open(GOOGLE_SHEET_NAME)
            print(f"‚úÖ Connected to existing sheet: '{GOOGLE_SHEET_NAME}'")
        except gspread.SpreadsheetNotFound:
            sheet = client.create(GOOGLE_SHEET_NAME)
            print(f"‚úÖ Created new sheet: '{GOOGLE_SHEET_NAME}'")
        
        # Open or create worksheet
        try:
            worksheet = sheet.worksheet(GOOGLE_WORKSHEET_NAME)
            print(f"‚úÖ Using worksheet: '{GOOGLE_WORKSHEET_NAME}'")
        except gspread.WorksheetNotFound:
            worksheet = sheet.add_worksheet(
                title=GOOGLE_WORKSHEET_NAME,
                rows=1000,
                cols=10
            )
            # Add headers (only 4 columns now)
            worksheet.update('A1:D1', [[
                'Timestamp',
                'Title',
                'Script',
                'Source Link'
            ]])
            
            # Format headers (bold, colored background, white text)
            worksheet.format('A1:D1', {
                'textFormat': {
                    'bold': True,
                    'foregroundColor': {'red': 1.0, 'green': 1.0, 'blue': 1.0}  # White text
                },
                'backgroundColor': {'red': 0.2, 'green': 0.6, 'blue': 0.9},  # Blue background
                'horizontalAlignment': 'CENTER'
            })
            
            # Set column widths
            worksheet.set_column_width('A', 180)  # Timestamp
            worksheet.set_column_width('B', 400)  # Title
            worksheet.set_column_width('C', 600)  # Script (wide)
            worksheet.set_column_width('D', 400)  # Source Link
            
            print(f"‚úÖ Created new worksheet with headers: '{GOOGLE_WORKSHEET_NAME}'")
        
        return worksheet
        
    except FileNotFoundError:
        print(f"‚ùå Error: '{GOOGLE_SHEETS_CREDENTIALS_FILE}' not found!")
        print("üí° Download credentials from Google Cloud Console")
        return None
    except Exception as e:
        print(f"‚ùå Google Sheets setup error: {e}")
        return None


def save_to_google_sheets(worksheet, script, source_link, news_title):
    """
    Append script data to Google Sheets with proper formatting (no overwriting)
    Only saves: Timestamp, Title, Script, Source Link
    """
    try:
        # Get current timestamp
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        # ‚úÖ FIX: Ensure all values are proper strings (not lists)
        # Convert script to plain string if it's in any other format
        if isinstance(script, list):
            script = '\n'.join(str(item) for item in script)
        else:
            script = str(script).strip()
        
        # Clean up any remaining brackets from the script
        script = script.replace('[', '').replace(']', '')
        
        # Ensure other fields are also strings
        news_title = str(news_title).strip()
        source_link = str(source_link).strip()
        
        # Prepare row data (only 4 columns) - all as strings
        row_data = [
            timestamp,
            news_title,
            script,
            source_link
        ]
        
        # Get next row number
        next_row = len(worksheet.get_all_values()) + 1
        
        # Append to the sheet (after last row) with RAW string values
        worksheet.append_row(row_data, value_input_option='RAW')  # Changed from USER_ENTERED to RAW
        
        # Format the newly added row (BLACK text, white background, wrap text)
        row_range = f'A{next_row}:D{next_row}'
        worksheet.format(row_range, {
            'textFormat': {
                'foregroundColor': {'red': 0.0, 'green': 0.0, 'blue': 0.0},  # BLACK text
                'fontSize': 10
            },
            'backgroundColor': {'red': 1.0, 'green': 1.0, 'blue': 1.0},  # White background
            'wrapStrategy': 'WRAP',  # Wrap text in cells
            'verticalAlignment': 'TOP'
        })
        
        # Format Script column (C) specifically - wrap and left align
        worksheet.format(f'C{next_row}', {
            'textFormat': {
                'foregroundColor': {'red': 0.0, 'green': 0.0, 'blue': 0.0},  # BLACK text
                'fontSize': 10
            },
            'wrapStrategy': 'WRAP',
            'verticalAlignment': 'TOP',
            'horizontalAlignment': 'LEFT'
        })
        
        # Format Title column (B) - left align
        worksheet.format(f'B{next_row}', {
            'textFormat': {
                'foregroundColor': {'red': 0.0, 'green': 0.0, 'blue': 0.0},  # BLACK text
                'fontSize': 10
            },
            'wrapStrategy': 'WRAP',
            'verticalAlignment': 'TOP',
            'horizontalAlignment': 'LEFT'
        })
        
        # Format link column (D) - make it clickable blue
        worksheet.format(f'D{next_row}', {
            'textFormat': {
                'foregroundColor': {'red': 0.06, 'green': 0.27, 'blue': 0.8},  # Blue text
                'fontSize': 10,
                'underline': True
            },
            'wrapStrategy': 'WRAP',
            'verticalAlignment': 'TOP'
        })
        
        print(f"‚úÖ Script saved to Google Sheets!")
        print(f"   Row #{next_row} added with timestamp: {timestamp}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error saving to Google Sheets: {e}")
        import traceback
        traceback.print_exc()
        return False


async def scrape_marathi_news_final():
    """
    Scraper that collects articles from all three sites
    """
    
    news_sites = [
        {
            "name": "TV9 Marathi",
            "url": "https://www.tv9marathi.com/latest-news",
            "article_selector": "article, div.story-card, div.news-item",
            "link_pattern": "tv9marathi.com"
        },
        {
            "name": "ABP Majha",
            "url": "https://marathi.abplive.com/news",
            "article_selector": "article, div.story-box, div.news-card",
            "link_pattern": "abplive.com"
        },
        {
            "name": "Lokmat",
            "url": "https://www.lokmat.com/latestnews/",
            "article_selector": "article, div.story-card, div.card-body",
            "link_pattern": "lokmat.com"
        }
    ]
    
    all_news = []
    
    async with AsyncWebCrawler(verbose=True) as crawler:
        
        for site in news_sites:
            print(f"\nüîç Scraping {site['name']}...")
            
            try:
                # Step 1: Fetch homepage with JavaScript rendering
                config = CrawlerRunConfig(
                    cache_mode=CacheMode.BYPASS,
                    wait_for="body",
                    word_count_threshold=10,
                    page_timeout=30000,
                    js_code="""
                    // Wait for content to load
                    await new Promise(r => setTimeout(r, 2000));
                    """
                )
                
                result = await crawler.arun(site['url'], config=config)
                
                if result.success:
                    soup = BeautifulSoup(result.html, 'html.parser')
                    
                    raw_articles = []
                    
                    # Strategy: Find all links with Marathi text
                    all_links = soup.find_all('a', href=True)
                    
                    for link_tag in all_links:
                        href = link_tag.get('href', '')
                        title = link_tag.get_text(strip=True)
                        
                        # Filter valid news links
                        if (len(title) > 15 and len(title) < 300 and
                            site['link_pattern'] in href and
                            not any(x in href for x in [
                                'javascript:', 'mailto:', '#', 
                                '/category/', '/tag/', '/author/',
                                'facebook.com', 'twitter.com', 'instagram.com',
                                'youtube.com', 'whatsapp.com', '/myaccount/',
                                '/install_app', '/advertisement', '/epaper',
                                'web-stories', 'photo-gallery', '/videos/',
                                '/sakhi/', '/astro/', '/bhakti/', '/games/',
                                '/jokes/', '/terms-and-conditions', '/utility-news',
                                '/spiritual-adhyatmik', '/rashi-bhavishya', 
                                '/topic/', '/elections/', '/career/'
                            ])):
                            
                            # Make absolute URL
                            if href.startswith('/'):
                                base_url = site['url'].split('/')[0] + '//' + site['url'].split('/')[2]
                                href = base_url + href
                            
                            if href.startswith('http'):
                                raw_articles.append({
                                    'title': title,
                                    'link': href
                                })
                    
                    # Remove duplicates by link
                    seen_links = set()
                    unique_articles = []
                    for article in raw_articles:
                        if article['link'] not in seen_links:
                            unique_articles.append(article)
                            seen_links.add(article['link'])
                    
                    print(f"üìã Found {len(unique_articles)} unique articles from {site['name']}")
                    
                    if len(unique_articles) > 0:
                        # CHANGED: Get top 10-15 articles per site (not 20)
                        print(f"üìÑ Fetching detailed content from top {min(12, len(unique_articles))} articles...")
                        
                        articles_with_content = []
                        for article in unique_articles[:12]:  # Top 12 per site
                            try:
                                article_result = await crawler.arun(
                                    article['link'],
                                    config=CrawlerRunConfig(
                                        cache_mode=CacheMode.BYPASS,
                                        word_count_threshold=50,
                                        page_timeout=15000
                                    )
                                )
                                
                                if article_result.success and len(article_result.markdown) > 100:
                                    articles_with_content.append({
                                        'title': article['title'],
                                        'link': article['link'],
                                        'content': article_result.markdown[:2500]
                                    })
                                    print(f"   ‚úì {article['title'][:60]}...")
                                    
                            except Exception as e:
                                continue
                        
                        print(f"‚úÖ Fetched content for {len(articles_with_content)} articles")
                        
                        # Step 3: AI analysis
                        if articles_with_content:
                            filtered_news = await smart_analyze_with_detailed_summary(
                                articles_with_content, 
                                site['name']
                            )
                            all_news.extend(filtered_news)
                            print(f"‚úÖ Extracted {len(filtered_news)} important articles with detailed summaries")
                    else:
                        print(f"‚ö†Ô∏è No articles found from {site['name']}")
                
                else:
                    print(f"‚ùå Failed to fetch {site['name']}: {result.error_message}")
                    
            except Exception as e:
                print(f"‚ùå Error scraping {site['name']}: {e}")
                import traceback
                traceback.print_exc()
    
    return all_news


async def smart_analyze_with_detailed_summary(articles, source_name):
    """
    AI analysis with CORRECTED token tracking ($1 per 1M tokens)
    """
    global total_tokens_used, total_cost
    
    print(f"\nüß† Using AI for detailed analysis of {source_name} articles...")
    
    all_filtered = []
    
    # Process in batches of 5
    for i in range(0, len(articles), 5):
        batch = articles[i:i+5]
        
        articles_text = ""
        for idx, article in enumerate(batch, i+1):
            articles_text += f"""
‡§¨‡§æ‡§§‡§Æ‡•Ä #{idx}:
‡§∂‡•Ä‡§∞‡•ç‡§∑‡§ï: {article['title']}
Link: {article['link']}
Content: {article['content'][:1200]}


---
"""
        
        prompt = f"""
‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä ‡§è‡§ï ‡§§‡§ú‡•ç‡§û ‡§Æ‡§∞‡§æ‡§†‡•Ä ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§ï ‡§Ü‡§π‡§æ‡§§. ‡§ñ‡§æ‡§≤‡•Ä‡§≤ ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ‡§Ç‡§ö‡•á ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£ ‡§ï‡§∞‡§æ.


**‡§´‡§ï‡•ç‡§§ ‡§π‡•á ‡§™‡•ç‡§∞‡§ï‡§æ‡§∞ ‡§®‡§ø‡§µ‡§°‡§æ:**
1. ‡§ó‡•Å‡§®‡•ç‡§π‡•á‡§ó‡§æ‡§∞‡•Ä ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ (Crime) - ‡§π‡§§‡•ç‡§Ø‡§æ, ‡§¶‡§∞‡•ã‡§°‡§æ, ‡§Ö‡§™‡§ò‡§æ‡§§, ‡§Ö‡§ü‡§ï, ‡§≤‡§æ‡§ö
2. ‡§∞‡§æ‡§ú‡§ï‡•Ä‡§Ø ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ (Political) - ‡§®‡§ø‡§µ‡§°‡§£‡•Å‡§ï‡§æ, ‡§∏‡§∞‡§ï‡§æ‡§∞, ‡§Æ‡§π‡§æ‡§™‡§æ‡§≤‡§ø‡§ï‡§æ, ‡§∞‡§æ‡§ú‡§ï‡•Ä‡§Ø ‡§ò‡§°‡§æ‡§Æ‡•ã‡§°‡•Ä
3. ‡§Æ‡§π‡§§‡•ç‡§§‡•ç‡§µ‡§æ‡§ö‡•ç‡§Ø‡§æ ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ (Important General) - ‡§∂‡§æ‡§∏‡§ï‡•Ä‡§Ø ‡§®‡§ø‡§∞‡•ç‡§£‡§Ø, ‡§∏‡§æ‡§Æ‡§æ‡§ú‡§ø‡§ï ‡§Æ‡•Å‡§¶‡•ç‡§¶‡•á


**‡§ü‡§æ‡§≥‡§æ‡§µ‡•á:** ‡§Æ‡§®‡•ã‡§∞‡§Ç‡§ú‡§® gossip, ‡§ú‡•ç‡§Ø‡•ã‡§§‡§ø‡§∑, ‡§´‡•Ö‡§∂‡§®, lifestyle, ‡§ñ‡•á‡§≥‡§æ‡§ö‡•Ä ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§¨‡§æ‡§§‡§Æ‡•Ä, job posts, ‡§ß‡§æ‡§∞‡•ç‡§Æ‡§ø‡§ï ‡§ï‡§•‡§æ, Bigg Boss, ‡§¨‡•â‡§≤‡•Ä‡§µ‡•Ç‡§° gossip


**JSON format (‡§´‡§ï‡•ç‡§§ array ‡§™‡§∞‡§§ ‡§ï‡§∞‡§æ, ‡§á‡§§‡§∞ ‡§ï‡§æ‡§π‡•Ä ‡§®‡§æ‡§π‡•Ä):**
[
  {{
    "title": "‡§Æ‡•Ç‡§≥ ‡§∂‡•Ä‡§∞‡•ç‡§∑‡§ï",
    "category": "crime/politics/general",
    "detailed_summary": "‡§∏‡§Ç‡§™‡•Ç‡§∞‡•ç‡§£ ‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§ ‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂ 150-250 ‡§∂‡§¨‡•ç‡§¶‡§æ‡§Ç‡§§ ‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§. ‡§ï‡§æ‡§Ø ‡§ò‡§°‡§≤‡§Ç? ‡§ï‡•Å‡§†‡•á? ‡§ï‡§ß‡•Ä? ‡§ï‡•ã‡§£ ‡§Ü‡§π‡•á‡§§? ‡§ï‡•ã‡§£‡§§‡•Ä ‡§ï‡§æ‡§∞‡§µ‡§æ‡§à? ‡§ï‡§æ‡§Ø ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ? ‡§á‡§§‡§∞ ‡§§‡§™‡§∂‡•Ä‡§≤ ‡§∏‡§Æ‡§æ‡§µ‡§ø‡§∑‡•ç‡§ü ‡§ï‡§∞‡§æ",
    "importance": "high/medium/low",
    "link": "URL",
    "article_number": number,
    "key_points": ["‡§Æ‡•Å‡§¶‡•ç‡§¶‡§æ 1", "‡§Æ‡•Å‡§¶‡•ç‡§¶‡§æ 2", "‡§Æ‡•Å‡§¶‡•ç‡§¶‡§æ 3"]
  }}
]


{articles_text}
"""
        
        try:
            response = perplexity_client.chat.completions.create(
                model="sonar-pro",
                messages=[
                    {
                        "role": "system",
                        "content": "You are an expert Marathi news analyst. Return ONLY valid JSON array. No markdown, no explanation, no extra text."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.3,
                max_tokens=4000
            )
            
            # Track tokens (CORRECTED: $1 per 1M tokens)
            if hasattr(response, 'usage'):
                batch_tokens = response.usage.total_tokens
                total_tokens_used += batch_tokens
                
                # CORRECT pricing: $1 per 1M tokens
                batch_cost = (batch_tokens / 1_000_000) * 1.0
                total_cost += batch_cost
                
                print(f"   üìä Batch tokens: {batch_tokens:,} | Cost: ${batch_cost:.4f}")
            
            content = response.choices[0].message.content
            
            # Extract JSON
            json_match = re.search(r'\[.*\]', content, re.DOTALL)
            
            if json_match:
                batch_articles = json.loads(json_match.group())
                all_filtered.extend(batch_articles)
                print(f"   ‚úÖ Extracted {len(batch_articles)} articles from this batch")
            else:
                print(f"   ‚ö†Ô∏è No valid JSON in AI response")
            
        except json.JSONDecodeError as e:
            print(f"   ‚ùå JSON parsing error: {e}")
        except Exception as e:
            print(f"   ‚ùå AI analysis error: {e}")
    
    # Add source and timestamp
    for article in all_filtered:
        article['source'] = source_name
        article['scraped_at'] = datetime.now().isoformat()
    
    return all_filtered


def create_reel_script(news_articles):
    """
    Generate ONE Instagram Reel script from news articles (takes list, returns one script)
    Returns: (script, source_link, news_title)
    """
    global total_tokens_used, total_cost
    
    # Prepare news summary for AI
    news_context = ""
    for idx, article in enumerate(news_articles[:5], 1):  # Use top 5 for context
        news_context += f"""
‡§¨‡§æ‡§§‡§Æ‡•Ä #{idx}:
‡§∂‡•Ä‡§∞‡•ç‡§∑‡§ï: {article['title']}
‡§™‡•ç‡§∞‡§ï‡§æ‡§∞: {article['category']}
‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂: {article['detailed_summary']}
‡§Æ‡§π‡§§‡•ç‡§§‡•ç‡§µ: {article['importance']}
‡§≤‡§ø‡§Ç‡§ï: {article['link']}
---
"""
    
    # System prompt with DIVERSE hook examples
    system_prompt = """
‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä "‡§ú‡§¨‡§∞‡•Ä ‡§ñ‡§¨‡§∞‡•Ä" Instagram Reels ‡§ö‡•á ‡§è‡§ï‡•ç‡§∏‡§™‡§∞‡•ç‡§ü ‡§∏‡•ç‡§ï‡•ç‡§∞‡§ø‡§™‡•ç‡§ü ‡§∞‡§æ‡§Ø‡§ü‡§∞ ‡§Ü‡§π‡§æ‡§§.


**CRITICAL: HOOK VARIETY (‡§™‡§π‡§ø‡§≤‡•ç‡§Ø‡§æ 2 ‡§ì‡§≥‡•Ä) - MUST USE DIFFERENT STYLES:**


**Hook Style 1: Shock Statement (‡§§‡§•‡•ç‡§Ø‡§æ‡§§‡•ç‡§Æ‡§ï ‡§ß‡§ï‡•ç‡§ï‡§æ)**
- "‡§è‡§ï‡§æ ‡§Ö‡§™‡§ò‡§æ‡§§‡§æ‡§®‡•á ‡§∏‡§Ç‡§™‡•Ç‡§∞‡•ç‡§£ ‡§∞‡§æ‡§ú‡•ç‡§Ø ‡§π‡§æ‡§¶‡§∞‡§≤‡§Ç."
- "‡§§‡•Ä‡§® ‡§µ‡§∞‡•ç‡§∑‡§æ‡§ö‡•ç‡§Ø‡§æ ‡§Æ‡•Å‡§≤‡•Ä‡§∏‡§Æ‡•ã‡§∞ ‡§Ü‡§à‡§®‡•á ‡§∏‡§Ç‡§™‡§µ‡§≤‡§Ç ‡§Ü‡§Ø‡•Å‡§∑‡•ç‡§Ø."
- "‡§Ü‡§§‡§æ ‡§Ø‡§æ ‡§ï‡•ç‡§∑‡§£‡§æ‡§ö‡•Ä ‡§∏‡§ó‡§≥‡•ç‡§Ø‡§æ‡§§ ‡§Æ‡•ã‡§†‡•Ä ‡§¨‡§æ‡§§‡§Æ‡•Ä ‡§Ø‡•á‡§§‡•Ä‡§Ø‡•á."


**Hook Style 2: Direct Question (‡§•‡•á‡§ü ‡§™‡•ç‡§∞‡§∂‡•ç‡§®)**
- "‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§≤‡§æ ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä ‡§Ü‡§π‡•á ‡§ï‡§æ, ‡§µ‡§ø‡§Æ‡§æ‡§®‡§æ‡§§‡§≤‡§æ ‡§¨‡•ç‡§≤‡•Ö‡§ï ‡§¨‡•â‡§ï‡•ç‡§∏ ‡§¨‡•ç‡§≤‡•Ö‡§ï ‡§®‡§∏‡§§‡•ã?"
- "‡§ï‡§ß‡•Ä ‡§µ‡§ø‡§ö‡§æ‡§∞ ‡§ï‡•á‡§≤‡§æ‡§Ø ‡§ï‡§æ, ‡§¶‡•á‡§∂ ‡§ö‡§æ‡§≤‡§µ‡§£‡§æ‡§∞‡•á ‡§≤‡•ã‡§ï ‡§ï‡§∏‡•á ‡§®‡§ø‡§Ø‡§§‡•Ä‡§ö‡•ç‡§Ø‡§æ ‡§ù‡§ü‡§ï‡•ç‡§Ø‡§æ‡§§ ‡§π‡§∞‡§™‡§§‡§æ‡§§?"
- "‡§Ø‡§æ‡§¨‡§¶‡•ç‡§¶‡§≤ ‡§ê‡§ï‡§≤‡§Ç‡§Ø ‡§ï‡§æ ‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä?"


**Hook Style 3: Breaking News (‡§¨‡•ç‡§∞‡•á‡§ï‡§ø‡§Ç‡§ó ‡§Ö‡§Ç‡§¶‡§æ‡§ú)**
- "‡§®‡•Å‡§ï‡§§‡•Ä‡§ö ‡§è‡§ï ‡§ß‡§ï‡•ç‡§ï‡§æ‡§¶‡§æ‡§Ø‡§ï ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä ‡§∏‡§Æ‡•ã‡§∞ ‡§Ü‡§≤‡•Ä ‡§Ü‡§π‡•á."
- "‡§ï‡§æ‡§≤ ‡§∞‡§æ‡§§‡•ç‡§∞‡•Ä ‡§ò‡§°‡§≤‡•á‡§≤‡•Ä ‡§π‡•Ä ‡§ò‡§ü‡§®‡§æ ‡§Ü‡§ú ‡§ö‡§∞‡•ç‡§ö‡•á‡§§ ‡§Ü‡§π‡•á."
- "‡§∏‡•ã‡§∂‡§≤ ‡§Æ‡•Ä‡§°‡§ø‡§Ø‡§æ‡§µ‡§∞ ‡§µ‡•ç‡§π‡§æ‡§Ø‡§∞‡§≤ ‡§π‡•ã‡§§‡•á‡§Ø ‡§π‡•á ‡§™‡•ç‡§∞‡§ï‡§∞‡§£."


**Hook Style 4: Name Drop (‡§®‡§æ‡§µ‡§æ‡§®‡•á ‡§∏‡•Å‡§∞‡•Å‡§µ‡§æ‡§§)**
- "‡§¨‡§æ‡§¨‡§æ‡§∏‡§æ‡§π‡•á‡§¨‡§æ‡§Ç‡§ö‡§Ç ‡§®‡§æ‡§µ ‡§ü‡§æ‡§≥‡§≤‡•á‡§≤‡§Ç ‡§ñ‡§™‡§µ‡•Ç‡§® ‡§ò‡•á‡§£‡§æ‡§∞ ‡§®‡§æ‡§π‡•Ä."
- "‡§Ö‡§ú‡§ø‡§§ ‡§¶‡§æ‡§¶‡§æ ‡§Æ‡•ç‡§π‡§£‡§ú‡•á ‡§´‡§ï‡•ç‡§§ ‡§∞‡§æ‡§ú‡§ï‡§æ‡§∞‡§£ ‡§®‡§æ‡§π‡•Ä."
- "[‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡•Ä/‡§†‡§ø‡§ï‡§æ‡§£ ‡§®‡§æ‡§µ] ‡§Ü‡§ú ‡§ö‡§∞‡•ç‡§ö‡•á‡§§ ‡§ï‡§æ ‡§Ü‡§π‡•á?"


**Hook Style 5: Contrast/Twist (‡§µ‡§ø‡§∞‡•ã‡§ß‡§æ‡§≠‡§æ‡§∏)**
- "‡§¶‡§ø‡§∏‡§§‡§Ç ‡§ï‡§æ‡§π‡•Ä‡§§‡§∞‡•Ä, ‡§™‡§£ ‡§µ‡§æ‡§∏‡•ç‡§§‡§µ ‡§ï‡§æ‡§π‡•Ä‡§§‡§∞‡•Ä ‡§µ‡•á‡§ó‡§≥‡§Ç‡§ö."
- "‡§∏‡§ó‡§≥‡•ç‡§Ø‡§æ‡§Ç‡§®‡§æ ‡§µ‡§æ‡§ü‡§§‡§Ç ‡§Ö‡§∏‡§Ç, ‡§™‡§£ ‡§ñ‡§∞‡§Ç ‡§ï‡§æ‡§Ø ‡§Ü‡§π‡•á?"
- "‡§∏‡§æ‡§°‡•Ä ‡§Æ‡•ç‡§π‡§£‡§ú‡•á ‡§´‡§ï‡•ç‡§§ ‡§´‡•Ö‡§∂‡§® ‡§®‡§æ‡§π‡•Ä, ‡§Ø‡§æ‡§Ü‡§° ‡§è‡§ï ‡§Æ‡•ã‡§†‡§æ ‡§∏‡§Ç‡§¶‡•á‡§∂ ‡§Ü‡§π‡•á."


**‚ö†Ô∏è WARNING: ‡§™‡•ç‡§∞‡§§‡•ç‡§Ø‡•á‡§ï script ‡§µ‡•á‡§ó‡§≥‡•ç‡§Ø‡§æ hook style ‡§®‡•á ‡§∏‡•Å‡§∞‡•Ç ‡§ï‡§∞‡§æ. SAME hook ‡§™‡•Å‡§®‡•ç‡§π‡§æ ‡§µ‡§æ‡§™‡§∞‡•Ç ‡§®‡§ï‡§æ!**


---


**‡§∏‡•ç‡§ü‡•ã‡§∞‡•Ä ‡§∏‡•ç‡§ü‡•ç‡§∞‡§ï‡•ç‡§ö‡§∞ (40-60 seconds):**
- ‡§™‡§π‡§ø‡§≤‡•á 2 ‡§ì‡§≥‡•Ä: ‡§∂‡•â‡§ï‡§ø‡§Ç‡§ó/‡§™‡•ç‡§∞‡§∂‡•ç‡§®‡§æ‡§§‡•ç‡§Æ‡§ï/‡§¨‡•ç‡§∞‡•á‡§ï‡§ø‡§Ç‡§ó hook (‡§µ‡§∞‡•Ä‡§≤ 5 styles ‡§™‡•à‡§ï‡•Ä ‡§è‡§ï)
- 3-10 ‡§ì‡§≥‡•Ä: ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§ò‡§ü‡§®‡§æ (‡§ï‡•ã‡§£/‡§ï‡§æ‡§Ø/‡§ï‡•Å‡§†‡•á/‡§ï‡§ß‡•Ä/‡§ï‡§∂‡•Ä) - ‡§§‡§™‡§∂‡•Ä‡§≤‡§µ‡§æ‡§∞
- 11-14 ‡§ì‡§≥‡•Ä: ‡§ü‡•ç‡§µ‡§ø‡§∏‡•ç‡§ü/‡§™‡•ç‡§∞‡§∂‡•ç‡§®/‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£ - "‡§Ü‡§§‡§æ ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§Ö‡§∏‡§æ..." ‡§ï‡§ø‡§Ç‡§µ‡§æ "‡§™‡§£ ‡§è‡§ï ‡§ó‡•ã‡§∑‡•ç‡§ü ‡§®‡§ï‡•ç‡§ï‡•Ä..."
- ‡§∂‡•á‡§µ‡§ü‡§ö‡•ç‡§Ø‡§æ 2-3 ‡§ì‡§≥‡•Ä: Call to Action


**‡§≠‡§æ‡§∑‡§æ ‡§∏‡•ç‡§ü‡§æ‡§à‡§≤:**
- ‡§∏‡§Ç‡§≠‡§æ‡§∑‡§£‡§æ‡§§‡•ç‡§Æ‡§ï ‡§Æ‡§∞‡§æ‡§†‡•Ä (formal news ‡§≠‡§æ‡§∑‡§æ ‡§®‡§æ‡§π‡•Ä!)
- ‡§≠‡§æ‡§µ‡§®‡§ø‡§ï ‡§∂‡§¨‡•ç‡§¶: "‡§π‡§æ‡§¶‡§∞‡§≤‡§Ç", "‡§ß‡§ï‡•ç‡§ï‡§æ‡§¶‡§æ‡§Ø‡§ï", "‡§∏‡•Å‡§®‡•ç‡§® ‡§ï‡§∞‡§£‡§æ‡§∞‡•Ä", "‡§ö‡§ü‡§ï‡§æ ‡§≤‡§æ‡§µ‡•Ç‡§® ‡§ú‡§æ‡§£‡§æ‡§∞‡§æ"
- ‡§™‡•ç‡§∞‡§∂‡•ç‡§®‡§æ‡§§‡•ç‡§Æ‡§ï ‡§µ‡§≥‡§£: "‡§Ö‡§∏‡§æ ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§â‡§™‡§∏‡•ç‡§•‡§ø‡§§ ‡§π‡•ã‡§§‡•ã‡§Ø ‡§ï‡•Ä..."
- ‡§•‡•á‡§ü ‡§∏‡§Ç‡§µ‡§æ‡§¶: "‡§§‡•Å‡§Æ‡§ö‡§Ç ‡§ï‡§æ‡§Ø ‡§Æ‡§§ ‡§Ü‡§π‡•á?"


**TONE BY CATEGORY:**
- CRIME: ‡§∂‡•â‡§ï‡§ø‡§Ç‡§ó + ‡§™‡•ç‡§∞‡§∂‡•ç‡§®‡§æ‡§Ç‡§ï‡§ø‡§§ ("‡§ï‡§∏‡§Ç ‡§∂‡§ï‡•ç‡§Ø ‡§ù‡§æ‡§≤‡§Ç? ‡§ï‡•ã‡§£‡•Ä ‡§§‡§™‡§æ‡§∏‡§≤‡§Ç ‡§®‡§æ‡§π‡•Ä ‡§ï‡§æ?")
- POLITICS: ‡§®‡§æ‡§ü‡•ç‡§Ø‡§Æ‡§Ø + ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£‡§æ‡§§‡•ç‡§Æ‡§ï ("‡§∞‡§æ‡§ú‡§ï‡§æ‡§∞‡§£‡§æ‡§§‡§≤‡•Ä ‡§π‡•Ä ‡§ö‡§æ‡§≤ ‡§ï‡§æ‡§Ø ‡§Ü‡§π‡•á?")
- GENERAL: ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä‡§™‡•Ç‡§∞‡•ç‡§£ + ‡§∞‡§Ç‡§ú‡§ï ("‡§π‡•Ä ‡§ó‡•ã‡§∑‡•ç‡§ü ‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§≤‡§æ ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä ‡§π‡•ã‡§§‡•Ä ‡§ï‡§æ?")


**SIGNATURE ENDING (‡§∂‡•á‡§µ‡§ü‡§ö‡•ç‡§Ø‡§æ 2-3 ‡§ì‡§≥‡•Ä - ‡§Ø‡§æ‡§™‡•à‡§ï‡•Ä ‡§è‡§ï ‡§µ‡§æ‡§™‡§∞‡§æ):**
- "‡§§‡•Å‡§Æ‡§ö‡§Ç ‡§ï‡§æ‡§Ø ‡§Æ‡§§ ‡§Ü‡§π‡•á? ‡§ï‡§Æ‡•á‡§Ç‡§ü ‡§ï‡§∞‡•Ç‡§® ‡§∏‡§æ‡§Ç‡§ó‡§æ ‡§Ü‡§£‡§ø ‡§Ö‡§∂‡§æ‡§ö ‡§Ö‡§™‡§°‡•á‡§ü‡§∏‡§æ‡§†‡•Ä ‡§´‡•â‡§≤‡•ã ‡§ï‡§∞‡§æ ‡§ú‡§¨‡§∞‡•Ä ‡§ñ‡§¨‡§∞‡•Ä."
- "‡§π‡•Ä ‡§ò‡§ü‡§®‡§æ ‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§≤‡§æ ‡§ï‡§∂‡•Ä ‡§µ‡§æ‡§ü‡§≤‡•Ä? ‡§Ü‡§Æ‡•ç‡§π‡§æ‡§≤‡§æ ‡§ï‡§Æ‡•á‡§Ç‡§ü ‡§ï‡§∞‡•Ç‡§® ‡§®‡§ï‡•ç‡§ï‡•Ä ‡§∏‡§æ‡§Ç‡§ó‡§æ ‡§Ü‡§£‡§ø ‡§´‡•â‡§≤‡•ã ‡§ï‡§∞‡§æ ‡§ú‡§¨‡§∞‡•Ä ‡§ñ‡§¨‡§∞‡•Ä."
- "‡§Ø‡§æ ‡§™‡•ç‡§∞‡§ï‡§∞‡§£‡§æ‡§§ ‡§§‡•Å‡§Æ‡§ö‡§æ ‡§ï‡§æ‡§Ø ‡§Ö‡§≠‡§ø‡§™‡•ç‡§∞‡§æ‡§Ø? ‡§ï‡§Æ‡•á‡§Ç‡§ü‡§Æ‡§ß‡•ç‡§Ø‡•á ‡§®‡§ï‡•ç‡§ï‡•Ä ‡§∏‡§æ‡§Ç‡§ó‡§æ ‡§Ü‡§£‡§ø ‡§Ö‡§∂‡§æ‡§ö ‡§ú‡§¨‡§∞‡•Ä ‡§Ö‡§™‡§°‡•á‡§ü‡§∏‡§æ‡§†‡•Ä ‡§´‡•â‡§≤‡•ã ‡§ï‡§∞‡§æ."


**FORBIDDEN:**
‚ùå ‡§™‡•ç‡§∞‡§§‡•ç‡§Ø‡•á‡§ï reel "‡§ï‡§ß‡•Ä ‡§µ‡§ø‡§ö‡§æ‡§∞ ‡§ï‡•á‡§≤‡§æ‡§Ø ‡§ï‡§æ" ‡§®‡•á ‡§∏‡•Å‡§∞‡•Ç ‡§ï‡§∞‡§£‡•á
‚ùå ‡§∏‡§Æ‡§æ‡§® hook pattern ‡§™‡•Å‡§®‡•ç‡§π‡§æ ‡§µ‡§æ‡§™‡§∞‡§£‡•á
‚ùå ‡§¨‡•Å‡§≤‡•á‡§ü ‡§™‡•â‡§á‡§Ç‡§ü‡•ç‡§∏ ‡§ï‡§ø‡§Ç‡§µ‡§æ lists
‚ùå ‡§´‡•â‡§∞‡•ç‡§Æ‡§≤ ‡§®‡•ç‡§Ø‡•Ç‡§ú ‡§≠‡§æ‡§∑‡§æ
‚ùå 15+ ‡§∂‡§¨‡•ç‡§¶‡§æ‡§Ç‡§ö‡•Ä ‡§≤‡§æ‡§Ç‡§¨ ‡§µ‡§æ‡§ï‡•ç‡§Ø‡•á


**OUTPUT FORMAT:**
‡§´‡§ï‡•ç‡§§ ‡§∏‡•ç‡§ï‡•ç‡§∞‡§ø‡§™‡•ç‡§ü ‡§™‡§∞‡§§ ‡§ï‡§∞‡§æ. 15-18 ‡§ì‡§≥‡•Ä. ‡§™‡•ç‡§∞‡§§‡•ç‡§Ø‡•á‡§ï reel ‡§∏‡§æ‡§†‡•Ä DIFFERENT hook style ‡§®‡§ø‡§µ‡§°‡§æ.


**IMPORTANT:** script ‡§∂‡•á‡§µ‡§ü‡•Ä ‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä ‡§ï‡•ã‡§£‡§§‡•Ä ‡§¨‡§æ‡§§‡§Æ‡•Ä ‡§µ‡§æ‡§™‡§∞‡§≤‡•Ä ‡§§‡•á ‡§∏‡§æ‡§Ç‡§ó‡§æ:
Format: [SCRIPT]\n\n---ARTICLE_NUMBER: X---
"""
    
    # User prompt
    user_prompt = f"""
‡§ñ‡§æ‡§≤‡•Ä‡§≤ ‡§Ü‡§ú‡§ö‡•ç‡§Ø‡§æ TOP ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ‡§Ç‡§™‡•à‡§ï‡•Ä ‡§∏‡§∞‡•ç‡§µ‡§æ‡§§ ENGAGING ‡§Ü‡§£‡§ø VIRAL ‡§π‡•ã‡§£‡•ç‡§Ø‡§æ‡§ö‡•Ä ‡§ï‡•ç‡§∑‡§Æ‡§§‡§æ ‡§Ö‡§∏‡§≤‡•á‡§≤‡•Ä ‡§¨‡§æ‡§§‡§Æ‡•Ä ‡§®‡§ø‡§µ‡§°‡•Ç‡§® ‡§§‡•ç‡§Ø‡§æ‡§µ‡§∞ ‡§è‡§ï Instagram Reel script ‡§§‡§Ø‡§æ‡§∞ ‡§ï‡§∞‡§æ.


**‡§®‡§ø‡§µ‡§° ‡§ï‡§∞‡§§‡§æ‡§®‡§æ:**
1. CRIME ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ ‡§™‡•ç‡§∞‡§æ‡§ß‡§æ‡§®‡•ç‡§Ø (‡§∏‡§∞‡•ç‡§µ‡§æ‡§ß‡§ø‡§ï viral)
2. SHOCKING ‡§ï‡§ø‡§Ç‡§µ‡§æ CONTROVERSIAL ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ ‡§™‡•Å‡§¢‡•á
3. EMOTIONAL CONNECTION ‡§Ö‡§∏‡§≤‡•á‡§≤‡•ç‡§Ø‡§æ ‡§ó‡•ã‡§∑‡•ç‡§ü‡•Ä


**‡§Ü‡§ú‡§ö‡•ç‡§Ø‡§æ ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ:**
{news_context}


**‡§§‡•Å‡§Æ‡§ö‡§Ç ‡§ï‡§æ‡§Æ:**
1. ‡§µ‡§∞‡•Ä‡§≤ ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ‡§Ç‡§™‡•à‡§ï‡•Ä ‡§∏‡§∞‡•ç‡§µ‡§æ‡§§ STRONG ‡§¨‡§æ‡§§‡§Æ‡•Ä ‡§®‡§ø‡§µ‡§°‡§æ
2. **5 HOOK STYLES ‡§™‡•à‡§ï‡•Ä ‡§¨‡§æ‡§§‡§Æ‡•Ä‡§≤‡§æ ‡§∏‡§∞‡•ç‡§µ‡§æ‡§§ ‡§Ø‡•ã‡§ó‡•ç‡§Ø hook ‡§®‡§ø‡§µ‡§°‡§æ** (‡§ï‡§ß‡•Ä ‡§µ‡§ø‡§ö‡§æ‡§∞ ‡§ï‡•á‡§≤‡§æ‡§Ø ‡§ï‡§æ - ‡§π‡§æ ‡§´‡§ï‡•ç‡§§ ‡§è‡§ï option ‡§Ü‡§π‡•á!)
3. Jabari Khabari ‡§ö‡•ç‡§Ø‡§æ EXACT ‡§∏‡•ç‡§ü‡§æ‡§à‡§≤‡§Æ‡§ß‡•ç‡§Ø‡•á 15-18 ‡§ì‡§≥‡•Ä‡§Ç‡§ö‡•Ä script ‡§≤‡§ø‡§π‡§æ
4. ‡§∏‡§Ç‡§≠‡§æ‡§∑‡§£‡§æ‡§§‡•ç‡§Æ‡§ï, ‡§®‡§æ‡§ü‡•ç‡§Ø‡§Æ‡§Ø, ‡§™‡•ç‡§∞‡§∂‡•ç‡§®‡§æ‡§§‡•ç‡§Æ‡§ï ‡§≠‡§æ‡§∑‡§æ ‡§µ‡§æ‡§™‡§∞‡§æ
5. ‡§∂‡•á‡§µ‡§ü‡•Ä article number ‡§¶‡•ç‡§Ø‡§æ


**CRITICAL: Hook MUST be VARIED. ‡§¨‡§æ‡§§‡§Æ‡•Ä‡§ö‡•ç‡§Ø‡§æ ‡§∏‡•ç‡§µ‡§∞‡•Ç‡§™‡§æ‡§®‡•Å‡§∏‡§æ‡§∞ ‡§Ø‡•ã‡§ó‡•ç‡§Ø hook style ‡§®‡§ø‡§µ‡§°‡§æ!**


OUTPUT FORMAT:
[‡§§‡•Å‡§Æ‡§ö‡•Ä script]


---ARTICLE_NUMBER: X---
"""
    
    # Call Perplexity API
    try:
        response = perplexity_client.chat.completions.create(
            model="sonar-pro",
            messages=[
                {
                    "role": "system",
                    "content": system_prompt
                },
                {
                    "role": "user",
                    "content": user_prompt
                }
            ],
            temperature=0.8,  # Increased for more creativity/variety
            max_tokens=1500
        )
        
        # Track tokens
        if hasattr(response, 'usage'):
            script_tokens = response.usage.total_tokens
            total_tokens_used += script_tokens
            script_cost = (script_tokens / 1_000_000) * 1.0
            total_cost += script_cost
        
        full_response = response.choices[0].message.content.strip()
        
        # Extract script and article number
        if "---ARTICLE_NUMBER:" in full_response:
            parts = full_response.split("---ARTICLE_NUMBER:")
            script = parts[0].strip()
            article_num_str = parts[1].strip().replace("---", "").strip()
            try:
                article_num = int(article_num_str) - 1  # Convert to 0-indexed
            except:
                article_num = 0  # Default to first article
        else:
            script = full_response
            article_num = 0  # Default to first article
        
        # Clean up the script
        script = script.replace('```', '').strip()
        script = script.replace('---ARTICLE_NUMBER:', '').strip()
        
        # Get source article details
        if article_num < len(news_articles):
            source_article = news_articles[article_num]
            source_link = source_article.get('link', 'N/A')
            news_title = source_article.get('title', 'N/A')
        else:
            source_article = news_articles
            source_link = source_article.get('link', 'N/A')
            news_title = source_article.get('title', 'N/A')
        
        return script, source_link, news_title
        
    except Exception as e:
        print(f"‚ùå Error generating script: {e}")
        return None, None, None


async def main():
    global total_tokens_used, total_cost
    
    print("üöÄ Starting Smart Marathi News Scraper + Script Generator")
    print("üìç Focus: Criminal, Political & Important General News")
    print("üìù Feature: Detailed summaries + Instagram Scripts")
    print("üí∞ Token tracking enabled")
    print("üìä Output: Direct to Google Sheets (No local files)\n")
    
    start_time = datetime.now()
    
    # ===== PART 1: SCRAPING (NO CHANGES) =====
    all_articles = await scrape_marathi_news_final()
    
    # Remove duplicates by title
    unique_articles = []
    seen_titles = set()
    
    for article in all_articles:
        title_lower = article['title'].lower()
        if title_lower not in seen_titles:
            unique_articles.append(article)
            seen_titles.add(title_lower)
    
    # Sort ALL articles by importance FIRST
    priority_order = {'high': 1, 'medium': 2, 'low': 3}
    unique_articles.sort(key=lambda x: priority_order.get(x.get('importance', 'medium'), 2))
    
    # ‚ùå REMOVED: JSON file saving
    # Get TOP 10
    top_10_articles = unique_articles[:10]
    
    end_scrape = datetime.now()
    scrape_duration = (end_scrape - start_time).total_seconds()
    
    # Scraping summary
    print("\n" + "="*80)
    print("üìä SCRAPING SUMMARY")
    print("="*80)
    print(f"   Total articles scraped: {len(unique_articles)}")
    print(f"   High importance: {len([a for a in unique_articles if a.get('importance') == 'high'])}")
    print(f"   Crime news: {len([a for a in unique_articles if a.get('category') == 'crime'])}")
    print(f"   Political news: {len([a for a in unique_articles if a.get('category') == 'politics'])}")
    print(f"   General news: {len([a for a in unique_articles if a.get('category') == 'general'])}")
    print(f"\n   By source:")
    for source in ['TV9 Marathi', 'ABP Majha', 'Lokmat']:
        count = len([a for a in unique_articles if a['source'] == source])
        count_top10 = len([a for a in top_10_articles if a['source'] == source])
        print(f"      ‚Ä¢ {source}: {count} total | {count_top10} in TOP 10")
    print(f"\n‚è±Ô∏è  Scraping time: {scrape_duration:.2f} seconds")
    print("="*80 + "\n")
    
    # ===== PART 2: SCRIPT GENERATION (PASS DATA DIRECTLY) =====
    print("="*80)
    print("üé¨ GENERATING INSTAGRAM SCRIPTS")
    print("="*80 + "\n")
    
    # Setup Google Sheets
    worksheet = setup_google_sheets()
    
    if worksheet and len(top_10_articles) > 0:
        print(f"\nüéØ Generating script from TOP 10 articles...\n")
        
        # Generate ONE script from top 10
        script, source_link, news_title = create_reel_script(top_10_articles)
        
        if script:
            print("\n" + "="*70)
            print("üìù GENERATED SCRIPT:")
            print("="*70)
            print(script)
            print("\n" + "="*70)
            print(f"üì∞ Title: {news_title}")
            print(f"üîó Source: {source_link}")
            print("="*70 + "\n")
            
            # Save to Google Sheets
            success = save_to_google_sheets(worksheet, script, source_link, news_title)
            
            if success:
                print(f"üìà View your sheet: https://docs.google.com/spreadsheets/d/{worksheet.spreadsheet.id}")
        else:
            print("‚ùå Failed to generate script")
    else:
        print("‚ö†Ô∏è No articles or Google Sheets unavailable")
    
    end_time = datetime.now()
    total_duration = (end_time - start_time).total_seconds()
    
    # Final summary
    print("\n" + "="*80)
    print("‚úÖ COMPLETE!")
    print("="*80)
    print(f"   Total articles scraped: {len(unique_articles)}")
    print(f"   Scripts generated: 1")
    print(f"   Saved to: {GOOGLE_SHEET_NAME}")
    print(f"\n   ‚è±Ô∏è Total time: {total_duration:.2f} seconds")
    print(f"   üî¢ Total tokens: {total_tokens_used:,}")
    print(f"   üí∞ Total cost: ${total_cost:.4f}")
    print("="*80 + "\n")


if __name__ == "__main__":
    asyncio.run(main())