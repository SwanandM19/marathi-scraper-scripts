
# import asyncio
# import json
# from datetime import datetime
# from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode
# from bs4 import BeautifulSoup
# from openai import OpenAI
# import re
# import gspread
# from google.oauth2.service_account import Credentials
# import os


# # Initialize Perplexity client from environment variable
# perplexity_client = OpenAI(
#     api_key=os.environ.get("PERPLEXITY_API_KEY"),  # Read from GitHub Secrets
#     base_url="https://api.perplexity.ai"
# )


# # Google Sheets Configuration
# GOOGLE_SHEETS_CREDENTIALS_FILE = "credentials.json"  # Created dynamically by GitHub Actions
# GOOGLE_SHEET_NAME = "Instagram Scripts"
# GOOGLE_WORKSHEET_NAME = "Scripts"


# # Track token usage (CORRECTED: $1 per 1M tokens)
# total_tokens_used = 0
# total_cost = 0.0


# def setup_google_sheets():
#     """
#     Initialize Google Sheets connection
#     """
#     try:
#         # Define the scope
#         scope = [
#             'https://spreadsheets.google.com/feeds',
#             'https://www.googleapis.com/auth/drive'
#         ]
        
#         # Load credentials
#         creds = Credentials.from_service_account_file(
#             GOOGLE_SHEETS_CREDENTIALS_FILE, 
#             scopes=scope
#         )
        
#         # Authorize and connect
#         client = gspread.authorize(creds)
        
#         # Open or create spreadsheet
#         try:
#             sheet = client.open(GOOGLE_SHEET_NAME)
#             print(f"‚úÖ Connected to existing sheet: '{GOOGLE_SHEET_NAME}'")
#         except gspread.SpreadsheetNotFound:
#             sheet = client.create(GOOGLE_SHEET_NAME)
#             print(f"‚úÖ Created new sheet: '{GOOGLE_SHEET_NAME}'")
        
#         # Open or create worksheet
#         try:
#             worksheet = sheet.worksheet(GOOGLE_WORKSHEET_NAME)
#             print(f"‚úÖ Using worksheet: '{GOOGLE_WORKSHEET_NAME}'")
#         except gspread.WorksheetNotFound:
#             worksheet = sheet.add_worksheet(
#                 title=GOOGLE_WORKSHEET_NAME,
#                 rows=1000,
#                 cols=10
#             )
#             # Add headers (only 4 columns now)
#             worksheet.update('A1:D1', [[
#                 'Timestamp',
#                 'Title',
#                 'Script',
#                 'Source Link'
#             ]])
            
#             # Format headers (bold, colored background, white text)
#             worksheet.format('A1:D1', {
#                 'textFormat': {
#                     'bold': True,
#                     'foregroundColor': {'red': 1.0, 'green': 1.0, 'blue': 1.0}  # White text
#                 },
#                 'backgroundColor': {'red': 0.2, 'green': 0.6, 'blue': 0.9},  # Blue background
#                 'horizontalAlignment': 'CENTER'
#             })
            
#             # Set column widths
#             worksheet.set_column_width('A', 180)  # Timestamp
#             worksheet.set_column_width('B', 400)  # Title
#             worksheet.set_column_width('C', 600)  # Script (wide)
#             worksheet.set_column_width('D', 400)  # Source Link
            
#             print(f"‚úÖ Created new worksheet with headers: '{GOOGLE_WORKSHEET_NAME}'")
        
#         return worksheet
        
#     except FileNotFoundError:
#         print(f"‚ùå Error: '{GOOGLE_SHEETS_CREDENTIALS_FILE}' not found!")
#         print("üí° This file is created automatically by GitHub Actions")
#         return None
#     except Exception as e:
#         print(f"‚ùå Google Sheets setup error: {e}")
#         import traceback
#         traceback.print_exc()
#         return None


# def save_to_google_sheets(worksheet, script, source_link, news_title):
#     """
#     Append script data to Google Sheets with proper formatting
#     Only saves: Timestamp, Title, Script, Source Link
#     """
#     try:
#         # Get current timestamp
#         timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
#         # Ensure all values are proper strings
#         if isinstance(script, list):
#             script = '\n'.join(str(item) for item in script)
#         else:
#             script = str(script).strip()
        
#         # Clean up any remaining brackets
#         script = script.replace('[', '').replace(']', '')
        
#         # Ensure other fields are strings
#         news_title = str(news_title).strip()
#         source_link = str(source_link).strip()
        
#         # Prepare row data (4 columns)
#         row_data = [
#             timestamp,
#             news_title,
#             script,
#             source_link
#         ]
        
#         # Get next row number
#         next_row = len(worksheet.get_all_values()) + 1
        
#         # Append to the sheet with RAW string values
#         worksheet.append_row(row_data, value_input_option='RAW')
        
#         # Format the newly added row (BLACK text, white background, wrap text)
#         row_range = f'A{next_row}:D{next_row}'
#         worksheet.format(row_range, {
#             'textFormat': {
#                 'foregroundColor': {'red': 0.0, 'green': 0.0, 'blue': 0.0},
#                 'fontSize': 10
#             },
#             'backgroundColor': {'red': 1.0, 'green': 1.0, 'blue': 1.0},
#             'wrapStrategy': 'WRAP',
#             'verticalAlignment': 'TOP'
#         })
        
#         # Format Script column (C) - wrap and left align
#         worksheet.format(f'C{next_row}', {
#             'textFormat': {
#                 'foregroundColor': {'red': 0.0, 'green': 0.0, 'blue': 0.0},
#                 'fontSize': 10
#             },
#             'wrapStrategy': 'WRAP',
#             'verticalAlignment': 'TOP',
#             'horizontalAlignment': 'LEFT'
#         })
        
#         # Format Title column (B) - left align
#         worksheet.format(f'B{next_row}', {
#             'textFormat': {
#                 'foregroundColor': {'red': 0.0, 'green': 0.0, 'blue': 0.0},
#                 'fontSize': 10
#             },
#             'wrapStrategy': 'WRAP',
#             'verticalAlignment': 'TOP',
#             'horizontalAlignment': 'LEFT'
#         })
        
#         # Format link column (D) - clickable blue
#         worksheet.format(f'D{next_row}', {
#             'textFormat': {
#                 'foregroundColor': {'red': 0.06, 'green': 0.27, 'blue': 0.8},
#                 'fontSize': 10,
#                 'underline': True
#             },
#             'wrapStrategy': 'WRAP',
#             'verticalAlignment': 'TOP'
#         })
        
#         print(f"‚úÖ Script saved to Google Sheets!")
#         print(f"   Row #{next_row} added with timestamp: {timestamp}")
        
#         return True
        
#     except Exception as e:
#         print(f"‚ùå Error saving to Google Sheets: {e}")
#         import traceback
#         traceback.print_exc()
#         return False


# async def scrape_marathi_news_final():
#     """
#     Scraper that collects articles from all three sites
#     """
    
#     news_sites = [
#         {
#             "name": "TV9 Marathi",
#             "url": "https://www.tv9marathi.com/latest-news",
#             "article_selector": "article, div.story-card, div.news-item",
#             "link_pattern": "tv9marathi.com"
#         },
#         {
#             "name": "ABP Majha",
#             "url": "https://marathi.abplive.com/news",
#             "article_selector": "article, div.story-box, div.news-card",
#             "link_pattern": "abplive.com"
#         },
#         {
#             "name": "Lokmat",
#             "url": "https://www.lokmat.com/latestnews/",
#             "article_selector": "article, div.story-card, div.card-body",
#             "link_pattern": "lokmat.com"
#         }
#     ]
    
#     all_news = []
    
#     async with AsyncWebCrawler(verbose=True) as crawler:
        
#         for site in news_sites:
#             print(f"\nüîç Scraping {site['name']}...")
            
#             try:
#                 # Fetch homepage with JavaScript rendering
#                 config = CrawlerRunConfig(
#                     cache_mode=CacheMode.BYPASS,
#                     wait_for="body",
#                     word_count_threshold=10,
#                     page_timeout=30000,
#                     js_code="""
#                     // Wait for content to load
#                     await new Promise(r => setTimeout(r, 2000));
#                     """
#                 )
                
#                 result = await crawler.arun(site['url'], config=config)
                
#                 if result.success:
#                     soup = BeautifulSoup(result.html, 'html.parser')
                    
#                     raw_articles = []
#                     all_links = soup.find_all('a', href=True)
                    
#                     for link_tag in all_links:
#                         href = link_tag.get('href', '')
#                         title = link_tag.get_text(strip=True)
                        
#                         # Filter valid news links
#                         if (len(title) > 15 and len(title) < 300 and
#                             site['link_pattern'] in href and
#                             not any(x in href for x in [
#                                 'javascript:', 'mailto:', '#', 
#                                 '/category/', '/tag/', '/author/',
#                                 'facebook.com', 'twitter.com', 'instagram.com',
#                                 'youtube.com', 'whatsapp.com', '/myaccount/',
#                                 '/install_app', '/advertisement', '/epaper',
#                                 'web-stories', 'photo-gallery', '/videos/',
#                                 '/sakhi/', '/astro/', '/bhakti/', '/games/',
#                                 '/jokes/', '/terms-and-conditions', '/utility-news',
#                                 '/spiritual-adhyatmik', '/rashi-bhavishya', 
#                                 '/topic/', '/elections/', '/career/'
#                             ])):
                            
#                             # Make absolute URL
#                             if href.startswith('/'):
#                                 base_url = site['url'].split('/')[0] + '//' + site['url'].split('/')[2]
#                                 href = base_url + href
                            
#                             if href.startswith('http'):
#                                 raw_articles.append({
#                                     'title': title,
#                                     'link': href
#                                 })
                    
#                     # Remove duplicates by link
#                     seen_links = set()
#                     unique_articles = []
#                     for article in raw_articles:
#                         if article['link'] not in seen_links:
#                             unique_articles.append(article)
#                             seen_links.add(article['link'])
                    
#                     print(f"üìã Found {len(unique_articles)} unique articles from {site['name']}")
                    
#                     if len(unique_articles) > 0:
#                         print(f"üìÑ Fetching detailed content from top {min(12, len(unique_articles))} articles...")
                        
#                         articles_with_content = []
#                         for article in unique_articles[:12]:
#                             try:
#                                 article_result = await crawler.arun(
#                                     article['link'],
#                                     config=CrawlerRunConfig(
#                                         cache_mode=CacheMode.BYPASS,
#                                         word_count_threshold=50,
#                                         page_timeout=15000
#                                     )
#                                 )
                                
#                                 if article_result.success and len(article_result.markdown) > 100:
#                                     articles_with_content.append({
#                                         'title': article['title'],
#                                         'link': article['link'],
#                                         'content': article_result.markdown[:2500]
#                                     })
#                                     print(f"   ‚úì {article['title'][:60]}...")
                                    
#                             except Exception as e:
#                                 continue
                        
#                         print(f"‚úÖ Fetched content for {len(articles_with_content)} articles")
                        
#                         # AI analysis
#                         if articles_with_content:
#                             filtered_news = await smart_analyze_with_detailed_summary(
#                                 articles_with_content, 
#                                 site['name']
#                             )
#                             all_news.extend(filtered_news)
#                             print(f"‚úÖ Extracted {len(filtered_news)} important articles")
#                     else:
#                         print(f"‚ö†Ô∏è No articles found from {site['name']}")
                
#                 else:
#                     print(f"‚ùå Failed to fetch {site['name']}: {result.error_message}")
                    
#             except Exception as e:
#                 print(f"‚ùå Error scraping {site['name']}: {e}")
#                 import traceback
#                 traceback.print_exc()
    
#     return all_news


# async def smart_analyze_with_detailed_summary(articles, source_name):
#     """
#     AI analysis with token tracking ($1 per 1M tokens)
#     """
#     global total_tokens_used, total_cost
    
#     print(f"\nüß† Using AI for detailed analysis of {source_name} articles...")
    
#     all_filtered = []
    
#     # Process in batches of 5
#     for i in range(0, len(articles), 5):
#         batch = articles[i:i+5]
        
#         articles_text = ""
#         for idx, article in enumerate(batch, i+1):
#             articles_text += f"""
# ‡§¨‡§æ‡§§‡§Æ‡•Ä #{idx}:
# ‡§∂‡•Ä‡§∞‡•ç‡§∑‡§ï: {article['title']}
# Link: {article['link']}
# Content: {article['content'][:1200]}

# ---
# """
        
#         prompt = f"""
# ‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä ‡§è‡§ï ‡§§‡§ú‡•ç‡§û ‡§Æ‡§∞‡§æ‡§†‡•Ä ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§ï ‡§Ü‡§π‡§æ‡§§. ‡§ñ‡§æ‡§≤‡•Ä‡§≤ ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ‡§Ç‡§ö‡•á ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£ ‡§ï‡§∞‡§æ.

# **‡§´‡§ï‡•ç‡§§ ‡§π‡•á ‡§™‡•ç‡§∞‡§ï‡§æ‡§∞ ‡§®‡§ø‡§µ‡§°‡§æ:**
# 1. ‡§ó‡•Å‡§®‡•ç‡§π‡•á‡§ó‡§æ‡§∞‡•Ä ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ (Crime) - ‡§π‡§§‡•ç‡§Ø‡§æ, ‡§¶‡§∞‡•ã‡§°‡§æ, ‡§Ö‡§™‡§ò‡§æ‡§§, ‡§Ö‡§ü‡§ï, ‡§≤‡§æ‡§ö
# 2. ‡§∞‡§æ‡§ú‡§ï‡•Ä‡§Ø ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ (Political) - ‡§®‡§ø‡§µ‡§°‡§£‡•Å‡§ï‡§æ, ‡§∏‡§∞‡§ï‡§æ‡§∞, ‡§Æ‡§π‡§æ‡§™‡§æ‡§≤‡§ø‡§ï‡§æ, ‡§∞‡§æ‡§ú‡§ï‡•Ä‡§Ø ‡§ò‡§°‡§æ‡§Æ‡•ã‡§°‡•Ä
# 3. ‡§Æ‡§π‡§§‡•ç‡§§‡•ç‡§µ‡§æ‡§ö‡•ç‡§Ø‡§æ ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ (Important General) - ‡§∂‡§æ‡§∏‡§ï‡•Ä‡§Ø ‡§®‡§ø‡§∞‡•ç‡§£‡§Ø, ‡§∏‡§æ‡§Æ‡§æ‡§ú‡§ø‡§ï ‡§Æ‡•Å‡§¶‡•ç‡§¶‡•á

# **‡§ü‡§æ‡§≥‡§æ‡§µ‡•á:** ‡§Æ‡§®‡•ã‡§∞‡§Ç‡§ú‡§® gossip, ‡§ú‡•ç‡§Ø‡•ã‡§§‡§ø‡§∑, ‡§´‡•Ö‡§∂‡§®, lifestyle, ‡§ñ‡•á‡§≥‡§æ‡§ö‡•Ä ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§¨‡§æ‡§§‡§Æ‡•Ä, job posts, ‡§ß‡§æ‡§∞‡•ç‡§Æ‡§ø‡§ï ‡§ï‡§•‡§æ, Bigg Boss, ‡§¨‡•â‡§≤‡•Ä‡§µ‡•Ç‡§° gossip

# **JSON format (‡§´‡§ï‡•ç‡§§ array ‡§™‡§∞‡§§ ‡§ï‡§∞‡§æ):**
# [
#   {{
#     "title": "‡§Æ‡•Ç‡§≥ ‡§∂‡•Ä‡§∞‡•ç‡§∑‡§ï",
#     "category": "crime/politics/general",
#     "detailed_summary": "‡§∏‡§Ç‡§™‡•Ç‡§∞‡•ç‡§£ ‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§ ‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂ 150-250 ‡§∂‡§¨‡•ç‡§¶‡§æ‡§Ç‡§§ ‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§",
#     "importance": "high/medium/low",
#     "link": "URL",
#     "article_number": number,
#     "key_points": ["‡§Æ‡•Å‡§¶‡•ç‡§¶‡§æ 1", "‡§Æ‡•Å‡§¶‡•ç‡§¶‡§æ 2", "‡§Æ‡•Å‡§¶‡•ç‡§¶‡§æ 3"]
#   }}
# ]

# {articles_text}
# """
        
#         try:
#             response = perplexity_client.chat.completions.create(
#                 model="sonar-pro",
#                 messages=[
#                     {
#                         "role": "system",
#                         "content": "You are an expert Marathi news analyst. Return ONLY valid JSON array."
#                     },
#                     {
#                         "role": "user",
#                         "content": prompt
#                     }
#                 ],
#                 temperature=0.3,
#                 max_tokens=4000
#             )
            
#             # Track tokens
#             if hasattr(response, 'usage'):
#                 batch_tokens = response.usage.total_tokens
#                 total_tokens_used += batch_tokens
#                 batch_cost = (batch_tokens / 1_000_000) * 1.0
#                 total_cost += batch_cost
#                 print(f"   üìä Batch tokens: {batch_tokens:,} | Cost: ${batch_cost:.4f}")
            
#             content = response.choices[0].message.content
#             json_match = re.search(r'\[.*\]', content, re.DOTALL)
            
#             if json_match:
#                 batch_articles = json.loads(json_match.group())
#                 all_filtered.extend(batch_articles)
#                 print(f"   ‚úÖ Extracted {len(batch_articles)} articles")
            
#         except Exception as e:
#             print(f"   ‚ùå AI analysis error: {e}")
    
#     # Add source and timestamp
#     for article in all_filtered:
#         article['source'] = source_name
#         article['scraped_at'] = datetime.now().isoformat()
    
#     return all_filtered


# def create_reel_script(news_articles):
#     """
#     Generate Instagram Reel script from news articles
#     Returns: (script, source_link, news_title)
#     """
#     global total_tokens_used, total_cost
    
#     # Prepare news context
#     news_context = ""
#     for idx, article in enumerate(news_articles[:5], 1):
#         news_context += f"""
# ‡§¨‡§æ‡§§‡§Æ‡•Ä #{idx}:
# ‡§∂‡•Ä‡§∞‡•ç‡§∑‡§ï: {article['title']}
# ‡§™‡•ç‡§∞‡§ï‡§æ‡§∞: {article['category']}
# ‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂: {article['detailed_summary']}
# ‡§Æ‡§π‡§§‡•ç‡§§‡•ç‡§µ: {article['importance']}
# ‡§≤‡§ø‡§Ç‡§ï: {article['link']}
# ---
# """
    
#     # System prompt with diverse hook examples
#     system_prompt = """
# ‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä "‡§ú‡§¨‡§∞‡•Ä ‡§ñ‡§¨‡§∞‡•Ä" Instagram Reels ‡§ö‡•á ‡§è‡§ï‡•ç‡§∏‡§™‡§∞‡•ç‡§ü ‡§∏‡•ç‡§ï‡•ç‡§∞‡§ø‡§™‡•ç‡§ü ‡§∞‡§æ‡§Ø‡§ü‡§∞ ‡§Ü‡§π‡§æ‡§§.

# **HOOK VARIETY (‡§™‡§π‡§ø‡§≤‡•ç‡§Ø‡§æ 2 ‡§ì‡§≥‡•Ä):**
# 1. Shock Statement: "‡§è‡§ï‡§æ ‡§Ö‡§™‡§ò‡§æ‡§§‡§æ‡§®‡•á ‡§∏‡§Ç‡§™‡•Ç‡§∞‡•ç‡§£ ‡§∞‡§æ‡§ú‡•ç‡§Ø ‡§π‡§æ‡§¶‡§∞‡§≤‡§Ç."
# 2. Direct Question: "‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§≤‡§æ ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä ‡§Ü‡§π‡•á ‡§ï‡§æ...?"
# 3. Breaking News: "‡§®‡•Å‡§ï‡§§‡•Ä‡§ö ‡§è‡§ï ‡§ß‡§ï‡•ç‡§ï‡§æ‡§¶‡§æ‡§Ø‡§ï ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä ‡§∏‡§Æ‡•ã‡§∞ ‡§Ü‡§≤‡•Ä."
# 4. Name Drop: "[‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡•Ä ‡§®‡§æ‡§µ] ‡§Ü‡§ú ‡§ö‡§∞‡•ç‡§ö‡•á‡§§ ‡§ï‡§æ ‡§Ü‡§π‡•á?"
# 5. Contrast/Twist: "‡§¶‡§ø‡§∏‡§§‡§Ç ‡§ï‡§æ‡§π‡•Ä‡§§‡§∞‡•Ä, ‡§™‡§£ ‡§µ‡§æ‡§∏‡•ç‡§§‡§µ ‡§µ‡•á‡§ó‡§≥‡§Ç‡§ö."

# **‡§∏‡•ç‡§ü‡•ç‡§∞‡§ï‡•ç‡§ö‡§∞ (40-60 seconds):**
# - ‡§™‡§π‡§ø‡§≤‡•á 2 ‡§ì‡§≥‡•Ä: Hook (‡§µ‡•á‡§ó‡§≥‡•Ä style)
# - 3-10 ‡§ì‡§≥‡•Ä: ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§ò‡§ü‡§®‡§æ
# - 11-14 ‡§ì‡§≥‡•Ä: ‡§ü‡•ç‡§µ‡§ø‡§∏‡•ç‡§ü/‡§™‡•ç‡§∞‡§∂‡•ç‡§®
# - ‡§∂‡•á‡§µ‡§ü‡§ö‡•ç‡§Ø‡§æ 2-3 ‡§ì‡§≥‡•Ä: CTA

# **ENDING:**
# "‡§§‡•Å‡§Æ‡§ö‡§Ç ‡§ï‡§æ‡§Ø ‡§Æ‡§§ ‡§Ü‡§π‡•á? ‡§ï‡§Æ‡•á‡§Ç‡§ü ‡§ï‡§∞‡•Ç‡§® ‡§∏‡§æ‡§Ç‡§ó‡§æ ‡§Ü‡§£‡§ø ‡§´‡•â‡§≤‡•ã ‡§ï‡§∞‡§æ ‡§ú‡§¨‡§∞‡•Ä ‡§ñ‡§¨‡§∞‡•Ä."

# **OUTPUT:** ‡§´‡§ï‡•ç‡§§ ‡§∏‡•ç‡§ï‡•ç‡§∞‡§ø‡§™‡•ç‡§ü. 15-18 ‡§ì‡§≥‡•Ä. ‡§∂‡•á‡§µ‡§ü‡•Ä: ---ARTICLE_NUMBER: X---
# """
    
#     user_prompt = f"""
# ‡§ñ‡§æ‡§≤‡•Ä‡§≤ ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ‡§Ç‡§™‡•à‡§ï‡•Ä ‡§∏‡§∞‡•ç‡§µ‡§æ‡§§ ENGAGING ‡§¨‡§æ‡§§‡§Æ‡•Ä ‡§®‡§ø‡§µ‡§°‡•Ç‡§® Instagram Reel script ‡§§‡§Ø‡§æ‡§∞ ‡§ï‡§∞‡§æ.

# **‡§Ü‡§ú‡§ö‡•ç‡§Ø‡§æ ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ:**
# {news_context}

# **‡§§‡•Å‡§Æ‡§ö‡§Ç ‡§ï‡§æ‡§Æ:**
# 1. ‡§∏‡§∞‡•ç‡§µ‡§æ‡§§ STRONG ‡§¨‡§æ‡§§‡§Æ‡•Ä ‡§®‡§ø‡§µ‡§°‡§æ
# 2. ‡§Ø‡•ã‡§ó‡•ç‡§Ø hook style ‡§µ‡§æ‡§™‡§∞‡§æ
# 3. 15-18 ‡§ì‡§≥‡•Ä‡§Ç‡§ö‡•Ä script ‡§≤‡§ø‡§π‡§æ
# 4. ‡§∂‡•á‡§µ‡§ü‡•Ä article number ‡§¶‡•ç‡§Ø‡§æ

# OUTPUT: [‡§∏‡•ç‡§ï‡•ç‡§∞‡§ø‡§™‡•ç‡§ü]\n\n---ARTICLE_NUMBER: X---
# """
    
#     try:
#         response = perplexity_client.chat.completions.create(
#             model="sonar-pro",
#             messages=[
#                 {"role": "system", "content": system_prompt},
#                 {"role": "user", "content": user_prompt}
#             ],
#             temperature=0.8,
#             max_tokens=1500
#         )
        
#         # Track tokens
#         if hasattr(response, 'usage'):
#             script_tokens = response.usage.total_tokens
#             total_tokens_used += script_tokens
#             total_cost += (script_tokens / 1_000_000) * 1.0
        
#         full_response = response.choices[0].message.content.strip()
        
#         # Extract script and article number
#         if "---ARTICLE_NUMBER:" in full_response:
#             parts = full_response.split("---ARTICLE_NUMBER:")
#             script = parts[0].strip()
#             article_num_str = parts[1].strip().replace("---", "").strip()
#             try:
#                 article_num = int(article_num_str) - 1
#             except:
#                 article_num = 0
#         else:
#             script = full_response
#             article_num = 0
        
#         # Clean up
#         script = script.replace('```', '').strip()
#         script = script.replace('---ARTICLE_NUMBER:', '').strip()
        
#         # Get source article
#         if article_num < len(news_articles):
#             source_article = news_articles[article_num]
#         else:
#             source_article = news_articles
        
#         source_link = source_article.get('link', 'N/A')
#         news_title = source_article.get('title', 'N/A')
        
#         return script, source_link, news_title
        
#     except Exception as e:
#         print(f"‚ùå Error generating script: {e}")
#         return None, None, None


# async def main():
#     global total_tokens_used, total_cost
    
#     print("üöÄ Starting Smart Marathi News Scraper + Script Generator")
#     print("üìç Focus: Criminal, Political & Important General News")
#     print("üìù Feature: Detailed summaries + Instagram Scripts")
#     print("üí∞ Token tracking enabled")
#     print("üìä Output: Direct to Google Sheets\n")
    
#     start_time = datetime.now()
    
#     # PART 1: SCRAPING
#     all_articles = await scrape_marathi_news_final()
    
#     # Remove duplicates
#     unique_articles = []
#     seen_titles = set()
    
#     for article in all_articles:
#         title_lower = article['title'].lower()
#         if title_lower not in seen_titles:
#             unique_articles.append(article)
#             seen_titles.add(title_lower)
    
#     # Sort by importance
#     priority_order = {'high': 1, 'medium': 2, 'low': 3}
#     unique_articles.sort(key=lambda x: priority_order.get(x.get('importance', 'medium'), 2))
    
#     top_10_articles = unique_articles[:10]
    
#     end_scrape = datetime.now()
#     scrape_duration = (end_scrape - start_time).total_seconds()
    
#     # Scraping summary
#     print("\n" + "="*80)
#     print("üìä SCRAPING SUMMARY")
#     print("="*80)
#     print(f"   Total articles: {len(unique_articles)}")
#     print(f"   High importance: {len([a for a in unique_articles if a.get('importance') == 'high'])}")
#     print(f"   Crime: {len([a for a in unique_articles if a.get('category') == 'crime'])}")
#     print(f"   Political: {len([a for a in unique_articles if a.get('category') == 'politics'])}")
#     print(f"   General: {len([a for a in unique_articles if a.get('category') == 'general'])}")
#     print(f"\n‚è±Ô∏è  Scraping time: {scrape_duration:.2f} seconds")
#     print("="*80 + "\n")
    
#     # PART 2: SCRIPT GENERATION
#     print("="*80)
#     print("üé¨ GENERATING INSTAGRAM SCRIPT")
#     print("="*80 + "\n")
    
#     worksheet = setup_google_sheets()
    
#     if worksheet and len(top_10_articles) > 0:
#         print(f"\nüéØ Generating script from TOP 10 articles...\n")
        
#         script, source_link, news_title = create_reel_script(top_10_articles)
        
#         if script:
#             print("\n" + "="*70)
#             print("üìù GENERATED SCRIPT:")
#             print("="*70)
#             print(script)
#             print("\n" + "="*70)
#             print(f"üì∞ Title: {news_title}")
#             print(f"üîó Source: {source_link}")
#             print("="*70 + "\n")
            
#             success = save_to_google_sheets(worksheet, script, source_link, news_title)
            
#             if success:
#                 print(f"üìà View sheet: https://docs.google.com/spreadsheets/d/{worksheet.spreadsheet.id}")
#         else:
#             print("‚ùå Failed to generate script")
#     else:
#         print("‚ö†Ô∏è No articles or Google Sheets unavailable")
    
#     end_time = datetime.now()
#     total_duration = (end_time - start_time).total_seconds()
    
#     # Final summary
#     print("\n" + "="*80)
#     print("‚úÖ COMPLETE!")
#     print("="*80)
#     print(f"   Total articles: {len(unique_articles)}")
#     print(f"   Scripts generated: 1")
#     print(f"   Saved to: {GOOGLE_SHEET_NAME}")
#     print(f"\n   ‚è±Ô∏è Total time: {total_duration:.2f} seconds")
#     print(f"   üî¢ Total tokens: {total_tokens_used:,}")
#     print(f"   üí∞ Total cost: ${total_cost:.4f}")
#     print("="*80 + "\n")


# if __name__ == "__main__":
#     asyncio.run(main())




# import asyncio
# import json
# from datetime import datetime, date
# from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode
# from bs4 import BeautifulSoup
# from openai import OpenAI
# import re
# import gspread
# from google.oauth2.service_account import Credentials
# import os
# from typing import List, Dict
# import hashlib


# # Initialize Perplexity client from environment variable
# perplexity_client = OpenAI(
#     api_key=os.environ.get("PERPLEXITY_API_KEY"),  # Read from GitHub Secrets
#     base_url="https://api.perplexity.ai"
# )


# # Google Sheets Configuration
# GOOGLE_SHEETS_CREDENTIALS_FILE = "credentials.json"  # Created by GitHub Actions
# GOOGLE_SHEET_NAME = "Instagram Scripts"
# GOOGLE_WORKSHEET_NAME = "Scripts"


# # Categories
# VALID_CATEGORIES = [
#     "sports", "general", "crime", "politics", 
#     "education", "economy", "entertainment", "horror"
# ]


# # Track token usage and costs
# total_tokens_used = 0
# total_cost = 0.0
# processed_hashes = set()  # To avoid duplicate news


# def setup_google_sheets():
#     """Initialize Google Sheets connection"""
#     try:
#         scope = [
#             'https://spreadsheets.google.com/feeds',
#             'https://www.googleapis.com/auth/drive'
#         ]
        
#         creds = Credentials.from_service_account_file(
#             GOOGLE_SHEETS_CREDENTIALS_FILE, 
#             scopes=scope
#         )
        
#         client = gspread.authorize(creds)
        
#         try:
#             sheet = client.open(GOOGLE_SHEET_NAME)
#             print(f"‚úÖ Connected to existing sheet: '{GOOGLE_SHEET_NAME}'")
#         except gspread.SpreadsheetNotFound:
#             sheet = client.create(GOOGLE_SHEET_NAME)
#             print(f"‚úÖ Created new sheet: '{GOOGLE_SHEET_NAME}'")
        
#         try:
#             worksheet = sheet.worksheet(GOOGLE_WORKSHEET_NAME)
#             print(f"‚úÖ Using worksheet: '{GOOGLE_WORKSHEET_NAME}'")
#         except gspread.WorksheetNotFound:
#             worksheet = sheet.add_worksheet(
#                 title=GOOGLE_WORKSHEET_NAME,
#                 rows=2000,
#                 cols=10
#             )
#             # Add headers (5 columns: Timestamp, Category, Title, Script, Source Link)
#             worksheet.update('A1:E1', [[
#                 'Timestamp',
#                 'Category',
#                 'Title',
#                 'Script',
#                 'Source Link'
#             ]])
            
#             # Format headers
#             worksheet.format('A1:E1', {
#                 'textFormat': {
#                     'bold': True,
#                     'foregroundColor': {'red': 1.0, 'green': 1.0, 'blue': 1.0}
#                 },
#                 'backgroundColor': {'red': 0.2, 'green': 0.6, 'blue': 0.9},
#                 'horizontalAlignment': 'CENTER'
#             })
            
#             # Set column widths
#             worksheet.set_column_width('A', 180)   # Timestamp
#             worksheet.set_column_width('B', 150)   # Category
#             worksheet.set_column_width('C', 400)   # Title
#             worksheet.set_column_width('D', 600)   # Script
#             worksheet.set_column_width('E', 400)   # Source Link
            
#             print(f"‚úÖ Created new worksheet with headers")
        
#         return worksheet
        
#     except FileNotFoundError:
#         print(f"‚ùå Error: '{GOOGLE_SHEETS_CREDENTIALS_FILE}' not found!")
#         print("üí° This file is created automatically by GitHub Actions")
#         return None
#     except Exception as e:
#         print(f"‚ùå Google Sheets setup error: {e}")
#         import traceback
#         traceback.print_exc()
#         return None


# def save_to_google_sheets(worksheet, category, title, script, source_link):
#     """Save script to Google Sheets with category"""
#     try:
#         timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
#         # Clean data
#         if isinstance(script, list):
#             script = '\n'.join(str(item) for item in script)
#         else:
#             script = str(script).strip()
        
#         script = script.replace('[', '').replace(']', '')
#         title = str(title).strip()
#         source_link = str(source_link).strip()
#         category = str(category).strip().lower()
        
#         # Validate category
#         if category not in VALID_CATEGORIES:
#             category = "general"
        
#         row_data = [timestamp, category, title, script, source_link]
        
#         next_row = len(worksheet.get_all_values()) + 1
#         worksheet.append_row(row_data, value_input_option='RAW')
        
#         # Format the row
#         row_range = f'A{next_row}:E{next_row}'
#         worksheet.format(row_range, {
#             'textFormat': {
#                 'foregroundColor': {'red': 0.0, 'green': 0.0, 'blue': 0.0},
#                 'fontSize': 10
#             },
#             'backgroundColor': {'red': 1.0, 'green': 1.0, 'blue': 1.0},
#             'wrapStrategy': 'WRAP',
#             'verticalAlignment': 'TOP'
#         })
        
#         # Format category column with color coding
#         category_colors = {
#             'crime': {'red': 0.95, 'green': 0.8, 'blue': 0.8},
#             'politics': {'red': 0.8, 'green': 0.9, 'blue': 1.0},
#             'sports': {'red': 0.8, 'green': 1.0, 'blue': 0.8},
#             'entertainment': {'red': 1.0, 'green': 0.9, 'blue': 0.8},
#             'education': {'red': 0.9, 'green': 0.95, 'blue': 1.0},
#             'economy': {'red': 0.95, 'green': 1.0, 'blue': 0.85},
#             'horror': {'red': 0.7, 'green': 0.7, 'blue': 0.7},
#             'general': {'red': 1.0, 'green': 1.0, 'blue': 0.9}
#         }
        
#         worksheet.format(f'B{next_row}', {
#             'textFormat': {
#                 'bold': True,
#                 'foregroundColor': {'red': 0.0, 'green': 0.0, 'blue': 0.0},
#                 'fontSize': 10
#             },
#             'backgroundColor': category_colors.get(category, category_colors['general']),
#             'horizontalAlignment': 'CENTER'
#         })
        
#         print(f"‚úÖ Saved [{category.upper()}] {title[:50]}...")
#         return True
        
#     except Exception as e:
#         print(f"‚ùå Error saving to Google Sheets: {e}")
#         return False


# def get_content_hash(title: str, content: str) -> str:
#     """Generate hash to detect duplicate news"""
#     combined = f"{title.lower()}{content[:200].lower()}"
#     return hashlib.md5(combined.encode()).hexdigest()


# async def scrape_multiple_marathi_sources():
#     """Scrape from multiple trusted Marathi news sources"""
    
#     today = date.today()
#     today_str = today.strftime('%Y-%m-%d')
    
#     news_sites = [
#         {
#             "name": "TV9 Marathi",
#             "url": "https://www.tv9marathi.com/latest-news",
#             "article_selector": "article, div.story-card",
#             "link_pattern": "tv9marathi.com",
#             "target": 10
#         },
#         {
#             "name": "ABP Majha",
#             "url": "https://marathi.abplive.com/news",
#             "article_selector": "article, div.story-box",
#             "link_pattern": "abplive.com",
#             "target": 10
#         },
#         {
#             "name": "Lokmat",
#             "url": "https://www.lokmat.com/latestnews/",
#             "article_selector": "article, div.story-card",
#             "link_pattern": "lokmat.com",
#             "target": 10
#         },
#         {
#             "name": "Maharashtra Times",
#             "url": "https://maharashtratimes.com/",
#             "article_selector": "article, div.brief-story",
#             "link_pattern": "maharashtratimes.com",
#             "target": 8
#         },
#         {
#             "name": "NDTV Marathi",
#             "url": "https://marathi.ndtv.com/",
#             "article_selector": "article, div.news_Itm",
#             "link_pattern": "marathi.ndtv.com",
#             "target": 8
#         },
#         {
#             "name": "Zee 24 Taas",
#             "url": "https://zeenews.india.com/marathi/",
#             "article_selector": "article, div.story",
#             "link_pattern": "zeenews.india.com/marathi",
#             "target": 8
#         },
#         {
#             "name": "Loksatta",
#             "url": "https://www.loksatta.com/",
#             "article_selector": "article, div.item",
#             "link_pattern": "loksatta.com",
#             "target": 6
#         }
#     ]
    
#     all_news = []
    
#     async with AsyncWebCrawler(verbose=False) as crawler:
        
#         for site in news_sites:
#             print(f"\nüîç Scraping {site['name']}...")
            
#             try:
#                 config = CrawlerRunConfig(
#                     cache_mode=CacheMode.BYPASS,
#                     wait_for="body",
#                     word_count_threshold=10,
#                     page_timeout=30000,
#                     js_code="await new Promise(r => setTimeout(r, 2000));"
#                 )
                
#                 result = await crawler.arun(site['url'], config=config)
                
#                 if result.success:
#                     soup = BeautifulSoup(result.html, 'html.parser')
                    
#                     raw_articles = []
#                     all_links = soup.find_all('a', href=True)
                    
#                     for link_tag in all_links:
#                         href = link_tag.get('href', '')
#                         title = link_tag.get_text(strip=True)
                        
#                         if (len(title) > 15 and len(title) < 300 and
#                             site['link_pattern'] in href and
#                             not any(x in href.lower() for x in [
#                                 'javascript:', 'mailto:', '#', 
#                                 '/category/', '/tag/', '/author/',
#                                 'facebook.com', 'twitter.com', 'instagram.com',
#                                 'youtube.com', 'whatsapp.com', '/myaccount/',
#                                 '/install_app', '/advertisement', '/epaper',
#                                 'web-stories', 'photo-gallery', '/videos/',
#                                 '/games/', '/jokes/', '/terms-and-conditions',
#                                 '/topic/', '/widget/'
#                             ])):
                            
#                             if href.startswith('/'):
#                                 base_url = site['url'].split('/')[0] + '//' + site['url'].split('/')[2]
#                                 href = base_url + href
                            
#                             if href.startswith('http'):
#                                 raw_articles.append({
#                                     'title': title,
#                                     'link': href
#                                 })
                    
#                     # Remove duplicates
#                     seen_links = set()
#                     unique_articles = []
#                     for article in raw_articles:
#                         if article['link'] not in seen_links:
#                             unique_articles.append(article)
#                             seen_links.add(article['link'])
                    
#                     print(f"üìã Found {len(unique_articles)} unique articles")
                    
#                     if len(unique_articles) > 0:
#                         articles_with_content = []
                        
#                         for article in unique_articles[:site['target']]:
#                             try:
#                                 article_result = await crawler.arun(
#                                     article['link'],
#                                     config=CrawlerRunConfig(
#                                         cache_mode=CacheMode.BYPASS,
#                                         word_count_threshold=50,
#                                         page_timeout=15000
#                                     )
#                                 )
                                
#                                 if article_result.success and len(article_result.markdown) > 100:
#                                     content_hash = get_content_hash(article['title'], article_result.markdown)
                                    
#                                     if content_hash not in processed_hashes:
#                                         articles_with_content.append({
#                                             'title': article['title'],
#                                             'link': article['link'],
#                                             'content': article_result.markdown[:2500],
#                                             'hash': content_hash
#                                         })
#                                         processed_hashes.add(content_hash)
#                                         print(f"   ‚úì {article['title'][:60]}...")
                                    
#                             except Exception as e:
#                                 continue
                        
#                         print(f"‚úÖ Fetched {len(articles_with_content)} articles")
                        
#                         if articles_with_content:
#                             filtered_news = await smart_analyze_with_category(
#                                 articles_with_content, 
#                                 site['name']
#                             )
#                             all_news.extend(filtered_news)
                
#                 else:
#                     print(f"‚ùå Failed to fetch {site['name']}")
                    
#             except Exception as e:
#                 print(f"‚ùå Error scraping {site['name']}: {e}")
            
#             await asyncio.sleep(2)
    
#     return all_news


# async def smart_analyze_with_category(articles: List[Dict], source_name: str):
#     """AI analysis with proper categorization"""
#     global total_tokens_used, total_cost
    
#     print(f"\nüß† Analyzing {source_name} articles...")
    
#     all_filtered = []
    
#     for i in range(0, len(articles), 3):
#         batch = articles[i:i+3]
        
#         articles_text = ""
#         for idx, article in enumerate(batch, i+1):
#             articles_text += f"""
# ‡§¨‡§æ‡§§‡§Æ‡•Ä #{idx}:
# ‡§∂‡•Ä‡§∞‡•ç‡§∑‡§ï: {article['title']}
# Link: {article['link']}
# Content: {article['content'][:1200]}

# ---
# """
        
#         prompt = f"""
# ‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä ‡§è‡§ï ‡§§‡§ú‡•ç‡§û ‡§Æ‡§∞‡§æ‡§†‡•Ä ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§ï ‡§Ü‡§π‡§æ‡§§. ‡§ñ‡§æ‡§≤‡•Ä‡§≤ ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ‡§Ç‡§ö‡•á ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£ ‡§ï‡§∞‡§æ ‡§Ü‡§£‡§ø ‡§™‡•ç‡§∞‡§§‡•ç‡§Ø‡•á‡§ï ‡§¨‡§æ‡§§‡§Æ‡•Ä‡§≤‡§æ ‡§Ø‡•ã‡§ó‡•ç‡§Ø category ‡§¶‡•ç‡§Ø‡§æ.

# **Categories (‡§´‡§ï‡•ç‡§§ ‡§Ø‡§æ‡§™‡•à‡§ï‡•Ä ‡§è‡§ï ‡§®‡§ø‡§µ‡§°‡§æ):**
# 1. sports - ‡§ï‡•ç‡§∞‡•Ä‡§°‡§æ ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ
# 2. general - ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§Æ‡§π‡§§‡•ç‡§§‡•ç‡§µ‡§æ‡§ö‡•ç‡§Ø‡§æ ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ
# 3. crime - ‡§ó‡•Å‡§®‡•ç‡§π‡•á‡§ó‡§æ‡§∞‡•Ä ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ
# 4. politics - ‡§∞‡§æ‡§ú‡§ï‡•Ä‡§Ø ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ
# 5. education - ‡§∂‡•à‡§ï‡•ç‡§∑‡§£‡§ø‡§ï ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ
# 6. economy - ‡§Ü‡§∞‡•ç‡§•‡§ø‡§ï/‡§µ‡•ç‡§Ø‡§µ‡§∏‡§æ‡§Ø ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ
# 7. entertainment - ‡§Æ‡§®‡•ã‡§∞‡§Ç‡§ú‡§® ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ
# 8. horror - ‡§≠‡§Ø‡§æ‡§®‡§ï/‡§¶‡•Å‡§É‡§ñ‡§¶ ‡§ò‡§ü‡§®‡§æ

# **JSON format:**
# [
#   {{
#     "title": "‡§Æ‡•Ç‡§≥ ‡§∂‡•Ä‡§∞‡•ç‡§∑‡§ï",
#     "category": "category name",
#     "detailed_summary": "‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§ ‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂ 150-250 ‡§∂‡§¨‡•ç‡§¶‡§æ‡§Ç‡§§",
#     "importance": "high/medium/low",
#     "link": "URL",
#     "article_number": number,
#     "key_points": ["‡§Æ‡•Å‡§¶‡•ç‡§¶‡§æ 1", "‡§Æ‡•Å‡§¶‡•ç‡§¶‡§æ 2", "‡§Æ‡•Å‡§¶‡•ç‡§¶‡§æ 3"]
#   }}
# ]

# {articles_text}
# """
        
#         try:
#             response = perplexity_client.chat.completions.create(
#                 model="sonar-pro",
#                 messages=[
#                     {
#                         "role": "system",
#                         "content": "You are an expert Marathi news analyst. Return ONLY valid JSON array."
#                     },
#                     {
#                         "role": "user",
#                         "content": prompt
#                     }
#                 ],
#                 temperature=0.3,
#                 max_tokens=3000
#             )
            
#             if hasattr(response, 'usage'):
#                 batch_tokens = response.usage.total_tokens
#                 total_tokens_used += batch_tokens
#                 batch_cost = (batch_tokens / 1_000_000) * 1.0
#                 total_cost += batch_cost
            
#             content = response.choices[0].message.content
#             json_match = re.search(r'\[.*\]', content, re.DOTALL)
            
#             if json_match:
#                 batch_articles = json.loads(json_match.group())
#                 all_filtered.extend(batch_articles)
#                 print(f"   ‚úÖ Extracted {len(batch_articles)} articles")
            
#         except Exception as e:
#             print(f"   ‚ùå AI analysis error: {e}")
        
#         await asyncio.sleep(1)
    
#     for article in all_filtered:
#         article['source'] = source_name
#         article['scraped_at'] = datetime.now().isoformat()
    
#     return all_filtered


# async def create_reel_script_single(news_article: Dict):
#     """Generate Instagram Reel script for a SINGLE news article"""
#     global total_tokens_used, total_cost
    
#     category = news_article.get('category', 'general')
    
#     system_prompt = """
# ‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä "‡§ú‡§¨‡§∞‡•Ä ‡§ñ‡§¨‡§∞‡•Ä" Instagram Reels ‡§ö‡•á ‡§è‡§ï‡•ç‡§∏‡§™‡§∞‡•ç‡§ü ‡§∏‡•ç‡§ï‡•ç‡§∞‡§ø‡§™‡•ç‡§ü ‡§∞‡§æ‡§Ø‡§ü‡§∞ ‡§Ü‡§π‡§æ‡§§.

# **HOOK VARIETY (‡§™‡§π‡§ø‡§≤‡•ç‡§Ø‡§æ 2 ‡§ì‡§≥‡•Ä):**
# 1. Shock Statement: "‡§è‡§ï‡§æ ‡§Ö‡§™‡§ò‡§æ‡§§‡§æ‡§®‡•á ‡§∏‡§Ç‡§™‡•Ç‡§∞‡•ç‡§£ ‡§∞‡§æ‡§ú‡•ç‡§Ø ‡§π‡§æ‡§¶‡§∞‡§≤‡§Ç."
# 2. Direct Question: "‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§≤‡§æ ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä ‡§Ü‡§π‡•á ‡§ï‡§æ...?"
# 3. Breaking News: "‡§®‡•Å‡§ï‡§§‡•Ä‡§ö ‡§è‡§ï ‡§ß‡§ï‡•ç‡§ï‡§æ‡§¶‡§æ‡§Ø‡§ï ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä ‡§∏‡§Æ‡•ã‡§∞ ‡§Ü‡§≤‡•Ä."
# 4. Name Drop: "[‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡•Ä ‡§®‡§æ‡§µ] ‡§Ü‡§ú ‡§ö‡§∞‡•ç‡§ö‡•á‡§§ ‡§ï‡§æ ‡§Ü‡§π‡•á?"
# 5. Contrast/Twist: "‡§¶‡§ø‡§∏‡§§‡§Ç ‡§ï‡§æ‡§π‡•Ä‡§§‡§∞‡•Ä, ‡§™‡§£ ‡§µ‡§æ‡§∏‡•ç‡§§‡§µ ‡§µ‡•á‡§ó‡§≥‡§Ç‡§ö."

# **‡§∏‡•ç‡§ü‡•ç‡§∞‡§ï‡•ç‡§ö‡§∞ (15-18 ‡§ì‡§≥‡•Ä):**
# - ‡§™‡§π‡§ø‡§≤‡•á 2 ‡§ì‡§≥‡•Ä: Hook
# - 3-10 ‡§ì‡§≥‡•Ä: ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§ò‡§ü‡§®‡§æ
# - 11-14 ‡§ì‡§≥‡•Ä: ‡§ü‡•ç‡§µ‡§ø‡§∏‡•ç‡§ü/‡§™‡•ç‡§∞‡§∂‡•ç‡§®
# - ‡§∂‡•á‡§µ‡§ü‡§ö‡•ç‡§Ø‡§æ 2-3 ‡§ì‡§≥‡•Ä: CTA

# **ENDING:**
# "‡§§‡•Å‡§Æ‡§ö‡§Ç ‡§ï‡§æ‡§Ø ‡§Æ‡§§ ‡§Ü‡§π‡•á? ‡§ï‡§Æ‡•á‡§Ç‡§ü ‡§ï‡§∞‡•Ç‡§® ‡§∏‡§æ‡§Ç‡§ó‡§æ ‡§Ü‡§£‡§ø ‡§´‡•â‡§≤‡•ã ‡§ï‡§∞‡§æ ‡§ú‡§¨‡§∞‡•Ä ‡§ñ‡§¨‡§∞‡•Ä."

# OUTPUT: ‡§´‡§ï‡•ç‡§§ ‡§∏‡•ç‡§ï‡•ç‡§∞‡§ø‡§™‡•ç‡§ü, 15-18 ‡§ì‡§≥‡•Ä
# """
    
#     user_prompt = f"""
# ‡§ñ‡§æ‡§≤‡•Ä‡§≤ ‡§¨‡§æ‡§§‡§Æ‡•Ä‡§µ‡§∞ Instagram Reel script ‡§§‡§Ø‡§æ‡§∞ ‡§ï‡§∞‡§æ.

# **‡§¨‡§æ‡§§‡§Æ‡•Ä:**
# ‡§∂‡•Ä‡§∞‡•ç‡§∑‡§ï: {news_article['title']}
# Category: {category}
# ‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂: {news_article['detailed_summary']}
# ‡§Æ‡•Å‡§¶‡•ç‡§¶‡•á: {', '.join(news_article.get('key_points', []))}

# ‡§´‡§ï‡•ç‡§§ ‡§∏‡•ç‡§ï‡•ç‡§∞‡§ø‡§™‡•ç‡§ü ‡§¶‡•ç‡§Ø‡§æ, 15-18 ‡§ì‡§≥‡•Ä.
# """
    
#     try:
#         response = perplexity_client.chat.completions.create(
#             model="sonar-pro",
#             messages=[
#                 {"role": "system", "content": system_prompt},
#                 {"role": "user", "content": user_prompt}
#             ],
#             temperature=0.8,
#             max_tokens=1500
#         )
        
#         if hasattr(response, 'usage'):
#             script_tokens = response.usage.total_tokens
#             total_tokens_used += script_tokens
#             total_cost += (script_tokens / 1_000_000) * 1.0
        
#         script = response.choices[0].message.content.strip()
#         script = script.replace('```', '').strip()
        
#         return script
        
#     except Exception as e:
#         print(f"‚ùå Error generating script: {e}")
#         return None


# async def main():
#     global total_tokens_used, total_cost
    
#     print("="*80)
#     print("üöÄ SMART MARATHI NEWS SCRAPER + SCRIPT GENERATOR v2.0")
#     print("="*80)
#     print("üìç Target: 50+ different news articles")
#     print("üìã Categories: Sports, General, Crime, Politics, Education, Economy, Entertainment, Horror")
#     print("üé¨ Output: Individual scripts for each news")
#     print("üíæ Storage: Google Sheets with category column")
#     print("="*80 + "\n")
    
#     start_time = datetime.now()
    
#     # STEP 1: SCRAPING
#     print("\n" + "="*80)
#     print("STEP 1: SCRAPING NEWS FROM MULTIPLE SOURCES")
#     print("="*80 + "\n")
    
#     all_articles = await scrape_multiple_marathi_sources()
    
#     # Remove duplicates
#     unique_articles = []
#     seen_hashes = set()
    
#     for article in all_articles:
#         article_hash = article.get('hash', get_content_hash(article['title'], article.get('detailed_summary', '')))
#         if article_hash not in seen_hashes:
#             unique_articles.append(article)
#             seen_hashes.add(article_hash)
    
#     print(f"\n‚úÖ Total unique articles: {len(unique_articles)}")
    
#     # Category breakdown
#     category_counts = {}
#     for article in unique_articles:
#         cat = article.get('category', 'general')
#         category_counts[cat] = category_counts.get(cat, 0) + 1
    
#     print("\nüìä Category Breakdown:")
#     for cat, count in sorted(category_counts.items()):
#         print(f"   {cat.upper()}: {count}")
    
#     # Select top 50 articles
#     priority_order = {'high': 1, 'medium': 2, 'low': 3}
#     unique_articles.sort(key=lambda x: priority_order.get(x.get('importance', 'medium'), 2))
    
#     selected_articles = unique_articles[:50]
    
#     print(f"\nüéØ Selected {len(selected_articles)} articles for scripts")
    
#     end_scrape = datetime.now()
#     scrape_duration = (end_scrape - start_time).total_seconds()
#     print(f"‚è±Ô∏è  Scraping: {scrape_duration:.2f} seconds\n")
    
#     # STEP 2: SCRIPT GENERATION
#     print("="*80)
#     print("STEP 2: GENERATING SCRIPTS & SAVING TO GOOGLE SHEETS")
#     print("="*80 + "\n")
    
#     worksheet = setup_google_sheets()
    
#     if worksheet and len(selected_articles) > 0:
#         successful_saves = 0
#         failed_saves = 0
        
#         for idx, article in enumerate(selected_articles, 1):
#             print(f"\n[{idx}/{len(selected_articles)}] {article['title'][:60]}...")
            
#             script = await create_reel_script_single(article)
            
#             if script:
#                 success = save_to_google_sheets(
#                     worksheet,
#                     article.get('category', 'general'),
#                     article['title'],
#                     script,
#                     article['link']
#                 )
                
#                 if success:
#                     successful_saves += 1
#                 else:
#                     failed_saves += 1
#             else:
#                 failed_saves += 1
            
#             await asyncio.sleep(1.5)
        
#         print("\n" + "="*80)
#         print("‚úÖ COMPLETE!")
#         print("="*80)
#         print(f"   Successfully saved: {successful_saves}")
#         print(f"   Failed: {failed_saves}")
#         print(f"   üìä View: https://docs.google.com/spreadsheets/d/{worksheet.spreadsheet.id}")
#     else:
#         print("‚ö†Ô∏è No articles or Google Sheets unavailable")
    
#     end_time = datetime.now()
#     total_duration = (end_time - start_time).total_seconds()
    
#     print("\n" + "="*80)
#     print("üìà SUMMARY")
#     print("="*80)
#     print(f"   Articles scraped: {len(unique_articles)}")
#     print(f"   Scripts generated: {successful_saves}")
#     print(f"   Time: {total_duration:.2f} seconds")
#     print(f"   Tokens: {total_tokens_used:,}")
#     print(f"   Cost: ${total_cost:.4f}")
#     print("="*80 + "\n")


# if __name__ == "__main__":
#     asyncio.run(main())






# import asyncio
# import json
# from datetime import datetime, date
# from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode
# from bs4 import BeautifulSoup
# from openai import OpenAI
# import re
# import gspread
# from google.oauth2.service_account import Credentials
# import os
# from typing import List, Dict
# import hashlib


# # ============================================================
# # Initialize Perplexity client - sonar-reasoning-pro
# # ============================================================
# perplexity_client = OpenAI(
#     api_key=os.environ.get("PERPLEXITY_API_KEY"),
#     base_url="https://api.perplexity.ai"
# )

# MODEL_NAME = "sonar-reasoning-pro"  # Updated model
# COST_PER_INPUT_TOKEN = 2.0 / 1_000_000   # $2 per 1M input tokens
# COST_PER_OUTPUT_TOKEN = 8.0 / 1_000_000  # $8 per 1M output tokens


# # ============================================================
# # Google Sheets Configuration
# # ============================================================
# GOOGLE_SHEETS_CREDENTIALS_FILE = "credentials.json"
# GOOGLE_SHEET_NAME = "Instagram Scripts"
# GOOGLE_WORKSHEET_NAME = "Scripts"


# # ============================================================
# # Categories
# # ============================================================
# VALID_CATEGORIES = [
#     "sports", "general", "crime", "politics",
#     "education", "economy", "entertainment", "horror"
# ]


# # ============================================================
# # Token tracking
# # ============================================================
# total_input_tokens = 0
# total_output_tokens = 0
# total_cost = 0.0
# processed_hashes = set()


# # ============================================================
# # 5 News Sites - 10 articles each = 50 total
# # ============================================================
# NEWS_SITES = [
#     {
#         "name": "TV9 Marathi",
#         "url": "https://www.tv9marathi.com/latest-news",
#         "link_pattern": "tv9marathi.com",
#         "target": 10
#     },
#     {
#         "name": "ABP Majha",
#         "url": "https://marathi.abplive.com/news",
#         "link_pattern": "abplive.com",
#         "target": 10
#     },
#     {
#         "name": "Lokmat",
#         "url": "https://www.lokmat.com/latestnews/",
#         "link_pattern": "lokmat.com",
#         "target": 10
#     },
#     {
#         "name": "Maharashtra Times",
#         "url": "https://maharashtratimes.com/",
#         "link_pattern": "maharashtratimes.com",
#         "target": 10
#     },
#     {
#         "name": "NDTV Marathi",
#         "url": "https://marathi.ndtv.com/",
#         "link_pattern": "marathi.ndtv.com",
#         "target": 10
#     }
# ]


# # ============================================================
# # Google Sheets Setup
# # ============================================================
# def setup_google_sheets():
#     """Initialize Google Sheets connection"""
#     try:
#         scope = [
#             'https://spreadsheets.google.com/feeds',
#             'https://www.googleapis.com/auth/drive'
#         ]

#         creds = Credentials.from_service_account_file(
#             GOOGLE_SHEETS_CREDENTIALS_FILE,
#             scopes=scope
#         )

#         client = gspread.authorize(creds)

#         try:
#             sheet = client.open(GOOGLE_SHEET_NAME)
#             print(f"‚úÖ Connected to existing sheet: '{GOOGLE_SHEET_NAME}'")
#         except gspread.SpreadsheetNotFound:
#             sheet = client.create(GOOGLE_SHEET_NAME)
#             print(f"‚úÖ Created new sheet: '{GOOGLE_SHEET_NAME}'")

#         try:
#             worksheet = sheet.worksheet(GOOGLE_WORKSHEET_NAME)
#             print(f"‚úÖ Using worksheet: '{GOOGLE_WORKSHEET_NAME}'")
#         except gspread.WorksheetNotFound:
#             worksheet = sheet.add_worksheet(
#                 title=GOOGLE_WORKSHEET_NAME,
#                 rows=5000,
#                 cols=10
#             )
#             worksheet.update('A1:E1', [[
#                 'Timestamp', 'Category', 'Title', 'Script', 'Source Link'
#             ]])
#             worksheet.format('A1:E1', {
#                 'textFormat': {
#                     'bold': True,
#                     'foregroundColor': {'red': 1.0, 'green': 1.0, 'blue': 1.0}
#                 },
#                 'backgroundColor': {'red': 0.2, 'green': 0.6, 'blue': 0.9},
#                 'horizontalAlignment': 'CENTER'
#             })
#             worksheet.set_column_width('A', 180)
#             worksheet.set_column_width('B', 150)
#             worksheet.set_column_width('C', 400)
#             worksheet.set_column_width('D', 600)
#             worksheet.set_column_width('E', 400)
#             print(f"‚úÖ Created new worksheet with headers")

#         return worksheet

#     except FileNotFoundError:
#         print(f"‚ùå Error: '{GOOGLE_SHEETS_CREDENTIALS_FILE}' not found!")
#         return None
#     except Exception as e:
#         print(f"‚ùå Google Sheets setup error: {e}")
#         import traceback
#         traceback.print_exc()
#         return None


# # ============================================================
# # Save to Google Sheets
# # ============================================================
# def save_to_google_sheets(worksheet, category, title, script, source_link):
#     """Save script to Google Sheets"""
#     try:
#         timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

#         if isinstance(script, list):
#             script = '\n'.join(str(item) for item in script)
#         else:
#             script = str(script).strip()

#         script = script.replace('[', '').replace(']', '')
#         title = str(title).strip()
#         source_link = str(source_link).strip()
#         category = str(category).strip().lower()

#         if category not in VALID_CATEGORIES:
#             category = "general"

#         row_data = [timestamp, category, title, script, source_link]

#         next_row = len(worksheet.get_all_values()) + 1
#         worksheet.append_row(row_data, value_input_option='RAW')

#         worksheet.format(f'A{next_row}:E{next_row}', {
#             'textFormat': {
#                 'foregroundColor': {'red': 0.0, 'green': 0.0, 'blue': 0.0},
#                 'fontSize': 10
#             },
#             'backgroundColor': {'red': 1.0, 'green': 1.0, 'blue': 1.0},
#             'wrapStrategy': 'WRAP',
#             'verticalAlignment': 'TOP'
#         })

#         category_colors = {
#             'crime':         {'red': 0.95, 'green': 0.8,  'blue': 0.8},
#             'politics':      {'red': 0.8,  'green': 0.9,  'blue': 1.0},
#             'sports':        {'red': 0.8,  'green': 1.0,  'blue': 0.8},
#             'entertainment': {'red': 1.0,  'green': 0.9,  'blue': 0.8},
#             'education':     {'red': 0.9,  'green': 0.95, 'blue': 1.0},
#             'economy':       {'red': 0.95, 'green': 1.0,  'blue': 0.85},
#             'horror':        {'red': 0.7,  'green': 0.7,  'blue': 0.7},
#             'general':       {'red': 1.0,  'green': 1.0,  'blue': 0.9}
#         }

#         worksheet.format(f'B{next_row}', {
#             'textFormat': {
#                 'bold': True,
#                 'foregroundColor': {'red': 0.0, 'green': 0.0, 'blue': 0.0},
#                 'fontSize': 10
#             },
#             'backgroundColor': category_colors.get(category, category_colors['general']),
#             'horizontalAlignment': 'CENTER'
#         })

#         print(f"‚úÖ Saved [{category.upper()}] {title[:50]}...")
#         return True

#     except Exception as e:
#         print(f"‚ùå Error saving to Google Sheets: {e}")
#         return False


# # ============================================================
# # Content Hash for Duplicate Detection
# # ============================================================
# def get_content_hash(title: str, content: str) -> str:
#     combined = f"{title.lower()}{content[:200].lower()}"
#     return hashlib.md5(combined.encode()).hexdigest()


# # ============================================================
# # Web Scraping - 5 Sites √ó 10 Articles = 50
# # ============================================================
# async def scrape_multiple_marathi_sources():
#     """Scrape exactly 10 articles from each of 5 sites = 50 total"""

#     all_news = []

#     async with AsyncWebCrawler(verbose=False) as crawler:

#         for site in NEWS_SITES:
#             print(f"\n{'='*60}")
#             print(f"üîç Scraping {site['name']} (Target: {site['target']} articles)")
#             print(f"{'='*60}")

#             try:
#                 config = CrawlerRunConfig(
#                     cache_mode=CacheMode.BYPASS,
#                     wait_for="body",
#                     word_count_threshold=10,
#                     page_timeout=30000,
#                     js_code="await new Promise(r => setTimeout(r, 2000));"
#                 )

#                 result = await crawler.arun(site['url'], config=config)

#                 if result.success:
#                     soup = BeautifulSoup(result.html, 'html.parser')
#                     raw_articles = []
#                     all_links = soup.find_all('a', href=True)

#                     for link_tag in all_links:
#                         href = link_tag.get('href', '')
#                         title = link_tag.get_text(strip=True)

#                         if (len(title) > 15 and len(title) < 300 and
#                             site['link_pattern'] in href and
#                             not any(x in href.lower() for x in [
#                                 'javascript:', 'mailto:', '#',
#                                 '/category/', '/tag/', '/author/',
#                                 'facebook.com', 'twitter.com', 'instagram.com',
#                                 'youtube.com', 'whatsapp.com', '/myaccount/',
#                                 '/install_app', '/advertisement', '/epaper',
#                                 'web-stories', 'photo-gallery', '/videos/',
#                                 '/games/', '/jokes/', '/terms-and-conditions',
#                                 '/topic/', '/widget/'
#                             ])):

#                             if href.startswith('/'):
#                                 base_url = site['url'].split('/')[0] + '//' + site['url'].split('/')[2]
#                                 href = base_url + href

#                             if href.startswith('http'):
#                                 raw_articles.append({
#                                     'title': title,
#                                     'link': href
#                                 })

#                     # Remove duplicates
#                     seen_links = set()
#                     unique_articles = []
#                     for article in raw_articles:
#                         if article['link'] not in seen_links:
#                             unique_articles.append(article)
#                             seen_links.add(article['link'])

#                     print(f"üìã Found {len(unique_articles)} unique links")

#                     # Fetch content for exactly 'target' articles
#                     articles_with_content = []
#                     fetch_attempts = 0

#                     for article in unique_articles:
#                         if len(articles_with_content) >= site['target']:
#                             break

#                         fetch_attempts += 1

#                         try:
#                             article_result = await crawler.arun(
#                                 article['link'],
#                                 config=CrawlerRunConfig(
#                                     cache_mode=CacheMode.BYPASS,
#                                     word_count_threshold=50,
#                                     page_timeout=15000
#                                 )
#                             )

#                             if article_result.success and len(article_result.markdown) > 100:
#                                 content_hash = get_content_hash(
#                                     article['title'],
#                                     article_result.markdown
#                                 )

#                                 if content_hash not in processed_hashes:
#                                     articles_with_content.append({
#                                         'title': article['title'],
#                                         'link': article['link'],
#                                         'content': article_result.markdown[:2500],
#                                         'hash': content_hash
#                                     })
#                                     processed_hashes.add(content_hash)
#                                     print(f"   ‚úì [{len(articles_with_content)}/{site['target']}] {article['title'][:55]}...")

#                         except Exception:
#                             continue

#                     print(f"‚úÖ {site['name']}: Fetched {len(articles_with_content)} articles")

#                     # AI Analysis for this site's articles
#                     if articles_with_content:
#                         filtered_news = await smart_analyze_with_category(
#                             articles_with_content,
#                             site['name']
#                         )
#                         all_news.extend(filtered_news)
#                         print(f"üß† {site['name']}: Analyzed {len(filtered_news)} articles")

#                 else:
#                     print(f"‚ùå Failed to fetch {site['name']}")

#             except Exception as e:
#                 print(f"‚ùå Error scraping {site['name']}: {e}")

#             # Delay between sites
#             print(f"‚è≥ Waiting before next site...")
#             await asyncio.sleep(3)

#     return all_news


# # ============================================================
# # AI Categorization using sonar-reasoning-pro
# # ============================================================
# async def smart_analyze_with_category(articles: List[Dict], source_name: str):
#     """AI categorization with sonar-reasoning-pro in batches of 5"""
#     global total_input_tokens, total_output_tokens, total_cost

#     all_filtered = []

#     # Process in batches of 5 for efficiency
#     for i in range(0, len(articles), 5):
#         batch = articles[i:i+5]

#         articles_text = ""
#         for idx, article in enumerate(batch, i+1):
#             articles_text += f"""
# ‡§¨‡§æ‡§§‡§Æ‡•Ä #{idx}:
# ‡§∂‡•Ä‡§∞‡•ç‡§∑‡§ï: {article['title']}
# Link: {article['link']}
# Content: {article['content'][:1000]}
# ---
# """

#         prompt = f"""
# ‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä ‡§è‡§ï ‡§§‡§ú‡•ç‡§û ‡§Æ‡§∞‡§æ‡§†‡•Ä ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§ï ‡§Ü‡§π‡§æ‡§§. ‡§ñ‡§æ‡§≤‡•Ä‡§≤ ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ‡§Ç‡§ö‡•á ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£ ‡§ï‡§∞‡§æ.

# **Categories (‡§´‡§ï‡•ç‡§§ ‡§Ø‡§æ‡§™‡•à‡§ï‡•Ä ‡§è‡§ï ‡§®‡§ø‡§µ‡§°‡§æ):**
# 1. sports - ‡§ï‡•ç‡§∞‡•Ä‡§°‡§æ ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ
# 2. general - ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§Æ‡§π‡§§‡•ç‡§§‡•ç‡§µ‡§æ‡§ö‡•ç‡§Ø‡§æ ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ
# 3. crime - ‡§ó‡•Å‡§®‡•ç‡§π‡•á‡§ó‡§æ‡§∞‡•Ä ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ
# 4. politics - ‡§∞‡§æ‡§ú‡§ï‡•Ä‡§Ø ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ
# 5. education - ‡§∂‡•à‡§ï‡•ç‡§∑‡§£‡§ø‡§ï ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ
# 6. economy - ‡§Ü‡§∞‡•ç‡§•‡§ø‡§ï ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ
# 7. entertainment - ‡§Æ‡§®‡•ã‡§∞‡§Ç‡§ú‡§® ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ
# 8. horror - ‡§≠‡§Ø‡§æ‡§®‡§ï ‡§ò‡§ü‡§®‡§æ

# **JSON format (‡§´‡§ï‡•ç‡§§ valid JSON array return ‡§ï‡§∞‡§æ):**
# [
#   {{
#     "title": "‡§Æ‡•Ç‡§≥ ‡§∂‡•Ä‡§∞‡•ç‡§∑‡§ï",
#     "category": "category name",
#     "detailed_summary": "‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§ ‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂ 150-200 ‡§∂‡§¨‡•ç‡§¶‡§æ‡§Ç‡§§ - ‡§ï‡•ã‡§£, ‡§ï‡§æ‡§Ø, ‡§ï‡•Å‡§†‡•á, ‡§ï‡§ß‡•Ä, ‡§ï‡§∏‡•á ‡§∏‡§∞‡•ç‡§µ details ‡§∏‡§π",
#     "importance": "high/medium/low",
#     "link": "URL ‡§ú‡§∏‡§æ‡§ö‡•ç‡§Ø‡§æ ‡§§‡§∏‡§æ",
#     "key_points": ["‡§Æ‡•Å‡§¶‡•ç‡§¶‡§æ 1", "‡§Æ‡•Å‡§¶‡•ç‡§¶‡§æ 2", "‡§Æ‡•Å‡§¶‡•ç‡§¶‡§æ 3"]
#   }}
# ]

# **‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ:**
# {articles_text}

# ‡§´‡§ï‡•ç‡§§ JSON array return ‡§ï‡§∞‡§æ. ‡§ï‡•ã‡§£‡§§‡•á‡§π‡•Ä explanation ‡§®‡§æ‡§π‡•Ä.
# """

#         try:
#             response = perplexity_client.chat.completions.create(
#                 model=MODEL_NAME,
#                 messages=[
#                     {
#                         "role": "system",
#                         "content": "You are an expert Marathi news analyst. Return ONLY valid JSON array. No markdown, no explanation."
#                     },
#                     {
#                         "role": "user",
#                         "content": prompt
#                     }
#                 ],
#                 temperature=0.2,
#                 max_tokens=4000
#             )

#             # Track tokens
#             if hasattr(response, 'usage'):
#                 input_t = response.usage.prompt_tokens
#                 output_t = response.usage.completion_tokens
#                 total_input_tokens += input_t
#                 total_output_tokens += output_t
#                 batch_cost = (input_t * COST_PER_INPUT_TOKEN) + (output_t * COST_PER_OUTPUT_TOKEN)
#                 total_cost += batch_cost
#                 print(f"   üìä Batch tokens: {input_t}in + {output_t}out = ${batch_cost:.4f}")

#             content = response.choices[0].message.content

#             # Clean thinking tags if present (sonar-reasoning-pro returns <think> tags)
#             content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL).strip()

#             json_match = re.search(r'\[.*\]', content, re.DOTALL)

#             if json_match:
#                 batch_articles = json.loads(json_match.group())
#                 all_filtered.extend(batch_articles)
#                 print(f"   ‚úÖ Categorized {len(batch_articles)} articles")
#             else:
#                 print(f"   ‚ö†Ô∏è Could not parse JSON from response")

#         except json.JSONDecodeError as e:
#             print(f"   ‚ùå JSON parse error: {e}")
#         except Exception as e:
#             print(f"   ‚ùå AI analysis error: {e}")

#         await asyncio.sleep(1.5)

#     # Add source metadata
#     for article in all_filtered:
#         article['source'] = source_name
#         article['scraped_at'] = datetime.now().isoformat()

#     return all_filtered


# # ============================================================
# # Script Generation using sonar-reasoning-pro
# # ============================================================
# async def create_reel_script_single(news_article: Dict):
#     """Generate Instagram Reel script using sonar-reasoning-pro"""
#     global total_input_tokens, total_output_tokens, total_cost

#     category = news_article.get('category', 'general')

#     system_prompt = """‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä "‡§ú‡§¨‡§∞‡•Ä ‡§ñ‡§¨‡§∞‡•Ä" Instagram Reels ‡§ö‡•á ‡§è‡§ï‡•ç‡§∏‡§™‡§∞‡•ç‡§ü ‡§∏‡•ç‡§ï‡•ç‡§∞‡§ø‡§™‡•ç‡§ü ‡§∞‡§æ‡§Ø‡§ü‡§∞ ‡§Ü‡§π‡§æ‡§§.

# **HOOK VARIETY (‡§™‡§π‡§ø‡§≤‡•ç‡§Ø‡§æ 2 ‡§ì‡§≥‡•Ä - ‡§¶‡§∞ ‡§µ‡•á‡§≥‡•Ä ‡§µ‡•á‡§ó‡§≥‡•Ä style):**
# 1. Shock Statement: "‡§è‡§ï‡§æ ‡§Ö‡§™‡§ò‡§æ‡§§‡§æ‡§®‡•á ‡§∏‡§Ç‡§™‡•Ç‡§∞‡•ç‡§£ ‡§∞‡§æ‡§ú‡•ç‡§Ø ‡§π‡§æ‡§¶‡§∞‡§≤‡§Ç."
# 2. Direct Question: "‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§≤‡§æ ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä ‡§Ü‡§π‡•á ‡§ï‡§æ...?"
# 3. Breaking News: "‡§®‡•Å‡§ï‡§§‡•Ä‡§ö ‡§è‡§ï ‡§ß‡§ï‡•ç‡§ï‡§æ‡§¶‡§æ‡§Ø‡§ï ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä ‡§∏‡§Æ‡•ã‡§∞ ‡§Ü‡§≤‡•Ä."
# 4. Name Drop: "[‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡•Ä ‡§®‡§æ‡§µ] ‡§Ü‡§ú ‡§ö‡§∞‡•ç‡§ö‡•á‡§§ ‡§ï‡§æ ‡§Ü‡§π‡•á?"
# 5. Contrast/Twist: "‡§¶‡§ø‡§∏‡§§‡§Ç ‡§ï‡§æ‡§π‡•Ä‡§§‡§∞‡•Ä, ‡§™‡§£ ‡§µ‡§æ‡§∏‡•ç‡§§‡§µ ‡§µ‡•á‡§ó‡§≥‡§Ç‡§ö."
# 6. Suspense: "‡§ï‡§æ‡§≤ ‡§∞‡§æ‡§§‡•ç‡§∞‡•Ä ‡§ò‡§°‡§≤‡§Ç ‡§§‡•á ‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§≤‡§æ ‡§µ‡§ø‡§∂‡•ç‡§µ‡§æ‡§∏ ‡§¨‡§∏‡§£‡§æ‡§∞ ‡§®‡§æ‡§π‡•Ä..."
# 7. Urgency: "‡§Æ‡§π‡§æ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§æ‡§§ ‡§Æ‡•ã‡§†‡•Ä ‡§ò‡§°‡§æ‡§Æ‡•ã‡§°..."

# **‡§∏‡•ç‡§ü‡•ç‡§∞‡§ï‡•ç‡§ö‡§∞ (15-18 ‡§ì‡§≥‡•Ä):**
# - ‡§ì‡§≥ 1-2: Hook (‡§≤‡§ï‡•ç‡§∑ ‡§µ‡•á‡§ß‡§£‡§æ‡§∞‡•Ä ‡§∏‡•Å‡§∞‡•Å‡§µ‡§æ‡§§)
# - ‡§ì‡§≥ 3-10: ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§ò‡§ü‡§®‡§æ (‡§∏‡§∞‡•ç‡§µ facts, ‡§®‡§æ‡§µ‡•á, ‡§†‡§ø‡§ï‡§æ‡§£‡•á, ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ ‡§∏‡§π)
# - ‡§ì‡§≥ 11-14: ‡§ü‡•ç‡§µ‡§ø‡§∏‡•ç‡§ü/‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£/‡§™‡•ç‡§∞‡§∂‡•ç‡§®
# - ‡§ì‡§≥ 15-18: Call To Action

# **‡§®‡§ø‡§Ø‡§Æ:**
# - Conversational Marathi ‡§≠‡§æ‡§∑‡§æ
# - ‡§™‡•ç‡§∞‡§§‡•ç‡§Ø‡•á‡§ï ‡§ì‡§≥ ‡§õ‡•ã‡§ü‡•Ä (1-2 ‡§µ‡§æ‡§ï‡•ç‡§Ø)
# - Suspense ‡§Ü‡§£‡§ø curiosity ‡§∞‡§æ‡§ñ‡§æ
# - Real facts ‡§µ‡§æ‡§™‡§∞‡§æ

# **‡§∂‡•á‡§µ‡§ü ‡§®‡§ï‡•ç‡§ï‡•Ä ‡§Ö‡§∏‡§æ:**
# "‡§§‡•Å‡§Æ‡§ö‡§Ç ‡§ï‡§æ‡§Ø ‡§Æ‡§§ ‡§Ü‡§π‡•á? ‡§ï‡§Æ‡•á‡§Ç‡§ü ‡§ï‡§∞‡•Ç‡§® ‡§∏‡§æ‡§Ç‡§ó‡§æ ‡§Ü‡§£‡§ø ‡§´‡•â‡§≤‡•ã ‡§ï‡§∞‡§æ ‡§ú‡§¨‡§∞‡•Ä ‡§ñ‡§¨‡§∞‡•Ä."

# OUTPUT: ‡§´‡§ï‡•ç‡§§ script, 15-18 ‡§ì‡§≥‡•Ä, ‡§á‡§§‡§∞ ‡§ï‡§æ‡§π‡•Ä‡§π‡•Ä ‡§®‡§æ‡§π‡•Ä."""

#     user_prompt = f"""‡§ñ‡§æ‡§≤‡•Ä‡§≤ {category.upper()} ‡§¨‡§æ‡§§‡§Æ‡•Ä‡§µ‡§∞ Instagram Reel script ‡§§‡§Ø‡§æ‡§∞ ‡§ï‡§∞‡§æ.

# ‡§∂‡•Ä‡§∞‡•ç‡§∑‡§ï: {news_article['title']}
# ‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂: {news_article.get('detailed_summary', '')}
# Key Points: {', '.join(news_article.get('key_points', []))}
# Source: {news_article.get('source', '')}

# 15-18 ‡§ì‡§≥‡•Ä‡§Ç‡§ö‡•Ä script ‡§¶‡•ç‡§Ø‡§æ. ‡§´‡§ï‡•ç‡§§ script, ‡§¨‡§æ‡§ï‡•Ä ‡§ï‡§æ‡§π‡•Ä ‡§®‡§æ‡§π‡•Ä."""

#     try:
#         response = perplexity_client.chat.completions.create(
#             model=MODEL_NAME,
#             messages=[
#                 {"role": "system", "content": system_prompt},
#                 {"role": "user", "content": user_prompt}
#             ],
#             temperature=0.8,
#             max_tokens=1500
#         )

#         # Track tokens
#         if hasattr(response, 'usage'):
#             input_t = response.usage.prompt_tokens
#             output_t = response.usage.completion_tokens
#             total_input_tokens += input_t
#             total_output_tokens += output_t
#             script_cost = (input_t * COST_PER_INPUT_TOKEN) + (output_t * COST_PER_OUTPUT_TOKEN)
#             total_cost += script_cost

#         script = response.choices[0].message.content.strip()

#         # Remove thinking tags from sonar-reasoning-pro
#         script = re.sub(r'<think>.*?</think>', '', script, flags=re.DOTALL).strip()
#         script = script.replace('```', '').strip()

#         return script

#     except Exception as e:
#         print(f"‚ùå Script generation error: {e}")
#         return None


# # ============================================================
# # Main Pipeline
# # ============================================================
# async def main():
#     global total_input_tokens, total_output_tokens, total_cost

#     print("=" * 70)
#     print("üöÄ JABARI KHABRI - SMART NEWS SCRAPER v3.0")
#     print(f"ü§ñ Model: {MODEL_NAME}")
#     print("=" * 70)
#     print("üìç Sites  : TV9 Marathi, ABP Majha, Lokmat, Mah Times, NDTV Marathi")
#     print("üìä Target : 10 articles √ó 5 sites = 50 total scripts")
#     print("üé¨ Output : 50 Reel Scripts ‚Üí Google Sheets")
#     print("=" * 70 + "\n")

#     start_time = datetime.now()

#     # ‚îÄ‚îÄ‚îÄ STEP 1: SCRAPING ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
#     print("\n" + "=" * 70)
#     print("STEP 1: SCRAPING 5 MARATHI NEWS SITES")
#     print("=" * 70 + "\n")

#     all_articles = await scrape_multiple_marathi_sources()

#     # Final deduplication
#     unique_articles = []
#     seen_hashes = set()

#     for article in all_articles:
#         article_hash = article.get(
#             'hash',
#             get_content_hash(article['title'], article.get('detailed_summary', ''))
#         )
#         if article_hash not in seen_hashes:
#             unique_articles.append(article)
#             seen_hashes.add(article_hash)

#     print(f"\n‚úÖ Total unique articles after dedup: {len(unique_articles)}")

#     # Category breakdown
#     category_counts = {}
#     for article in unique_articles:
#         cat = article.get('category', 'general')
#         category_counts[cat] = category_counts.get(cat, 0) + 1

#     print("\nüìä Category Breakdown:")
#     for cat, count in sorted(category_counts.items(), key=lambda x: -x[1]):
#         bar = "‚ñà" * count
#         print(f"   {cat.upper():<15} {bar} ({count})")


#     # Sort by importance and take top 50
#     priority_order = {'high': 1, 'medium': 2, 'low': 3}
#     unique_articles.sort(
#         key=lambda x: priority_order.get(x.get('importance', 'medium'), 2)
#     )
#     selected_articles = unique_articles[:50]

#     print(f"\nüéØ Selected top {len(selected_articles)} articles for script generation")

#     scrape_duration = (datetime.now() - start_time).total_seconds()
#     print(f"‚è±Ô∏è  Scraping done in {scrape_duration:.0f} seconds\n")

#     # ‚îÄ‚îÄ‚îÄ STEP 2: SCRIPT GENERATION + SHEETS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
#     print("=" * 70)
#     print("STEP 2: GENERATING 50 REEL SCRIPTS ‚Üí GOOGLE SHEETS")
#     print("=" * 70 + "\n")

#     worksheet = setup_google_sheets()

#     successful_saves = 0
#     failed_saves = 0

#     if worksheet and selected_articles:
#         for idx, article in enumerate(selected_articles, 1):
#             print(f"\n[{idx:02d}/50] {article.get('source', '')} | "
#                   f"{article.get('category', '').upper()} | "
#                   f"{article['title'][:50]}...")

#             script = await create_reel_script_single(article)

#             if script:
#                 success = save_to_google_sheets(
#                     worksheet,
#                     article.get('category', 'general'),
#                     article['title'],
#                     script,
#                     article.get('link', '')
#                 )
#                 if success:
#                     successful_saves += 1
#                 else:
#                     failed_saves += 1
#             else:
#                 failed_saves += 1
#                 print(f"   ‚ùå Script generation failed")

#             # Delay to respect rate limits
#             await asyncio.sleep(2)

#         print("\n" + "=" * 70)
#         print("‚úÖ ALL SCRIPTS GENERATED!")
#         print("=" * 70)
#         print(f"   ‚úÖ Successfully saved : {successful_saves}/50")
#         print(f"   ‚ùå Failed             : {failed_saves}")
#         print(f"   üìä Google Sheet       : https://docs.google.com/spreadsheets/d/{worksheet.spreadsheet.id}")

#     else:
#         print("‚ö†Ô∏è No articles found or Google Sheets unavailable!")

#     # ‚îÄ‚îÄ‚îÄ FINAL SUMMARY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
#     total_duration = (datetime.now() - start_time).total_seconds()
#     total_tokens = total_input_tokens + total_output_tokens

#     print("\n" + "=" * 70)
#     print("üìà FINAL SUMMARY")
#     print("=" * 70)
#     print(f"   ü§ñ Model              : {MODEL_NAME}")
#     print(f"   üì∞ Articles scraped   : {len(unique_articles)}")
#     print(f"   ‚úÖ Scripts generated  : {successful_saves}")
#     print(f"   ‚è±Ô∏è  Total time         : {total_duration:.0f} seconds ({total_duration/60:.1f} mins)")
#     print(f"   üì• Input tokens       : {total_input_tokens:,}")
#     print(f"   üì§ Output tokens      : {total_output_tokens:,}")
#     print(f"   üî¢ Total tokens       : {total_tokens:,}")
#     print(f"   üí∞ Total cost         : ${total_cost:.4f} (~‚Çπ{total_cost*84:.2f})")
#     print(f"   üíµ Cost per script    : ${total_cost/max(successful_saves,1):.4f}")
#     print("=" * 70 + "\n")


# if __name__ == "__main__":
#     asyncio.run(main())







# ---------------------working 50 scripts --------------

# import asyncio
# import json
# from datetime import datetime, date
# from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode
# from bs4 import BeautifulSoup
# from openai import OpenAI
# import re
# import gspread
# from google.oauth2.service_account import Credentials
# import os
# from typing import List, Dict
# import hashlib


# # ============================================================
# # Perplexity Client
# # ============================================================
# perplexity_client = OpenAI(
#     api_key=os.environ.get("PERPLEXITY_API_KEY"),
#     base_url="https://api.perplexity.ai"
# )

# ANALYSIS_MODEL      = "sonar-pro"           # ‚úÖ Cheaper for categorization
# SCRIPT_MODEL        = "sonar-reasoning-pro" # ‚úÖ Quality for scripts

# ANALYSIS_INPUT_COST  = 1.0 / 1_000_000
# ANALYSIS_OUTPUT_COST = 1.0 / 1_000_000
# SCRIPT_INPUT_COST    = 2.0 / 1_000_000
# SCRIPT_OUTPUT_COST   = 8.0 / 1_000_000


# # ============================================================
# # Config
# # ============================================================
# GOOGLE_SHEETS_CREDENTIALS_FILE = "credentials.json"
# GOOGLE_SHEET_NAME               = "Instagram Scripts"
# GOOGLE_WORKSHEET_NAME           = "Scripts"
# TARGET_SCRIPTS                  = 50   # ‚úÖ Hard target

# VALID_CATEGORIES = [
#     "sports", "general", "crime", "politics",
#     "education", "economy", "entertainment", "horror"
# ]


# # ============================================================
# # Token tracking
# # ============================================================
# total_input_tokens  = 0
# total_output_tokens = 0
# total_cost          = 0.0
# processed_hashes    = set()


# # ============================================================
# # 5 News Sites
# # ============================================================
# NEWS_SITES = [
#     {
#         "name": "TV9 Marathi",
#         "url": "https://www.tv9marathi.com/latest-news",
#         "link_pattern": "tv9marathi.com",
#         "target": 10,
#         "fetch_limit": 40
#     },
#     {
#         "name": "ABP Majha",
#         "url": "https://marathi.abplive.com/news",
#         "link_pattern": "abplive.com",
#         "target": 10,
#         "fetch_limit": 40
#     },
#     {
#         "name": "Lokmat",
#         "url": "https://www.lokmat.com/latestnews/",
#         "link_pattern": "lokmat.com",
#         "target": 10,
#         "fetch_limit": 40
#     },
#     {
#         "name": "Maharashtra Times",
#         "url": "https://maharashtratimes.com/",
#         "link_pattern": "maharashtratimes.com",
#         "target": 10,
#         "fetch_limit": 40
#     },
#     {
#         "name": "NDTV Marathi",
#         "url": "https://marathi.ndtv.com/",
#         "link_pattern": "marathi.ndtv.com",
#         "target": 10,
#         "fetch_limit": 40
#     }
# ]


# # ============================================================
# # Google Sheets Setup
# # ============================================================
# def setup_google_sheets():
#     try:
#         scope = [
#             'https://spreadsheets.google.com/feeds',
#             'https://www.googleapis.com/auth/drive'
#         ]
#         creds = Credentials.from_service_account_file(
#             GOOGLE_SHEETS_CREDENTIALS_FILE, scopes=scope
#         )
#         client = gspread.authorize(creds)

#         try:
#             sheet = client.open(GOOGLE_SHEET_NAME)
#             print(f"‚úÖ Connected: '{GOOGLE_SHEET_NAME}'")
#         except gspread.SpreadsheetNotFound:
#             sheet = client.create(GOOGLE_SHEET_NAME)
#             print(f"‚úÖ Created: '{GOOGLE_SHEET_NAME}'")

#         try:
#             worksheet = sheet.worksheet(GOOGLE_WORKSHEET_NAME)
#             print(f"‚úÖ Worksheet: '{GOOGLE_WORKSHEET_NAME}'")
#         except gspread.WorksheetNotFound:
#             worksheet = sheet.add_worksheet(
#                 title=GOOGLE_WORKSHEET_NAME, rows=5000, cols=10
#             )
#             worksheet.update('A1:E1', [[
#                 'Timestamp', 'Category', 'Title', 'Script', 'Source Link'
#             ]])
#             worksheet.format('A1:E1', {
#                 'textFormat': {
#                     'bold': True,
#                     'foregroundColor': {'red': 1.0, 'green': 1.0, 'blue': 1.0}
#                 },
#                 'backgroundColor': {'red': 0.2, 'green': 0.6, 'blue': 0.9},
#                 'horizontalAlignment': 'CENTER'
#             })
#             worksheet.set_column_width('A', 180)
#             worksheet.set_column_width('B', 150)
#             worksheet.set_column_width('C', 400)
#             worksheet.set_column_width('D', 600)
#             worksheet.set_column_width('E', 400)
#             print(f"‚úÖ Created worksheet with headers")

#         return worksheet

#     except FileNotFoundError:
#         print(f"‚ùå credentials.json not found!")
#         return None
#     except Exception as e:
#         print(f"‚ùå Sheets setup error: {e}")
#         import traceback
#         traceback.print_exc()
#         return None


# # ============================================================
# # Save to Google Sheets
# # ============================================================
# def save_to_google_sheets(worksheet, category, title, script, source_link):
#     try:
#         timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

#         script = '\n'.join(str(i) for i in script) if isinstance(script, list) else str(script).strip()
#         script = script.replace('[', '').replace(']', '')
#         title = str(title).strip()
#         source_link = str(source_link).strip()
#         category = str(category).strip().lower()

#         if category not in VALID_CATEGORIES:
#             category = "general"

#         next_row = len(worksheet.get_all_values()) + 1
#         worksheet.append_row(
#             [timestamp, category, title, script, source_link],
#             value_input_option='RAW'
#         )

#         worksheet.format(f'A{next_row}:E{next_row}', {
#             'textFormat': {'foregroundColor': {'red': 0.0, 'green': 0.0, 'blue': 0.0}, 'fontSize': 10},
#             'backgroundColor': {'red': 1.0, 'green': 1.0, 'blue': 1.0},
#             'wrapStrategy': 'WRAP',
#             'verticalAlignment': 'TOP'
#         })

#         category_colors = {
#             'crime':         {'red': 0.95, 'green': 0.8,  'blue': 0.8},
#             'politics':      {'red': 0.8,  'green': 0.9,  'blue': 1.0},
#             'sports':        {'red': 0.8,  'green': 1.0,  'blue': 0.8},
#             'entertainment': {'red': 1.0,  'green': 0.9,  'blue': 0.8},
#             'education':     {'red': 0.9,  'green': 0.95, 'blue': 1.0},
#             'economy':       {'red': 0.95, 'green': 1.0,  'blue': 0.85},
#             'horror':        {'red': 0.7,  'green': 0.7,  'blue': 0.7},
#             'general':       {'red': 1.0,  'green': 1.0,  'blue': 0.9}
#         }
#         worksheet.format(f'B{next_row}', {
#             'textFormat': {'bold': True, 'foregroundColor': {'red': 0.0, 'green': 0.0, 'blue': 0.0}, 'fontSize': 10},
#             'backgroundColor': category_colors.get(category, category_colors['general']),
#             'horizontalAlignment': 'CENTER'
#         })

#         print(f"‚úÖ Saved [{category.upper()}] {title[:50]}...")
#         return True

#     except Exception as e:
#         print(f"‚ùå Save error: {e}")
#         return False


# # ============================================================
# # Content Hash
# # ============================================================
# def get_content_hash(title: str, content: str) -> str:
#     return hashlib.md5(f"{title.lower()}{content[:200].lower()}".encode()).hexdigest()


# # ============================================================
# # Fetch with Retry
# # ============================================================
# async def fetch_article_with_retry(crawler, url: str, retries: int = 3) -> str:
#     for attempt in range(1, retries + 1):
#         try:
#             result = await crawler.arun(
#                 url,
#                 config=CrawlerRunConfig(
#                     cache_mode=CacheMode.BYPASS,
#                     word_count_threshold=10,
#                     page_timeout=25000
#                 )
#             )
#             if result.success and len(result.markdown) > 50:
#                 return result.markdown
#             await asyncio.sleep(2)
#         except Exception:
#             await asyncio.sleep(2)
#     return ""


# # ============================================================
# # Web Scraping
# # ============================================================
# async def scrape_multiple_marathi_sources():
#     all_news = []

#     async with AsyncWebCrawler(verbose=False) as crawler:
#         for site in NEWS_SITES:
#             print(f"\n{'='*60}")
#             print(f"üîç {site['name']} | Target: {site['target']}")
#             print(f"{'='*60}")

#             site_articles = []

#             try:
#                 result = await crawler.arun(
#                     site['url'],
#                     config=CrawlerRunConfig(
#                         cache_mode=CacheMode.BYPASS,
#                         wait_for="body",
#                         word_count_threshold=10,
#                         page_timeout=30000,
#                         js_code="await new Promise(r => setTimeout(r, 3000));"
#                     )
#                 )

#                 if not result.success:
#                     print(f"‚ùå Failed: {site['name']}")
#                     continue

#                 soup = BeautifulSoup(result.html, 'html.parser')
#                 raw_articles = []

#                 for link_tag in soup.find_all('a', href=True):
#                     href  = link_tag.get('href', '')
#                     title = link_tag.get_text(strip=True)

#                     if (15 < len(title) < 300 and
#                         site['link_pattern'] in href and
#                         not any(x in href.lower() for x in [
#                             'javascript:', 'mailto:', '#',
#                             '/category/', '/tag/', '/author/',
#                             'facebook.com', 'twitter.com', 'instagram.com',
#                             'youtube.com', 'whatsapp.com', '/myaccount/',
#                             '/install_app', '/advertisement', '/epaper',
#                             'web-stories', 'photo-gallery', '/videos/',
#                             '/games/', '/jokes/', '/terms-and-conditions',
#                             '/topic/', '/widget/'
#                         ])):

#                         if href.startswith('/'):
#                             base = site['url'].split('/')[0] + '//' + site['url'].split('/')[2]
#                             href = base + href

#                         if href.startswith('http'):
#                             raw_articles.append({'title': title, 'link': href})

#                 # Deduplicate
#                 seen = set()
#                 unique_links = []
#                 for a in raw_articles:
#                     if a['link'] not in seen:
#                         unique_links.append(a)
#                         seen.add(a['link'])

#                 print(f"üìã Found {len(unique_links)} unique links")

#                 # Fetch articles until target reached
#                 for article in unique_links[:site['fetch_limit']]:
#                     if len(site_articles) >= site['target']:
#                         break

#                     print(f"   üîó [{len(site_articles)+1}/{site['target']}] {article['title'][:50]}...")

#                     markdown = await fetch_article_with_retry(crawler, article['link'])

#                     # ‚úÖ Always add article - use content if available, title as fallback
#                     content = markdown if markdown else article['title']
#                     content_hash = get_content_hash(article['title'], content)

#                     if content_hash not in processed_hashes:
#                         site_articles.append({
#                             'title':   article['title'],
#                             'link':    article['link'],
#                             'content': content[:2500],
#                             'hash':    content_hash,
#                             'has_full_content': bool(markdown)  # ‚úÖ Track if real content
#                         })
#                         processed_hashes.add(content_hash)
#                         status = "‚úÖ" if markdown else "‚ö†Ô∏è fallback"
#                         print(f"   {status} [{len(site_articles)}/{site['target']}] {article['title'][:50]}...")
#                     else:
#                         print(f"   üîÑ Duplicate skipped")

#                     await asyncio.sleep(1)

#                 print(f"\nüì¶ {site['name']}: {len(site_articles)}/{site['target']} collected")

#                 if site_articles:
#                     filtered = await smart_analyze_with_category(site_articles, site['name'])
#                     all_news.extend(filtered)
#                     print(f"üß† {site['name']}: Analyzed {len(filtered)} articles")

#             except Exception as e:
#                 print(f"‚ùå Error {site['name']}: {e}")

#             await asyncio.sleep(3)

#     return all_news


# # ============================================================
# # AI Categorization - sonar-pro (cheap)
# # ============================================================
# async def smart_analyze_with_category(articles: List[Dict], source_name: str):
#     global total_input_tokens, total_output_tokens, total_cost

#     all_filtered = []

#     for i in range(0, len(articles), 5):
#         batch = articles[i:i+5]

#         articles_text = ""
#         for idx, article in enumerate(batch, i+1):
#             # ‚úÖ Only 500 chars for analysis - enough to categorize
#             articles_text += f"#{idx}: {article['title']}\n{article['content'][:500]}\n---\n"

#         # ‚úÖ Shorter prompt
#         prompt = f"""‡§Æ‡§∞‡§æ‡§†‡•Ä ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§ï: ‡§ñ‡§æ‡§≤‡•Ä‡§≤ ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ‡§Ç‡§®‡§æ category ‡§Ü‡§£‡§ø summary ‡§¶‡•ç‡§Ø‡§æ.

# Categories: sports, general, crime, politics, education, economy, entertainment, horror

# JSON array format:
# [{{"title":"‡§∂‡•Ä‡§∞‡•ç‡§∑‡§ï","category":"cat","detailed_summary":"150 ‡§∂‡§¨‡•ç‡§¶ Marathi summary","importance":"high/medium/low","link":"url","key_points":["1","2","3"]}}]

# ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ:
# {articles_text}

# ‡§´‡§ï‡•ç‡§§ JSON array."""

#         try:
#             response = perplexity_client.chat.completions.create(
#                 model=ANALYSIS_MODEL,  # ‚úÖ sonar-pro (cheaper)
#                 messages=[
#                     {"role": "system", "content": "Return ONLY valid JSON array."},
#                     {"role": "user",   "content": prompt}
#                 ],
#                 temperature=0.2,
#                 max_tokens=3000
#             )

#             if hasattr(response, 'usage'):
#                 i_t = response.usage.prompt_tokens
#                 o_t = response.usage.completion_tokens
#                 total_input_tokens  += i_t
#                 total_output_tokens += o_t
#                 c = (i_t * ANALYSIS_INPUT_COST) + (o_t * ANALYSIS_OUTPUT_COST)
#                 total_cost += c
#                 print(f"   üìä {i_t}in + {o_t}out = ${c:.4f}")

#             content = response.choices[0].message.content
#             content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL).strip()
#             match = re.search(r'\[.*\]', content, re.DOTALL)

#             if match:
#                 parsed = json.loads(match.group())
#                 all_filtered.extend(parsed)
#                 print(f"   ‚úÖ Categorized {len(parsed)} articles")
#             else:
#                 # ‚úÖ FALLBACK: manually create entries if JSON fails
#                 print(f"   ‚ö†Ô∏è JSON failed - using fallback entries")
#                 for article in batch:
#                     all_filtered.append({
#                         'title':            article['title'],
#                         'category':         'general',
#                         'detailed_summary': article['title'],
#                         'importance':       'medium',
#                         'link':             article['link'],
#                         'key_points':       [article['title']]
#                     })

#         except json.JSONDecodeError:
#             # ‚úÖ FALLBACK on JSON error too
#             for article in batch:
#                 all_filtered.append({
#                     'title':            article['title'],
#                     'category':         'general',
#                     'detailed_summary': article['content'][:300],
#                     'importance':       'medium',
#                     'link':             article['link'],
#                     'key_points':       [article['title']]
#                 })
#         except Exception as e:
#             print(f"   ‚ùå AI error: {e}")

#         await asyncio.sleep(1.5)

#     for article in all_filtered:
#         article['source']      = source_name
#         article['scraped_at']  = datetime.now().isoformat()

#     return all_filtered


# # ============================================================
# # Script Generation - sonar-reasoning-pro (quality)
# # ============================================================
# async def create_reel_script_single(news_article: Dict):
#     global total_input_tokens, total_output_tokens, total_cost

#     category = news_article.get('category', 'general')

#     # ‚úÖ Shorter system prompt - same quality
#     system_prompt = """‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä "‡§ú‡§¨‡§∞‡•Ä ‡§ñ‡§¨‡§∞‡•Ä" Reels script writer ‡§Ü‡§π‡§æ‡§§.
# Hook (2 ‡§ì‡§≥‡•Ä) ‚Üí Story+Facts (8 ‡§ì‡§≥‡•Ä) ‚Üí Twist/Question (4 ‡§ì‡§≥‡•Ä) ‚Üí CTA (2-4 ‡§ì‡§≥‡•Ä)
# ‡§∂‡•á‡§µ‡§ü: "‡§§‡•Å‡§Æ‡§ö‡§Ç ‡§ï‡§æ‡§Ø ‡§Æ‡§§ ‡§Ü‡§π‡•á? ‡§ï‡§Æ‡•á‡§Ç‡§ü ‡§ï‡§∞‡•Ç‡§® ‡§∏‡§æ‡§Ç‡§ó‡§æ ‡§Ü‡§£‡§ø ‡§´‡•â‡§≤‡•ã ‡§ï‡§∞‡§æ ‡§ú‡§¨‡§∞‡•Ä ‡§ñ‡§¨‡§∞‡•Ä."
# Output: ‡§´‡§ï‡•ç‡§§ 15-18 ‡§ì‡§≥‡•Ä script. ‡§á‡§§‡§∞ ‡§ï‡§æ‡§π‡•Ä‡§π‡•Ä ‡§®‡§æ‡§π‡•Ä."""

#     # ‚úÖ Trim summary to 300 chars to save tokens
#     summary     = news_article.get('detailed_summary', news_article.get('title', ''))[:300]
#     key_points  = ', '.join(news_article.get('key_points', [news_article.get('title', '')]))

#     user_prompt = f"""{category.upper()} ‡§¨‡§æ‡§§‡§Æ‡•Ä:
# ‡§∂‡•Ä‡§∞‡•ç‡§∑‡§ï: {news_article['title']}
# ‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂: {summary}
# ‡§Æ‡•Å‡§¶‡•ç‡§¶‡•á: {key_points}

# 15-18 ‡§ì‡§≥‡•Ä script ‡§¶‡•ç‡§Ø‡§æ."""

#     # ‚úÖ Retry script generation up to 2 times
#     for attempt in range(1, 3):
#         try:
#             response = perplexity_client.chat.completions.create(
#                 model=SCRIPT_MODEL,
#                 messages=[
#                     {"role": "system", "content": system_prompt},
#                     {"role": "user",   "content": user_prompt}
#                 ],
#                 temperature=0.8,
#                 max_tokens=1200  # ‚úÖ Reduced from 1500
#             )

#             if hasattr(response, 'usage'):
#                 i_t = response.usage.prompt_tokens
#                 o_t = response.usage.completion_tokens
#                 total_input_tokens  += i_t
#                 total_output_tokens += o_t
#                 total_cost += (i_t * SCRIPT_INPUT_COST) + (o_t * SCRIPT_OUTPUT_COST)

#             script = response.choices[0].message.content.strip()
#             script = re.sub(r'<think>.*?</think>', '', script, flags=re.DOTALL).strip()
#             script = script.replace('```', '').strip()

#             if len(script) > 50:  # ‚úÖ Valid script check
#                 return script

#         except Exception as e:
#             print(f"   ‚ö†Ô∏è Script attempt {attempt} failed: {e}")
#             await asyncio.sleep(2)

#     # ‚úÖ LAST RESORT fallback script if all attempts fail
#     return f"""‡§Æ‡§π‡§æ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§æ‡§§ ‡§è‡§ï ‡§Æ‡§π‡§§‡•ç‡§§‡•ç‡§µ‡§æ‡§ö‡•Ä ‡§¨‡§æ‡§§‡§Æ‡•Ä ‡§∏‡§Æ‡•ã‡§∞ ‡§Ü‡§≤‡•Ä ‡§Ü‡§π‡•á.

# {news_article['title']}

# ‡§π‡•Ä ‡§¨‡§æ‡§§‡§Æ‡•Ä ‡§∏‡§ß‡•ç‡§Ø‡§æ ‡§ö‡§∞‡•ç‡§ö‡•á‡§§ ‡§Ü‡§π‡•á ‡§Ü‡§£‡§ø ‡§∏‡§∞‡•ç‡§µ‡§æ‡§Ç‡§ö‡•á ‡§≤‡§ï‡•ç‡§∑ ‡§µ‡•á‡§ß‡•Ç‡§® ‡§ò‡•á‡§§ ‡§Ü‡§π‡•á.

# ‡§Ö‡§ß‡§ø‡§ï ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä‡§∏‡§æ‡§†‡•Ä ‡§Ü‡§Æ‡§ö‡•á ‡§™‡•á‡§ú ‡§´‡•â‡§≤‡•ã ‡§ï‡§∞‡§æ.

# ‡§§‡•Å‡§Æ‡§ö‡§Ç ‡§ï‡§æ‡§Ø ‡§Æ‡§§ ‡§Ü‡§π‡•á? ‡§ï‡§Æ‡•á‡§Ç‡§ü ‡§ï‡§∞‡•Ç‡§® ‡§∏‡§æ‡§Ç‡§ó‡§æ ‡§Ü‡§£‡§ø ‡§´‡•â‡§≤‡•ã ‡§ï‡§∞‡§æ ‡§ú‡§¨‡§∞‡•Ä ‡§ñ‡§¨‡§∞‡•Ä."""


# # ============================================================
# # Main Pipeline
# # ============================================================
# async def main():
#     global total_input_tokens, total_output_tokens, total_cost

#     print("=" * 70)
#     print("üöÄ JABARI KHABRI - SMART NEWS SCRAPER v4.0")
#     print(f"üîç Analysis : {ANALYSIS_MODEL}")
#     print(f"‚úçÔ∏è  Scripts  : {SCRIPT_MODEL}")
#     print("=" * 70)
#     print("üìç Sites  : TV9, ABP Majha, Lokmat, Mah Times, NDTV Marathi")
#     print(f"üìä Target : 10 √ó 5 = {TARGET_SCRIPTS} scripts STRICTLY")
#     print("üîÅ Buffer : 40 links/site | üîÑ Retry: 3√ó fetch + 2√ó script")
#     print("=" * 70 + "\n")

#     start_time = datetime.now()

#     # ‚îÄ‚îÄ STEP 1: SCRAPING ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
#     print("=" * 70)
#     print("STEP 1: SCRAPING 5 MARATHI NEWS SITES")
#     print("=" * 70 + "\n")

#     all_articles = await scrape_multiple_marathi_sources()

#     # Final deduplication
#     unique_articles = []
#     seen_hashes = set()
#     for article in all_articles:
#         h = article.get('hash', get_content_hash(
#             article['title'], article.get('detailed_summary', '')
#         ))
#         if h not in seen_hashes:
#             unique_articles.append(article)
#             seen_hashes.add(h)

#     print(f"\n‚úÖ Total unique articles: {len(unique_articles)}")

#     # ‚úÖ FIXED category breakdown
#     category_counts = {}
#     for article in unique_articles:
#         cat = article.get('category', 'general')
#         category_counts[cat] = category_counts.get(cat, 0) + 1

#     print("\nüìä Category Breakdown:")
#     for cat, count in sorted(category_counts.items(), key=lambda x: -x[1]):  # ‚úÖ FIXED[1]
#         bar = "‚ñà" * count
#         print(f"   {cat.upper():<15} {bar} ({count})")

#     # Sort by importance
#     priority_order = {'high': 1, 'medium': 2, 'low': 3}
#     unique_articles.sort(
#         key=lambda x: priority_order.get(x.get('importance', 'medium'), 2)
#     )
#     selected_articles = unique_articles[:TARGET_SCRIPTS]

#     print(f"\nüéØ Selected: {len(selected_articles)}/{TARGET_SCRIPTS} articles")
#     print(f"‚è±Ô∏è  Scraping: {(datetime.now()-start_time).total_seconds():.0f}s\n")

#     # ‚îÄ‚îÄ STEP 2: SCRIPTS + SHEETS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
#     print("=" * 70)
#     print("STEP 2: GENERATING SCRIPTS ‚Üí GOOGLE SHEETS")
#     print("=" * 70 + "\n")

#     worksheet = setup_google_sheets()
#     successful_saves = 0
#     failed_saves     = 0

#     if worksheet and selected_articles:
#         for idx, article in enumerate(selected_articles, 1):
#             print(f"\n[{idx:02d}/{len(selected_articles)}] "
#                   f"{article.get('source','')[:12]} | "
#                   f"{article.get('category','').upper():<12} | "
#                   f"{article['title'][:40]}...")

#             script = await create_reel_script_single(article)  # Always returns something now

#             success = save_to_google_sheets(
#                 worksheet,
#                 article.get('category', 'general'),
#                 article['title'],
#                 script,
#                 article.get('link', '')
#             )
#             if success:
#                 successful_saves += 1
#             else:
#                 failed_saves += 1

#             await asyncio.sleep(1)  # ‚úÖ Reduced from 2s ‚Üí 1s

#         print("\n" + "=" * 70)
#         print("‚úÖ DONE!")
#         print("=" * 70)
#         print(f"   ‚úÖ Saved  : {successful_saves}/{len(selected_articles)}")
#         print(f"   ‚ùå Failed : {failed_saves}")
#         print(f"   üìä Sheet  : https://docs.google.com/spreadsheets/d/{worksheet.spreadsheet.id}")

#     else:
#         print("‚ö†Ô∏è No articles or Sheets unavailable!")

#     # ‚îÄ‚îÄ FINAL SUMMARY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
#     total_duration = (datetime.now() - start_time).total_seconds()
#     total_tokens   = total_input_tokens + total_output_tokens

#     print("\n" + "=" * 70)
#     print("üìà FINAL SUMMARY")
#     print("=" * 70)
#     print(f"   üîç Analysis model     : {ANALYSIS_MODEL}")
#     print(f"   ‚úçÔ∏è  Script model       : {SCRIPT_MODEL}")
#     print(f"   üì∞ Articles scraped   : {len(unique_articles)}")
#     print(f"   ‚úÖ Scripts generated  : {successful_saves}")
#     print(f"   ‚è±Ô∏è  Total time         : {total_duration:.0f}s ({total_duration/60:.1f} mins)")
#     print(f"   üì• Input tokens       : {total_input_tokens:,}")
#     print(f"   üì§ Output tokens      : {total_output_tokens:,}")
#     print(f"   üî¢ Total tokens       : {total_tokens:,}")
#     print(f"   üí∞ Total cost         : ${total_cost:.4f} (~‚Çπ{total_cost*84:.2f})")
#     print(f"   üíµ Cost per script    : ${total_cost/max(successful_saves,1):.4f}")
#     print("=" * 70 + "\n")


# if __name__ == "__main__":
#     asyncio.run(main())



# -----------------------------links mistmatch code below but working very fine ---------------------


# import asyncio
# import json
# from datetime import datetime
# from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode
# from bs4 import BeautifulSoup
# from openai import OpenAI
# import re
# import gspread
# from google.oauth2.service_account import Credentials
# import os
# from typing import List, Dict
# import hashlib


# # ============================================================
# # Perplexity Client
# # ============================================================
# perplexity_client = OpenAI(
#     api_key=os.environ.get("PERPLEXITY_API_KEY"),
#     base_url="https://api.perplexity.ai"
# )

# ANALYSIS_MODEL       = "sonar-pro"            # Cheap: categorization
# SCRIPT_MODEL         = "sonar-reasoning-pro"  # Quality: script writing

# ANALYSIS_INPUT_COST  = 1.0 / 1_000_000
# ANALYSIS_OUTPUT_COST = 1.0 / 1_000_000
# SCRIPT_INPUT_COST    = 2.0 / 1_000_000
# SCRIPT_OUTPUT_COST   = 8.0 / 1_000_000


# # ============================================================
# # Config
# # ============================================================
# GOOGLE_SHEETS_CREDENTIALS_FILE = "credentials.json"
# GOOGLE_SHEET_NAME               = "Instagram Scripts"
# GOOGLE_WORKSHEET_NAME           = "Scripts"
# TARGET_SCRIPTS                  = 50

# VALID_CATEGORIES = [
#     "sports", "general", "crime", "politics",
#     "education", "economy", "entertainment", "horror"
# ]

# # Refusal detection keywords
# REFUSAL_KEYWORDS = [
#     "I appreciate", "I should clarify", "I'm Perplexity",
#     "search assistant", "I'm not able", "I cannot create",
#     "Would you like", "clarify my role", "I'm an AI",
#     "as an AI", "I don't create"
# ]


# # ============================================================
# # Token Tracking
# # ============================================================
# total_input_tokens  = 0
# total_output_tokens = 0
# total_cost          = 0.0
# processed_hashes    = set()


# # ============================================================
# # News Sites - 10 articles √ó 5 sites = 50
# # ============================================================
# NEWS_SITES = [
#     {
#         "name": "TV9 Marathi",
#         "url": "https://www.tv9marathi.com/latest-news",
#         "link_pattern": "tv9marathi.com",
#         "target": 10,
#         "fetch_limit": 40
#     },
#     {
#         "name": "ABP Majha",
#         "url": "https://marathi.abplive.com/news",
#         "link_pattern": "abplive.com",
#         "target": 10,
#         "fetch_limit": 40
#     },
#     {
#         "name": "Lokmat",
#         "url": "https://www.lokmat.com/latestnews/",
#         "link_pattern": "lokmat.com",
#         "target": 10,
#         "fetch_limit": 40
#     },
#     {
#         "name": "Maharashtra Times",
#         "url": "https://maharashtratimes.com/",
#         "link_pattern": "maharashtratimes.com",
#         "target": 10,
#         "fetch_limit": 40
#     },
#     {
#         "name": "NDTV Marathi",
#         "url": "https://marathi.ndtv.com/",
#         "link_pattern": "marathi.ndtv.com",
#         "target": 10,
#         "fetch_limit": 40
#     }
# ]


# # ============================================================
# # Google Sheets Setup
# # ============================================================
# def setup_google_sheets():
#     try:
#         scope = [
#             'https://spreadsheets.google.com/feeds',
#             'https://www.googleapis.com/auth/drive'
#         ]
#         creds = Credentials.from_service_account_file(
#             GOOGLE_SHEETS_CREDENTIALS_FILE, scopes=scope
#         )
#         client = gspread.authorize(creds)

#         try:
#             sheet = client.open(GOOGLE_SHEET_NAME)
#             print(f"‚úÖ Connected: '{GOOGLE_SHEET_NAME}'")
#         except gspread.SpreadsheetNotFound:
#             sheet = client.create(GOOGLE_SHEET_NAME)
#             print(f"‚úÖ Created: '{GOOGLE_SHEET_NAME}'")

#         try:
#             worksheet = sheet.worksheet(GOOGLE_WORKSHEET_NAME)
#             print(f"‚úÖ Worksheet: '{GOOGLE_WORKSHEET_NAME}'")
#         except gspread.WorksheetNotFound:
#             worksheet = sheet.add_worksheet(
#                 title=GOOGLE_WORKSHEET_NAME, rows=5000, cols=10
#             )
#             worksheet.update('A1:E1', [[
#                 'Timestamp', 'Category', 'Title', 'Script', 'Source Link'
#             ]])
#             worksheet.format('A1:E1', {
#                 'textFormat': {
#                     'bold': True,
#                     'foregroundColor': {'red': 1.0, 'green': 1.0, 'blue': 1.0}
#                 },
#                 'backgroundColor': {'red': 0.2, 'green': 0.6, 'blue': 0.9},
#                 'horizontalAlignment': 'CENTER'
#             })
#             worksheet.set_column_width('A', 180)
#             worksheet.set_column_width('B', 150)
#             worksheet.set_column_width('C', 400)
#             worksheet.set_column_width('D', 600)
#             worksheet.set_column_width('E', 400)
#             print(f"‚úÖ Created worksheet with headers")

#         return worksheet

#     except FileNotFoundError:
#         print(f"‚ùå credentials.json not found!")
#         return None
#     except Exception as e:
#         print(f"‚ùå Sheets setup error: {e}")
#         import traceback
#         traceback.print_exc()
#         return None


# # ============================================================
# # Save to Google Sheets
# # ============================================================
# def save_to_google_sheets(worksheet, category, title, script, source_link):
#     try:
#         timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

#         script = '\n'.join(str(i) for i in script) if isinstance(script, list) else str(script).strip()
#         script = script.replace('[', '').replace(']', '')
#         title = str(title).strip()
#         source_link = str(source_link).strip()
#         category = str(category).strip().lower()

#         if category not in VALID_CATEGORIES:
#             category = "general"

#         next_row = len(worksheet.get_all_values()) + 1
#         worksheet.append_row(
#             [timestamp, category, title, script, source_link],
#             value_input_option='RAW'
#         )

#         worksheet.format(f'A{next_row}:E{next_row}', {
#             'textFormat': {
#                 'foregroundColor': {'red': 0.0, 'green': 0.0, 'blue': 0.0},
#                 'fontSize': 10
#             },
#             'backgroundColor': {'red': 1.0, 'green': 1.0, 'blue': 1.0},
#             'wrapStrategy': 'WRAP',
#             'verticalAlignment': 'TOP'
#         })

#         category_colors = {
#             'crime':         {'red': 0.95, 'green': 0.8,  'blue': 0.8},
#             'politics':      {'red': 0.8,  'green': 0.9,  'blue': 1.0},
#             'sports':        {'red': 0.8,  'green': 1.0,  'blue': 0.8},
#             'entertainment': {'red': 1.0,  'green': 0.9,  'blue': 0.8},
#             'education':     {'red': 0.9,  'green': 0.95, 'blue': 1.0},
#             'economy':       {'red': 0.95, 'green': 1.0,  'blue': 0.85},
#             'horror':        {'red': 0.7,  'green': 0.7,  'blue': 0.7},
#             'general':       {'red': 1.0,  'green': 1.0,  'blue': 0.9}
#         }
#         worksheet.format(f'B{next_row}', {
#             'textFormat': {
#                 'bold': True,
#                 'foregroundColor': {'red': 0.0, 'green': 0.0, 'blue': 0.0},
#                 'fontSize': 10
#             },
#             'backgroundColor': category_colors.get(category, category_colors['general']),
#             'horizontalAlignment': 'CENTER'
#         })

#         print(f"‚úÖ Saved [{category.upper()}] {title[:50]}...")
#         return True

#     except Exception as e:
#         print(f"‚ùå Save error: {e}")
#         return False


# # ============================================================
# # Content Hash
# # ============================================================
# def get_content_hash(title: str, content: str) -> str:
#     return hashlib.md5(
#         f"{title.lower()}{content[:200].lower()}".encode()
#     ).hexdigest()


# # ============================================================
# # Sort helper - FIX for lambda crash
# # ============================================================
# def sort_by_count(item):
#     return -item[1]


# def sort_by_priority(item):
#     priority_order = {'high': 1, 'medium': 2, 'low': 3}
#     return priority_order.get(item.get('importance', 'medium'), 2)


# # ============================================================
# # Marathi Validator
# # ============================================================
# def is_valid_marathi_script(script: str) -> bool:
#     if len(script) < 100:
#         return False
#     if any(kw.lower() in script.lower() for kw in REFUSAL_KEYWORDS):
#         return False
#     devanagari = len(re.findall(r'[\u0900-\u097F]', script))
#     total      = len(script.replace(' ', '').replace('\n', ''))
#     return (devanagari / max(total, 1)) > 0.35


# # ============================================================
# # Fetch Article with Retry
# # ============================================================
# async def fetch_article_with_retry(crawler, url: str, retries: int = 3) -> str:
#     for attempt in range(1, retries + 1):
#         try:
#             result = await crawler.arun(
#                 url,
#                 config=CrawlerRunConfig(
#                     cache_mode=CacheMode.BYPASS,
#                     word_count_threshold=10,
#                     page_timeout=25000
#                 )
#             )
#             if result.success and len(result.markdown) > 50:
#                 return result.markdown
#             await asyncio.sleep(2)
#         except Exception:
#             await asyncio.sleep(2)
#     return ""


# # ============================================================
# # Scraping - 10 per site guaranteed
# # ============================================================
# async def scrape_multiple_marathi_sources():
#     all_news = []

#     async with AsyncWebCrawler(verbose=False) as crawler:
#         for site in NEWS_SITES:
#             print(f"\n{'='*60}")
#             print(f"üîç {site['name']} | Target: {site['target']}")
#             print(f"{'='*60}")

#             site_articles = []

#             try:
#                 result = await crawler.arun(
#                     site['url'],
#                     config=CrawlerRunConfig(
#                         cache_mode=CacheMode.BYPASS,
#                         wait_for="body",
#                         word_count_threshold=10,
#                         page_timeout=30000,
#                         js_code="await new Promise(r => setTimeout(r, 3000));"
#                     )
#                 )

#                 if not result.success:
#                     print(f"‚ùå Failed: {site['name']}")
#                     continue

#                 soup = BeautifulSoup(result.html, 'html.parser')
#                 raw_articles = []

#                 for link_tag in soup.find_all('a', href=True):
#                     href  = link_tag.get('href', '')
#                     title = link_tag.get_text(strip=True)

#                     if (15 < len(title) < 300 and
#                         site['link_pattern'] in href and
#                         not any(x in href.lower() for x in [
#                             'javascript:', 'mailto:', '#',
#                             '/category/', '/tag/', '/author/',
#                             'facebook.com', 'twitter.com', 'instagram.com',
#                             'youtube.com', 'whatsapp.com', '/myaccount/',
#                             '/install_app', '/advertisement', '/epaper',
#                             'web-stories', 'photo-gallery', '/videos/',
#                             '/games/', '/jokes/', '/terms-and-conditions',
#                             '/topic/', '/widget/', '/livetv',
#                             'articlelist', '/live'
#                         ])):

#                         if href.startswith('/'):
#                             base = (site['url'].split('/')[0] + '//'
#                                     + site['url'].split('/')[2])
#                             href = base + href

#                         if href.startswith('http'):
#                             raw_articles.append({'title': title, 'link': href})

#                 # Deduplicate
#                 seen = set()
#                 unique_links = []
#                 for a in raw_articles:
#                     if a['link'] not in seen:
#                         unique_links.append(a)
#                         seen.add(a['link'])

#                 print(f"üìã Found {len(unique_links)} unique links")

#                 for article in unique_links[:site['fetch_limit']]:
#                     if len(site_articles) >= site['target']:
#                         break

#                     print(f"   üîó [{len(site_articles)+1}/{site['target']}] "
#                         f"{article['title'][:50]}...")

#                     markdown = await fetch_article_with_retry(crawler, article['link'])
#                     content  = markdown if markdown else article['title']
#                     content_hash = get_content_hash(article['title'], content)

#                     if content_hash not in processed_hashes:
#                         site_articles.append({
#                             'title':            article['title'],
#                             'link':             article['link'],  # ‚úÖ Always original URL
#                             'content':          content[:2500],
#                             'hash':             content_hash,
#                             'has_full_content': bool(markdown)
#                         })
#                         processed_hashes.add(content_hash)
#                         tag = "‚úÖ" if markdown else "‚ö†Ô∏è fallback"
#                         print(f"   {tag} [{len(site_articles)}/{site['target']}] "
#                             f"{article['title'][:50]}...")
#                     else:
#                         print(f"   üîÑ Duplicate skipped")

#                     await asyncio.sleep(1)

#                 print(f"\nüì¶ {site['name']}: {len(site_articles)}/{site['target']} collected")

#                 if site_articles:
#                     filtered = await smart_analyze_with_category(
#                         site_articles, site['name']
#                     )
#                     all_news.extend(filtered)
#                     print(f"üß† {site['name']}: Analyzed {len(filtered)} articles")

#             except Exception as e:
#                 print(f"‚ùå Error {site['name']}: {e}")

#             await asyncio.sleep(3)

#     return all_news


# # ============================================================
# # AI Categorization - sonar-pro + link preservation
# # ============================================================
# async def smart_analyze_with_category(articles: List[Dict], source_name: str):
#     global total_input_tokens, total_output_tokens, total_cost

#     all_filtered = []

#     # ‚úÖ FIX: Store original links BEFORE sending to AI
#     original_links = {article['title']: article['link'] for article in articles}

#     for i in range(0, len(articles), 5):
#         batch = articles[i:i+5]

#         articles_text = ""
#         for idx, article in enumerate(batch, i+1):
#             # ‚úÖ 500 chars is enough to categorize (saves tokens)
#             articles_text += f"#{idx}: {article['title']}\n{article['content'][:500]}\n---\n"

#         # ‚úÖ No link field in JSON - prevents AI from overwriting real URLs
#         prompt = f"""‡§Æ‡§∞‡§æ‡§†‡•Ä ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§ï: ‡§ñ‡§æ‡§≤‡•Ä‡§≤ ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ‡§Ç‡§®‡§æ category ‡§Ü‡§£‡§ø Marathi summary ‡§¶‡•ç‡§Ø‡§æ.

# ‚ö†Ô∏è ‡§®‡§ø‡§Ø‡§Æ: detailed_summary ‡§Ü‡§£‡§ø key_points ‡§´‡§ï‡•ç‡§§ ‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§≤‡§ø‡§π‡§æ.

# Categories: sports, general, crime, politics, education, economy, entertainment, horror

# JSON array (link field ‡§®‡§ï‡•ã):
# [{{"title":"EXACT ‡§∂‡•Ä‡§∞‡•ç‡§∑‡§ï","category":"cat","detailed_summary":"‡•ß‡•´‡•¶-‡•®‡•¶‡•¶ ‡§∂‡§¨‡•ç‡§¶ ‡§Æ‡§∞‡§æ‡§†‡•Ä ‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂ - ‡§ï‡•ã‡§£, ‡§ï‡§æ‡§Ø, ‡§ï‡•Å‡§†‡•á, ‡§ï‡•á‡§µ‡•ç‡§π‡§æ ‡§∏‡§π","importance":"high/medium/low","key_points":["‡§Æ‡§∞‡§æ‡§†‡•Ä ‡§Æ‡•Å‡§¶‡•ç‡§¶‡§æ ‡•ß","‡§Æ‡§∞‡§æ‡§†‡•Ä ‡§Æ‡•Å‡§¶‡•ç‡§¶‡§æ ‡•®","‡§Æ‡§∞‡§æ‡§†‡•Ä ‡§Æ‡•Å‡§¶‡•ç‡§¶‡§æ ‡•©"]}}]

# ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ:
# {articles_text}

# ‡§´‡§ï‡•ç‡§§ JSON array. Link field ‡§®‡§ï‡•ã."""

#         try:
#             response = perplexity_client.chat.completions.create(
#                 model=ANALYSIS_MODEL,
#                 messages=[
#                     {
#                         "role": "system",
#                         "content": "Return ONLY valid JSON array. No link field. Summaries in Marathi only."
#                     },
#                     {"role": "user", "content": prompt}
#                 ],
#                 temperature=0.2,
#                 max_tokens=3000
#             )

#             if hasattr(response, 'usage'):
#                 i_t = response.usage.prompt_tokens
#                 o_t = response.usage.completion_tokens
#                 total_input_tokens  += i_t
#                 total_output_tokens += o_t
#                 c = (i_t * ANALYSIS_INPUT_COST) + (o_t * ANALYSIS_OUTPUT_COST)
#                 total_cost += c
#                 print(f"   üìä {i_t}in + {o_t}out = ${c:.4f}")

#             content = response.choices[0].message.content
#             content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL).strip()
#             match = re.search(r'\[.*\]', content, re.DOTALL)

#             if match:
#                 parsed = json.loads(match.group())

#                 # ‚úÖ FIX: Restore original scraped links - never trust AI for URLs
#                 for art in parsed:
#                     ai_title = art.get('title', '')

#                     # Exact match first
#                     if ai_title in original_links:
#                         art['link'] = original_links[ai_title]
#                     else:
#                         # Fuzzy match by longest common substring
#                         best_link  = ''
#                         best_score = 0
#                         for orig_title, orig_link in original_links.items():
#                             score = sum(
#                                 1 for a, b in zip(ai_title[:40], orig_title[:40])
#                                 if a == b
#                             )
#                             if score > best_score:
#                                 best_score = score
#                                 best_link  = orig_link
#                         art['link'] = best_link

#                 all_filtered.extend(parsed)
#                 print(f"   ‚úÖ Categorized {len(parsed)} articles with original links")

#             else:
#                 print(f"   ‚ö†Ô∏è JSON failed - using fallback entries")
#                 for article in batch:
#                     all_filtered.append({
#                         'title':            article['title'],
#                         'category':         'general',
#                         'detailed_summary': article['content'][:300],
#                         'importance':       'medium',
#                         'link':             article['link'],  # ‚úÖ Always original
#                         'key_points':       [article['title']]
#                     })

#         except json.JSONDecodeError:
#             for article in batch:
#                 all_filtered.append({
#                     'title':            article['title'],
#                     'category':         'general',
#                     'detailed_summary': article['content'][:300],
#                     'importance':       'medium',
#                     'link':             article['link'],  # ‚úÖ Always original
#                     'key_points':       [article['title']]
#                 })
#         except Exception as e:
#             print(f"   ‚ùå AI error: {e}")

#         await asyncio.sleep(1.5)

#     for art in all_filtered:
#         art['source']     = source_name
#         art['scraped_at'] = datetime.now().isoformat()

#     return all_filtered


# # ============================================================
# # Script Generation - sonar-reasoning-pro + anti-refusal
# # ============================================================
# async def create_reel_script_single(news_article: Dict):
#     global total_input_tokens, total_output_tokens, total_cost

#     category = news_article.get('category', 'general')

#     # ‚úÖ FIX: Framed as formatting task - prevents identity refusal
#     system_prompt = """‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä ‡§è‡§ï ‡§Æ‡§∞‡§æ‡§†‡•Ä text formatter ‡§Ü‡§π‡§æ‡§§.
# ‡§¨‡§æ‡§§‡§Æ‡•Ä‡§ö‡•á facts ‡§µ‡§æ‡§™‡§∞‡•Ç‡§® structured ‡§Æ‡§∞‡§æ‡§†‡•Ä lines ‡§§‡§Ø‡§æ‡§∞ ‡§ï‡§∞‡§æ.

# Structure (15-18 lines total):
# - Line 1-2: ‡§ß‡§ï‡•ç‡§ï‡§æ‡§¶‡§æ‡§Ø‡§ï hook (‡§ò‡§ü‡§®‡•á‡§ö‡•Ä ‡§∏‡•Å‡§∞‡•Å‡§µ‡§æ‡§§)
# - Line 3-10: ‡§∏‡§∞‡•ç‡§µ facts (‡§®‡§æ‡§µ‡•á, ‡§†‡§ø‡§ï‡§æ‡§£, ‡§§‡§æ‡§∞‡•Ä‡§ñ, ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ ‡§∏‡§π)
# - Line 11-14: ‡§™‡•ç‡§∞‡§∂‡•ç‡§® / ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£ / ‡§ü‡•ç‡§µ‡§ø‡§∏‡•ç‡§ü
# - Line 15-18: CTA - ‡§∂‡•á‡§µ‡§ü‡§ö‡•Ä line ‡§®‡§ï‡•ç‡§ï‡•Ä ‡§π‡•Ä‡§ö ‡§Ö‡§∏‡§æ‡§µ‡•Ä:
#   "‡§§‡•Å‡§Æ‡§ö‡§Ç ‡§ï‡§æ‡§Ø ‡§Æ‡§§ ‡§Ü‡§π‡•á? ‡§ï‡§Æ‡•á‡§Ç‡§ü ‡§ï‡§∞‡•Ç‡§® ‡§∏‡§æ‡§Ç‡§ó‡§æ ‡§Ü‡§£‡§ø ‡§´‡•â‡§≤‡•ã ‡§ï‡§∞‡§æ ‡§ú‡§¨‡§∞‡•Ä ‡§ñ‡§¨‡§∞‡•Ä."

# ‡§®‡§ø‡§Ø‡§Æ:
# - ‡§∏‡§Ç‡§™‡•Ç‡§∞‡•ç‡§£ output ‡§´‡§ï‡•ç‡§§ ‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ (proper nouns ‡§∏‡•ã‡§°‡•Ç‡§®)
# - 15-18 lines, ‡§™‡•ç‡§∞‡§§‡•ç‡§Ø‡•á‡§ï line 1-2 sentences
# - ‡§ï‡•ã‡§£‡§§‡•á‡§π‡•Ä heading, explanation, markdown ‡§®‡§æ‡§π‡•Ä
# - ‡§´‡§ï‡•ç‡§§ script lines output ‡§ï‡§∞‡§æ"""

#     summary    = news_article.get('detailed_summary', news_article.get('title', ''))[:300]
#     key_points = ', '.join(news_article.get('key_points', [news_article.get('title', '')]))

#     # Standard prompt
#     user_prompt_v1 = f"""Category: {category.upper()}
# ‡§∂‡•Ä‡§∞‡•ç‡§∑‡§ï: {news_article['title']}
# ‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂: {summary}
# ‡§Æ‡•Å‡§¶‡•ç‡§¶‡•á: {key_points}

# ‡§µ‡§∞‡•Ä‡§≤ ‡§¨‡§æ‡§§‡§Æ‡•Ä‡§ö‡•á facts ‡§µ‡§æ‡§™‡§∞‡•Ç‡§® 15-18 ‡§Æ‡§∞‡§æ‡§†‡•Ä lines ‡§§‡§Ø‡§æ‡§∞ ‡§ï‡§∞‡§æ."""

#     # Stronger fallback prompt (used on refusal)
#     user_prompt_v2 = f"""‡§ñ‡§æ‡§≤‡•Ä‡§≤ ‡§¨‡§æ‡§§‡§Æ‡•Ä‡§ö‡•ç‡§Ø‡§æ facts ‡§µ‡§æ‡§™‡§∞‡•Ç‡§® 15 ‡§Æ‡§∞‡§æ‡§†‡•Ä ‡§µ‡§æ‡§ï‡•ç‡§Ø‡•á ‡§≤‡§ø‡§π‡§æ.
# ‡§¨‡§æ‡§§‡§Æ‡•Ä: {news_article['title']}. {summary[:200]}
# ‡§™‡•ç‡§∞‡§§‡•ç‡§Ø‡•á‡§ï ‡§µ‡§æ‡§ï‡•ç‡§Ø ‡§®‡§µ‡•Ä‡§® line ‡§µ‡§∞ ‡§≤‡§ø‡§π‡§æ.
# ‡§∂‡•á‡§µ‡§ü‡§ö‡•Ä line: "‡§§‡•Å‡§Æ‡§ö‡§Ç ‡§ï‡§æ‡§Ø ‡§Æ‡§§ ‡§Ü‡§π‡•á? ‡§ï‡§Æ‡•á‡§Ç‡§ü ‡§ï‡§∞‡•Ç‡§® ‡§∏‡§æ‡§Ç‡§ó‡§æ ‡§Ü‡§£‡§ø ‡§´‡•â‡§≤‡•ã ‡§ï‡§∞‡§æ ‡§ú‡§¨‡§∞‡•Ä ‡§ñ‡§¨‡§∞‡•Ä." """

#     prompts = [user_prompt_v1, user_prompt_v2]

#     for attempt in range(1, 3):
#         try:
#             response = perplexity_client.chat.completions.create(
#                 model=SCRIPT_MODEL,
#                 messages=[
#                     {"role": "system", "content": system_prompt},
#                     {"role": "user",   "content": prompts[attempt - 1]}
#                 ],
#                 temperature=0.8,
#                 max_tokens=1200
#             )

#             if hasattr(response, 'usage'):
#                 i_t = response.usage.prompt_tokens
#                 o_t = response.usage.completion_tokens
#                 total_input_tokens  += i_t
#                 total_output_tokens += o_t
#                 total_cost += (i_t * SCRIPT_INPUT_COST) + (o_t * SCRIPT_OUTPUT_COST)

#             script = response.choices[0].message.content.strip()
#             script = re.sub(r'<think>.*?</think>', '', script, flags=re.DOTALL).strip()
#             script = script.replace('```', '').strip()

#             if is_valid_marathi_script(script):
#                 return script

#             # Check reason for failure
#             is_refusal = any(kw.lower() in script.lower() for kw in REFUSAL_KEYWORDS)
#             if is_refusal:
#                 print(f"   ‚ö†Ô∏è Attempt {attempt}: Model refused - retrying...")
#             else:
#                 print(f"   ‚ö†Ô∏è Attempt {attempt}: Not valid Marathi - retrying...")

#         except Exception as e:
#             print(f"   ‚ö†Ô∏è Attempt {attempt} error: {e}")
#             await asyncio.sleep(2)

#     # ‚úÖ 100% Marathi hardcoded fallback - always valid
#     title = news_article.get('title', '‡§è‡§ï ‡§Æ‡§π‡§§‡•ç‡§§‡•ç‡§µ‡§æ‡§ö‡•Ä ‡§¨‡§æ‡§§‡§Æ‡•Ä')[:80]
#     return f"""‡§Æ‡§π‡§æ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§æ‡§§ ‡§è‡§ï ‡§Æ‡§π‡§§‡•ç‡§§‡•ç‡§µ‡§æ‡§ö‡•Ä ‡§ò‡§°‡§æ‡§Æ‡•ã‡§° ‡§∏‡§Æ‡•ã‡§∞ ‡§Ü‡§≤‡•Ä ‡§Ü‡§π‡•á.

# {title}

# ‡§π‡•Ä ‡§¨‡§æ‡§§‡§Æ‡•Ä ‡§∏‡§ß‡•ç‡§Ø‡§æ ‡§∏‡§∞‡•ç‡§µ‡§§‡•ç‡§∞ ‡§ö‡§∞‡•ç‡§ö‡•á‡§§ ‡§Ü‡§π‡•á.

# ‡§Ø‡§æ ‡§ò‡§ü‡§®‡•á‡§®‡•á ‡§Ö‡§®‡•á‡§ï‡§æ‡§Ç‡§®‡§æ ‡§Ü‡§∂‡•ç‡§ö‡§∞‡•ç‡§Ø‡§ö‡§ï‡§ø‡§§ ‡§ï‡•á‡§≤‡•á ‡§Ü‡§π‡•á.

# ‡§∏‡§∞‡•ç‡§µ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§®‡§æ‡§ó‡§∞‡§ø‡§ï‡§æ‡§Ç‡§µ‡§∞ ‡§Ø‡§æ‡§ö‡§æ ‡§•‡•á‡§ü ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ ‡§π‡•ã‡§£‡§æ‡§∞ ‡§Ü‡§π‡•á.

# ‡§™‡•ç‡§∞‡§∂‡§æ‡§∏‡§®‡§æ‡§®‡•á ‡§Ø‡§æ‡§¨‡§æ‡§¨‡§§ ‡§Ö‡§¶‡•ç‡§Ø‡§æ‡§™ ‡§Ö‡§ß‡§ø‡§ï‡•É‡§§ ‡§™‡•ç‡§∞‡§§‡§ø‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ ‡§¶‡§ø‡§≤‡•á‡§≤‡•Ä ‡§®‡§æ‡§π‡•Ä.

# ‡§µ‡§ø‡§∞‡•ã‡§ß‡§ï‡§æ‡§Ç‡§®‡•Ä ‡§Ø‡§æ ‡§®‡§ø‡§∞‡•ç‡§£‡§Ø‡§æ‡§µ‡§∞ ‡§ú‡•ã‡§∞‡§¶‡§æ‡§∞ ‡§ü‡•Ä‡§ï‡§æ ‡§ï‡•á‡§≤‡•Ä ‡§Ü‡§π‡•á.

# ‡§Ø‡•á‡§§‡•ç‡§Ø‡§æ ‡§ï‡§æ‡§π‡•Ä ‡§¶‡§ø‡§µ‡§∏‡§æ‡§Ç‡§§ ‡§Ø‡§æ‡§µ‡§∞ ‡§Æ‡•ã‡§†‡§æ ‡§®‡§ø‡§∞‡•ç‡§£‡§Ø ‡§π‡•ã‡§£‡•ç‡§Ø‡§æ‡§ö‡•Ä ‡§∂‡§ï‡•ç‡§Ø‡§§‡§æ ‡§Ü‡§π‡•á.

# ‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§≤‡§æ ‡§Ø‡§æ ‡§¨‡§æ‡§§‡§Æ‡•Ä‡§¨‡§¶‡•ç‡§¶‡§≤ ‡§ï‡§æ‡§Ø ‡§µ‡§æ‡§ü‡§§‡•á?

# ‡§Ø‡§æ ‡§™‡•ç‡§∞‡§ï‡§∞‡§£‡§æ‡§ï‡§°‡•á ‡§∏‡§∞‡•ç‡§µ‡§æ‡§Ç‡§ö‡•á ‡§≤‡§ï‡•ç‡§∑ ‡§≤‡§æ‡§ó‡§≤‡•á ‡§Ü‡§π‡•á.

# ‡§Ö‡§∂‡§æ ‡§Æ‡§π‡§§‡•ç‡§§‡•ç‡§µ‡§æ‡§ö‡•ç‡§Ø‡§æ ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ‡§Ç‡§∏‡§æ‡§†‡•Ä ‡§Ü‡§Æ‡§ö‡•á ‡§™‡•á‡§ú ‡§´‡•â‡§≤‡•ã ‡§ï‡§∞‡§æ.

# ‡§ú‡§¨‡§∞‡•Ä ‡§ñ‡§¨‡§∞‡•Ä ‡§∏‡•ã‡§¨‡§§ ‡§∞‡§æ‡§π‡§æ, ‡§∏‡§§‡•ç‡§Ø ‡§ú‡§æ‡§£‡•Ç‡§® ‡§ò‡•ç‡§Ø‡§æ.

# ‡§§‡•Å‡§Æ‡§ö‡§Ç ‡§ï‡§æ‡§Ø ‡§Æ‡§§ ‡§Ü‡§π‡•á? ‡§ï‡§Æ‡•á‡§Ç‡§ü ‡§ï‡§∞‡•Ç‡§® ‡§∏‡§æ‡§Ç‡§ó‡§æ ‡§Ü‡§£‡§ø ‡§´‡•â‡§≤‡•ã ‡§ï‡§∞‡§æ ‡§ú‡§¨‡§∞‡•Ä ‡§ñ‡§¨‡§∞‡•Ä."""


# # ============================================================
# # Main Pipeline
# # ============================================================
# async def main():
#     global total_input_tokens, total_output_tokens, total_cost

#     print("=" * 70)
#     print("üöÄ JABARI KHABRI - SMART NEWS SCRAPER v5.0")
#     print(f"üîç Analysis : {ANALYSIS_MODEL}")
#     print(f"‚úçÔ∏è  Scripts  : {SCRIPT_MODEL}")
#     print("=" * 70)
#     print(f"üìç Sites    : TV9, ABP Majha, Lokmat, Mah Times, NDTV Marathi")
#     print(f"üìä Target   : 10 √ó 5 = {TARGET_SCRIPTS} scripts STRICTLY")
#     print(f"üîÅ Buffer   : 40 links/site | üîÑ Retry: 3√ó fetch + 2√ó script")
#     print(f"üáÆüá≥ Language: ‡§Æ‡§∞‡§æ‡§†‡•Ä only | üîó Links: original URLs preserved")
#     print("=" * 70 + "\n")

#     start_time = datetime.now()

#     # ‚îÄ‚îÄ STEP 1: SCRAPING ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
#     print("=" * 70)
#     print("STEP 1: SCRAPING 5 MARATHI NEWS SITES")
#     print("=" * 70 + "\n")

#     all_articles = await scrape_multiple_marathi_sources()

#     # Final deduplication
#     unique_articles = []
#     seen_hashes = set()
#     for article in all_articles:
#         h = article.get('hash', get_content_hash(
#             article['title'], article.get('detailed_summary', '')
#         ))
#         if h not in seen_hashes:
#             unique_articles.append(article)
#             seen_hashes.add(h)

#     print(f"\n‚úÖ Total unique articles: {len(unique_articles)}")

#     # ‚úÖ FIX: named function - no more lambda crash
#     category_counts = {}
#     for article in unique_articles:
#         cat = article.get('category', 'general')
#         category_counts[cat] = category_counts.get(cat, 0) + 1

#     print("\nüìä Category Breakdown:")
#     for cat, count in sorted(category_counts.items(), key=sort_by_count):
#         bar = "‚ñà" * count
#         print(f"   {cat.upper():<15} {bar} ({count})")

#     # Sort by importance
#     unique_articles.sort(key=sort_by_priority)
#     selected_articles = unique_articles[:TARGET_SCRIPTS]

#     print(f"\nüéØ Selected : {len(selected_articles)}/{TARGET_SCRIPTS}")
#     print(f"‚è±Ô∏è  Scraping : {(datetime.now()-start_time).total_seconds():.0f}s\n")

#     # ‚îÄ‚îÄ STEP 2: SCRIPTS + SHEETS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
#     print("=" * 70)
#     print("STEP 2: GENERATING SCRIPTS ‚Üí GOOGLE SHEETS")
#     print("=" * 70 + "\n")

#     worksheet        = setup_google_sheets()
#     successful_saves = 0
#     failed_saves     = 0

#     if worksheet and selected_articles:
#         for idx, article in enumerate(selected_articles, 1):
#             print(f"\n[{idx:02d}/{len(selected_articles)}] "
#                 f"{article.get('source','')[:12]} | "
#                 f"{article.get('category','').upper():<13} | "
#                 f"{article['title'][:40]}...")

#             script = await create_reel_script_single(article)

#             # ‚úÖ Log Marathi percentage for debugging
#             dev_chars  = len(re.findall(r'[\u0900-\u097F]', script))
#             total_ch   = len(script.replace(' ', '').replace('\n', ''))
#             marathi_pct = (dev_chars / max(total_ch, 1)) * 100
#             lang_tag    = "üáÆüá≥" if marathi_pct > 35 else "‚ö†Ô∏è Non-Marathi"
#             print(f"   üìù {lang_tag} ({marathi_pct:.0f}% Devanagari) | "
#                 f"üîó {article.get('link','')[:60]}...")

#             success = save_to_google_sheets(
#                 worksheet,
#                 article.get('category', 'general'),
#                 article['title'],
#                 script,
#                 article.get('link', '')
#             )
#             if success:
#                 successful_saves += 1
#             else:
#                 failed_saves += 1

#             await asyncio.sleep(1)

#         print("\n" + "=" * 70)
#         print(f"   ‚úÖ Saved  : {successful_saves}/{len(selected_articles)}")
#         print(f"   ‚ùå Failed : {failed_saves}")
#         print(f"   üìä Sheet  : https://docs.google.com/spreadsheets/d/"
#             f"{worksheet.spreadsheet.id}")

#     else:
#         print("‚ö†Ô∏è No articles or Sheets unavailable!")

#     # ‚îÄ‚îÄ FINAL SUMMARY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
#     total_duration = (datetime.now() - start_time).total_seconds()
#     total_tokens   = total_input_tokens + total_output_tokens

#     print("\n" + "=" * 70)
#     print("üìà FINAL SUMMARY")
#     print("=" * 70)
#     print(f"   üîç Analysis model     : {ANALYSIS_MODEL}")
#     print(f"   ‚úçÔ∏è  Script model       : {SCRIPT_MODEL}")
#     print(f"   üì∞ Articles scraped   : {len(unique_articles)}")
#     print(f"   ‚úÖ Scripts generated  : {successful_saves}")
#     print(f"   ‚è±Ô∏è  Total time         : {total_duration:.0f}s ({total_duration/60:.1f} mins)")
#     print(f"   üì• Input tokens       : {total_input_tokens:,}")
#     print(f"   üì§ Output tokens      : {total_output_tokens:,}")
#     print(f"   üî¢ Total tokens       : {total_tokens:,}")
#     print(f"   üí∞ Total cost         : ${total_cost:.4f} (~‚Çπ{total_cost*84:.2f})")
#     print(f"   üíµ Cost per script    : ${total_cost/max(successful_saves,1):.4f}")
#     print("=" * 70 + "\n")


# if __name__ == "__main__":
#     asyncio.run(main())





# -----------------------------------------------


import asyncio
import json
from datetime import datetime
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode
from bs4 import BeautifulSoup
from openai import OpenAI
import re
import gspread
from google.oauth2.service_account import Credentials
import os
from typing import List, Dict
import hashlib


# ============================================================
# Custom Exceptions
# ============================================================
class CreditExhaustedException(Exception):
    """Raised when Perplexity API credits are exhausted"""
    pass


# ============================================================
# Perplexity Client
# ============================================================
perplexity_client = OpenAI(
    api_key=os.environ.get("PERPLEXITY_API_KEY"),
    base_url="https://api.perplexity.ai"
)

ANALYSIS_MODEL       = "sonar-pro"
SCRIPT_MODEL         = "sonar-reasoning-pro"

ANALYSIS_INPUT_COST  = 1.0 / 1_000_000
ANALYSIS_OUTPUT_COST = 1.0 / 1_000_000
SCRIPT_INPUT_COST    = 2.0 / 1_000_000
SCRIPT_OUTPUT_COST   = 8.0 / 1_000_000


# ============================================================
# Config
# ============================================================
GOOGLE_SHEETS_CREDENTIALS_FILE = "credentials.json"
GOOGLE_SHEET_NAME               = "Instagram Scripts"
GOOGLE_WORKSHEET_NAME           = "Scripts"
TARGET_SCRIPTS                  = 10

VALID_CATEGORIES = [
    "sports", "general", "crime", "politics",
    "education", "economy", "entertainment", "horror"
]

REFUSAL_KEYWORDS = [
    "I appreciate", "I should clarify", "I'm Perplexity",
    "search assistant", "I'm not able", "I cannot create",
    "Would you like", "clarify my role", "I'm an AI",
    "as an AI", "I don't create"
]


# ============================================================
# Token Tracking
# ============================================================
total_input_tokens  = 0
total_output_tokens = 0
total_cost          = 0.0
processed_hashes    = set()


# ============================================================
# News Sites
# ============================================================
NEWS_SITES = [
    {
        "name": "TV9 Marathi",
        "url": "https://www.tv9marathi.com/latest-news",
        "link_pattern": "tv9marathi.com",
        "target": 2,
        "fetch_limit": 10
    },
    {
        "name": "ABP Majha",
        "url": "https://marathi.abplive.com/news",
        "link_pattern": "abplive.com",
        "target": 2,
        "fetch_limit": 10
    },
    {
        "name": "Lokmat",
        "url": "https://www.lokmat.com/latestnews/",
        "link_pattern": "lokmat.com",
        "target": 2,
        "fetch_limit": 10
    },
    {
        "name": "Maharashtra Times",
        "url": "https://maharashtratimes.com/",
        "link_pattern": "maharashtratimes.com",
        "target": 2,
        "fetch_limit": 10
    },
    {
        "name": "NDTV Marathi",
        "url": "https://marathi.ndtv.com/",
        "link_pattern": "marathi.ndtv.com",
        "target": 2,
        "fetch_limit": 10
    }
]


# ============================================================
# Google Sheets Setup
# ============================================================
def setup_google_sheets():
    try:
        scope = [
            'https://spreadsheets.google.com/feeds',
            'https://www.googleapis.com/auth/drive'
        ]
        creds = Credentials.from_service_account_file(
            GOOGLE_SHEETS_CREDENTIALS_FILE, scopes=scope
        )
        client = gspread.authorize(creds)

        try:
            sheet = client.open(GOOGLE_SHEET_NAME)
            print(f"‚úÖ Connected: '{GOOGLE_SHEET_NAME}'")
        except gspread.SpreadsheetNotFound:
            sheet = client.create(GOOGLE_SHEET_NAME)
            print(f"‚úÖ Created: '{GOOGLE_SHEET_NAME}'")

        try:
            worksheet = sheet.worksheet(GOOGLE_WORKSHEET_NAME)
            print(f"‚úÖ Worksheet: '{GOOGLE_WORKSHEET_NAME}'")
        except gspread.WorksheetNotFound:
            worksheet = sheet.add_worksheet(
                title=GOOGLE_WORKSHEET_NAME, rows=5000, cols=10
            )
            worksheet.update('A1:E1', [[
                'Timestamp', 'Category', 'Title', 'Script', 'Source Link'
            ]])
            worksheet.format('A1:E1', {
                'textFormat': {
                    'bold': True,
                    'foregroundColor': {'red': 1.0, 'green': 1.0, 'blue': 1.0}
                },
                'backgroundColor': {'red': 0.2, 'green': 0.6, 'blue': 0.9},
                'horizontalAlignment': 'CENTER'
            })
            worksheet.set_column_width('A', 180)
            worksheet.set_column_width('B', 150)
            worksheet.set_column_width('C', 400)
            worksheet.set_column_width('D', 600)
            worksheet.set_column_width('E', 400)
            print(f"‚úÖ Created worksheet with headers")

        return worksheet

    except FileNotFoundError:
        print(f"‚ùå credentials.json not found!")
        return None
    except Exception as e:
        print(f"‚ùå Sheets setup error: {e}")
        import traceback
        traceback.print_exc()
        return None


# ============================================================
# Save to Google Sheets
# ============================================================
def save_to_google_sheets(worksheet, category, title, script, source_link):
    try:
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        script = '\n'.join(str(i) for i in script) if isinstance(script, list) else str(script).strip()
        script = script.replace('[', '').replace(']', '')
        title = str(title).strip()
        source_link = str(source_link).strip()
        category = str(category).strip().lower()

        if category not in VALID_CATEGORIES:
            category = "general"

        next_row = len(worksheet.get_all_values()) + 1
        worksheet.append_row(
            [timestamp, category, title, script, source_link],
            value_input_option='RAW'
        )

        worksheet.format(f'A{next_row}:E{next_row}', {
            'textFormat': {
                'foregroundColor': {'red': 0.0, 'green': 0.0, 'blue': 0.0},
                'fontSize': 10
            },
            'backgroundColor': {'red': 1.0, 'green': 1.0, 'blue': 1.0},
            'wrapStrategy': 'WRAP',
            'verticalAlignment': 'TOP'
        })

        category_colors = {
            'crime':         {'red': 0.95, 'green': 0.8,  'blue': 0.8},
            'politics':      {'red': 0.8,  'green': 0.9,  'blue': 1.0},
            'sports':        {'red': 0.8,  'green': 1.0,  'blue': 0.8},
            'entertainment': {'red': 1.0,  'green': 0.9,  'blue': 0.8},
            'education':     {'red': 0.9,  'green': 0.95, 'blue': 1.0},
            'economy':       {'red': 0.95, 'green': 1.0,  'blue': 0.85},
            'horror':        {'red': 0.7,  'green': 0.7,  'blue': 0.7},
            'general':       {'red': 1.0,  'green': 1.0,  'blue': 0.9}
        }
        worksheet.format(f'B{next_row}', {
            'textFormat': {
                'bold': True,
                'foregroundColor': {'red': 0.0, 'green': 0.0, 'blue': 0.0},
                'fontSize': 10
            },
            'backgroundColor': category_colors.get(category, category_colors['general']),
            'horizontalAlignment': 'CENTER'
        })

        print(f"‚úÖ Saved [{category.upper()}] {title[:50]}...")
        return True

    except Exception as e:
        print(f"‚ùå Save error: {e}")
        return False


# ============================================================
# Content Hash
# ============================================================
def get_content_hash(title: str, content: str) -> str:
    return hashlib.md5(
        f"{title.lower()}{content[:200].lower()}".encode()
    ).hexdigest()


# ============================================================
# Sort helpers
# ============================================================
def sort_by_count(item):
    return -item[1]


def sort_by_priority(item):
    priority_order = {'high': 1, 'medium': 2, 'low': 3}
    return priority_order.get(item.get('importance', 'medium'), 2)


# ============================================================
# Marathi Validator
# ============================================================
def is_valid_marathi_script(script: str) -> bool:
    if len(script) < 100:
        return False
    if any(kw.lower() in script.lower() for kw in REFUSAL_KEYWORDS):
        return False
    devanagari = len(re.findall(r'[\u0900-\u097F]', script))
    total      = len(script.replace(' ', '').replace('\n', ''))
    return (devanagari / max(total, 1)) > 0.35


# ============================================================
# API Credit Check
# ============================================================
async def check_api_credits():
    try:
        perplexity_client.chat.completions.create(
            model=ANALYSIS_MODEL,
            messages=[{"role": "user", "content": "ok"}],
            max_tokens=1
        )
        print("‚úÖ API credits OK")
        return True
    except Exception as e:
        error_str = str(e).lower()
        if any(code in error_str for code in [
            '402', '429', '401',
            'insufficient', 'credit', 'quota',
            'balance', 'payment', 'billing',
            'rate limit', 'exceeded'
        ]):
            print("=" * 60)
            print("‚ùå PERPLEXITY API CREDITS EXHAUSTED!")
            print(f"   Error: {str(e)}")
            print("=" * 60)
            print("üëâ Top up: https://www.perplexity.ai/settings/api")
            return False
        print(f"‚ùå Unknown API error: {e}")
        return False


# ============================================================
# Fetch Article with Retry
# ============================================================
async def fetch_article_with_retry(crawler, url: str, retries: int = 3) -> str:
    for attempt in range(1, retries + 1):
        try:
            result = await crawler.arun(
                url,
                config=CrawlerRunConfig(
                    cache_mode=CacheMode.BYPASS,
                    word_count_threshold=10,
                    page_timeout=25000
                )
            )
            if result.success and len(result.markdown) > 50:
                return result.markdown
            await asyncio.sleep(2)
        except Exception:
            await asyncio.sleep(2)
    return ""


# ============================================================
# Scraping - 10 per site guaranteed
# ============================================================
async def scrape_multiple_marathi_sources():
    all_news = []

    async with AsyncWebCrawler(verbose=False) as crawler:
        for site in NEWS_SITES:
            print(f"\n{'='*60}")
            print(f"üîç {site['name']} | Target: {site['target']}")
            print(f"{'='*60}")

            site_articles = []

            try:
                result = await crawler.arun(
                    site['url'],
                    config=CrawlerRunConfig(
                        cache_mode=CacheMode.BYPASS,
                        wait_for="body",
                        word_count_threshold=10,
                        page_timeout=30000,
                        js_code="await new Promise(r => setTimeout(r, 3000));"
                    )
                )

                if not result.success:
                    print(f"‚ùå Failed: {site['name']}")
                    continue

                soup = BeautifulSoup(result.html, 'html.parser')
                raw_articles = []

                for link_tag in soup.find_all('a', href=True):
                    href  = link_tag.get('href', '')
                    title = link_tag.get_text(strip=True)

                    if (15 < len(title) < 300 and
                        site['link_pattern'] in href and
                        not any(x in href.lower() for x in [
                            'javascript:', 'mailto:', '#',
                            '/category/', '/tag/', '/author/',
                            'facebook.com', 'twitter.com', 'instagram.com',
                            'youtube.com', 'whatsapp.com', '/myaccount/',
                            '/install_app', '/advertisement', '/epaper',
                            'web-stories', 'photo-gallery', '/videos/',
                            '/games/', '/jokes/', '/terms-and-conditions',
                            '/topic/', '/widget/', '/livetv',
                            'articlelist', '/live'
                        ])):

                        if href.startswith('/'):
                            base = (site['url'].split('/')[0] + '//'
                                    + site['url'].split('/')[2])
                            href = base + href

                        if href.startswith('http'):
                            raw_articles.append({'title': title, 'link': href})

                # Deduplicate
                seen = set()
                unique_links = []
                for a in raw_articles:
                    if a['link'] not in seen:
                        unique_links.append(a)
                        seen.add(a['link'])

                print(f"üìã Found {len(unique_links)} unique links")

                for article in unique_links[:site['fetch_limit']]:
                    if len(site_articles) >= site['target']:
                        break

                    print(f"   üîó [{len(site_articles)+1}/{site['target']}] "
                        f"{article['title'][:50]}...")

                    markdown = await fetch_article_with_retry(crawler, article['link'])
                    content  = markdown if markdown else article['title']
                    content_hash = get_content_hash(article['title'], content)

                    if content_hash not in processed_hashes:
                        site_articles.append({
                            'title':            article['title'],
                            'link':             article['link'],
                            'content':          content[:2500],
                            'hash':             content_hash,
                            'has_full_content': bool(markdown)
                        })
                        processed_hashes.add(content_hash)
                        tag = "‚úÖ" if markdown else "‚ö†Ô∏è fallback"
                        print(f"   {tag} [{len(site_articles)}/{site['target']}] "
                            f"{article['title'][:50]}...")
                    else:
                        print(f"   üîÑ Duplicate skipped")

                    await asyncio.sleep(1)

                print(f"\nüì¶ {site['name']}: {len(site_articles)}/{site['target']} collected")

                if site_articles:
                    filtered = await smart_analyze_with_category(
                        site_articles, site['name']
                    )
                    all_news.extend(filtered)
                    print(f"üß† {site['name']}: Analyzed {len(filtered)} articles")

            except Exception as e:
                print(f"‚ùå Error {site['name']}: {e}")

            await asyncio.sleep(3)

    return all_news


# ============================================================
# AI Categorization - INDEX-BASED link preservation
# ============================================================
async def smart_analyze_with_category(articles: List[Dict], source_name: str):
    global total_input_tokens, total_output_tokens, total_cost

    all_filtered = []

    for batch_start in range(0, len(articles), 5):
        batch = articles[batch_start:batch_start + 5]

        # ‚úÖ Store links and titles BY POSITION INDEX
        index_to_link  = {i: article['link']  for i, article in enumerate(batch)}
        index_to_title = {i: article['title'] for i, article in enumerate(batch)}

        articles_text = ""
        for idx, article in enumerate(batch):
            articles_text += f"INDEX_{idx}: {article['title']}\n{article['content'][:500]}\n---\n"

        prompt = f"""‡§Æ‡§∞‡§æ‡§†‡•Ä ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§ï: ‡§ñ‡§æ‡§≤‡•Ä‡§≤ ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ‡§Ç‡§®‡§æ category ‡§Ü‡§£‡§ø Marathi summary ‡§¶‡•ç‡§Ø‡§æ.

‚ö†Ô∏è ‡§®‡§ø‡§Ø‡§Æ:
1. detailed_summary ‡§Ü‡§£‡§ø key_points ‡§´‡§ï‡•ç‡§§ ‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§≤‡§ø‡§π‡§æ
2. JSON ‡§Æ‡§ß‡•ç‡§Ø‡•á "index" field EXACTLY ‡§ú‡§∏‡§æ ‡§¶‡§ø‡§≤‡§æ (0,1,2,3,4) ‡§§‡§∏‡§æ‡§ö ‡§™‡§∞‡§§ ‡§¶‡•ç‡§Ø‡§æ
3. title ‡§Ü‡§£‡§ø link field ‡§®‡§ï‡•ã - ‡§´‡§ï‡•ç‡§§ index ‡§µ‡§æ‡§™‡§∞‡§æ

Categories: sports, general, crime, politics, education, economy, entertainment, horror

JSON array format:
[{{"index": 0, "category": "cat", "detailed_summary": "‡§Æ‡§∞‡§æ‡§†‡•Ä ‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂ ‡•ß‡•´‡•¶-‡•®‡•¶‡•¶ ‡§∂‡§¨‡•ç‡§¶", "importance": "high/medium/low", "key_points": ["‡§Æ‡•Å‡§¶‡•ç‡§¶‡§æ ‡•ß", "‡§Æ‡•Å‡§¶‡•ç‡§¶‡§æ ‡•®", "‡§Æ‡•Å‡§¶‡•ç‡§¶‡§æ ‡•©"]}}]

‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ:
{articles_text}

‡§´‡§ï‡•ç‡§§ JSON array. Index 0 ‡§§‡•á {len(batch)-1} ‡§™‡§∞‡•ç‡§Ø‡§Ç‡§§."""

        try:
            response = perplexity_client.chat.completions.create(
                model=ANALYSIS_MODEL,
                messages=[
                    {
                        "role": "system",
                        "content": "Return ONLY valid JSON array. Use index field (0,1,2...). No title or link fields."
                    },
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
                max_tokens=3000
            )

            if hasattr(response, 'usage'):
                i_t = response.usage.prompt_tokens
                o_t = response.usage.completion_tokens
                total_input_tokens  += i_t
                total_output_tokens += o_t
                c = (i_t * ANALYSIS_INPUT_COST) + (o_t * ANALYSIS_OUTPUT_COST)
                total_cost += c
                print(f"   üìä {i_t}in + {o_t}out = ${c:.4f}")

            content = response.choices[0].message.content
            content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL).strip()

            # ‚úÖ CORRECT regex - single backslash, not double
            match = re.search(r'\[.*\]', content, re.DOTALL)

            if match:
                parsed = json.loads(match.group())

                for art in parsed:
                    idx = art.get('index')

                    # ‚úÖ Always restore link and title from index
                    if idx is not None and idx in index_to_link:
                        art['link']  = index_to_link[idx]
                        art['title'] = index_to_title[idx]
                    else:
                        # Fallback: sequential position
                        pos = len(all_filtered) % len(batch)
                        art['link']  = index_to_link.get(pos, '')
                        art['title'] = index_to_title.get(pos, art.get('title', ''))

                    # Validate category
                    if art.get('category') not in VALID_CATEGORIES:
                        art['category'] = 'general'

                all_filtered.extend(parsed)
                print(f"   ‚úÖ Categorized {len(parsed)} | Links: INDEX-matched ‚úÖ")

            else:
                print(f"   ‚ö†Ô∏è JSON failed - fallback entries")
                for i, article in enumerate(batch):
                    all_filtered.append({
                        'index':            i,
                        'title':            article['title'],
                        'category':         'general',
                        'detailed_summary': article['content'][:300],
                        'importance':       'medium',
                        'link':             article['link'],
                        'key_points':       [article['title']]
                    })

        except json.JSONDecodeError as e:
            print(f"   ‚ùå JSON parse error: {e}")
            for i, article in enumerate(batch):
                all_filtered.append({
                    'index':            i,
                    'title':            article['title'],
                    'category':         'general',
                    'detailed_summary': article['content'][:300],
                    'importance':       'medium',
                    'link':             article['link'],
                    'key_points':       [article['title']]
                })
        except Exception as e:
            error_str = str(e).lower()
            if any(code in error_str for code in [
                '402', '429', 'credit', 'quota',
                'insufficient', 'balance', 'billing'
            ]):
                print(f"\nüí≥ CREDITS EXHAUSTED during analysis!")
                raise CreditExhaustedException(str(e))
            print(f"   ‚ùå AI error: {e}")

        await asyncio.sleep(1.5)

    for art in all_filtered:
        art['source']     = source_name
        art['scraped_at'] = datetime.now().isoformat()

    return all_filtered


# ============================================================
# Script Generation
# ============================================================
async def create_reel_script_single(news_article: Dict):
    global total_input_tokens, total_output_tokens, total_cost

    category = news_article.get('category', 'general')

    system_prompt = """‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä ‡§è‡§ï ‡§Æ‡§∞‡§æ‡§†‡•Ä text formatter ‡§Ü‡§π‡§æ‡§§.
‡§¨‡§æ‡§§‡§Æ‡•Ä‡§ö‡•á facts ‡§µ‡§æ‡§™‡§∞‡•Ç‡§® structured ‡§Æ‡§∞‡§æ‡§†‡•Ä lines ‡§§‡§Ø‡§æ‡§∞ ‡§ï‡§∞‡§æ.

Structure (15-18 lines total):
- Line 1-2: ‡§ß‡§ï‡•ç‡§ï‡§æ‡§¶‡§æ‡§Ø‡§ï hook (‡§ò‡§ü‡§®‡•á‡§ö‡•Ä ‡§∏‡•Å‡§∞‡•Å‡§µ‡§æ‡§§)
- Line 3-10: ‡§∏‡§∞‡•ç‡§µ facts (‡§®‡§æ‡§µ‡•á, ‡§†‡§ø‡§ï‡§æ‡§£, ‡§§‡§æ‡§∞‡•Ä‡§ñ, ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ ‡§∏‡§π)
- Line 11-14: ‡§™‡•ç‡§∞‡§∂‡•ç‡§® / ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£ / ‡§ü‡•ç‡§µ‡§ø‡§∏‡•ç‡§ü
- Line 15-18: CTA - ‡§∂‡•á‡§µ‡§ü‡§ö‡•Ä line ‡§®‡§ï‡•ç‡§ï‡•Ä ‡§π‡•Ä‡§ö ‡§Ö‡§∏‡§æ‡§µ‡•Ä:
"‡§§‡•Å‡§Æ‡§ö‡§Ç ‡§ï‡§æ‡§Ø ‡§Æ‡§§ ‡§Ü‡§π‡•á? ‡§ï‡§Æ‡•á‡§Ç‡§ü ‡§ï‡§∞‡•Ç‡§® ‡§∏‡§æ‡§Ç‡§ó‡§æ ‡§Ü‡§£‡§ø ‡§´‡•â‡§≤‡•ã ‡§ï‡§∞‡§æ ‡§ú‡§¨‡§∞‡•Ä ‡§ñ‡§¨‡§∞‡•Ä."

‡§®‡§ø‡§Ø‡§Æ:
- ‡§∏‡§Ç‡§™‡•Ç‡§∞‡•ç‡§£ output ‡§´‡§ï‡•ç‡§§ ‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ (proper nouns ‡§∏‡•ã‡§°‡•Ç‡§®)
- 15-18 lines, ‡§™‡•ç‡§∞‡§§‡•ç‡§Ø‡•á‡§ï line 1-2 sentences
- ‡§ï‡•ã‡§£‡§§‡•á‡§π‡•Ä heading, explanation, markdown ‡§®‡§æ‡§π‡•Ä
- ‡§´‡§ï‡•ç‡§§ script lines output ‡§ï‡§∞‡§æ"""

    summary    = news_article.get('detailed_summary', news_article.get('title', ''))[:300]
    key_points = ', '.join(news_article.get('key_points', [news_article.get('title', '')]))

    user_prompt_v1 = f"""Category: {category.upper()}
‡§∂‡•Ä‡§∞‡•ç‡§∑‡§ï: {news_article['title']}
‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂: {summary}
‡§Æ‡•Å‡§¶‡•ç‡§¶‡•á: {key_points}

‡§µ‡§∞‡•Ä‡§≤ ‡§¨‡§æ‡§§‡§Æ‡•Ä‡§ö‡•á facts ‡§µ‡§æ‡§™‡§∞‡•Ç‡§® 15-18 ‡§Æ‡§∞‡§æ‡§†‡•Ä lines ‡§§‡§Ø‡§æ‡§∞ ‡§ï‡§∞‡§æ."""

    user_prompt_v2 = f"""‡§ñ‡§æ‡§≤‡•Ä‡§≤ ‡§¨‡§æ‡§§‡§Æ‡•Ä‡§ö‡•ç‡§Ø‡§æ facts ‡§µ‡§æ‡§™‡§∞‡•Ç‡§® 15 ‡§Æ‡§∞‡§æ‡§†‡•Ä ‡§µ‡§æ‡§ï‡•ç‡§Ø‡•á ‡§≤‡§ø‡§π‡§æ.
‡§¨‡§æ‡§§‡§Æ‡•Ä: {news_article['title']}. {summary[:200]}
‡§™‡•ç‡§∞‡§§‡•ç‡§Ø‡•á‡§ï ‡§µ‡§æ‡§ï‡•ç‡§Ø ‡§®‡§µ‡•Ä‡§® line ‡§µ‡§∞ ‡§≤‡§ø‡§π‡§æ.
‡§∂‡•á‡§µ‡§ü‡§ö‡•Ä line: "‡§§‡•Å‡§Æ‡§ö‡§Ç ‡§ï‡§æ‡§Ø ‡§Æ‡§§ ‡§Ü‡§π‡•á? ‡§ï‡§Æ‡•á‡§Ç‡§ü ‡§ï‡§∞‡•Ç‡§® ‡§∏‡§æ‡§Ç‡§ó‡§æ ‡§Ü‡§£‡§ø ‡§´‡•â‡§≤‡•ã ‡§ï‡§∞‡§æ ‡§ú‡§¨‡§∞‡•Ä ‡§ñ‡§¨‡§∞‡•Ä." """

    prompts = [user_prompt_v1, user_prompt_v2]

    for attempt in range(1, 3):
        try:
            response = perplexity_client.chat.completions.create(
                model=SCRIPT_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user",   "content": prompts[attempt - 1]}
                ],
                temperature=0.8,
                max_tokens=1200
            )

            if hasattr(response, 'usage'):
                i_t = response.usage.prompt_tokens
                o_t = response.usage.completion_tokens
                total_input_tokens  += i_t
                total_output_tokens += o_t
                total_cost += (i_t * SCRIPT_INPUT_COST) + (o_t * SCRIPT_OUTPUT_COST)

            script = response.choices[0].message.content.strip()
            script = re.sub(r'<think>.*?</think>', '', script, flags=re.DOTALL).strip()
            script = script.replace('```', '').strip()

            if is_valid_marathi_script(script):
                return script

            is_refusal = any(kw.lower() in script.lower() for kw in REFUSAL_KEYWORDS)
            if is_refusal:
                print(f"   ‚ö†Ô∏è Attempt {attempt}: Model refused - retrying...")
            else:
                print(f"   ‚ö†Ô∏è Attempt {attempt}: Not valid Marathi - retrying...")

        except Exception as e:
            error_str = str(e).lower()
            if any(code in error_str for code in [
                '402', '429', 'credit', 'quota',
                'insufficient', 'balance', 'billing'
            ]):
                print(f"\nüí≥ CREDITS EXHAUSTED during script generation!")
                raise CreditExhaustedException(str(e))
            print(f"   ‚ö†Ô∏è Attempt {attempt} error: {e}")
            await asyncio.sleep(2)

    # 100% Marathi hardcoded fallback
    title = news_article.get('title', '‡§è‡§ï ‡§Æ‡§π‡§§‡•ç‡§§‡•ç‡§µ‡§æ‡§ö‡•Ä ‡§¨‡§æ‡§§‡§Æ‡•Ä')[:80]
    return f"""‡§Æ‡§π‡§æ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§æ‡§§ ‡§è‡§ï ‡§Æ‡§π‡§§‡•ç‡§§‡•ç‡§µ‡§æ‡§ö‡•Ä ‡§ò‡§°‡§æ‡§Æ‡•ã‡§° ‡§∏‡§Æ‡•ã‡§∞ ‡§Ü‡§≤‡•Ä ‡§Ü‡§π‡•á.

{title}

‡§π‡•Ä ‡§¨‡§æ‡§§‡§Æ‡•Ä ‡§∏‡§ß‡•ç‡§Ø‡§æ ‡§∏‡§∞‡•ç‡§µ‡§§‡•ç‡§∞ ‡§ö‡§∞‡•ç‡§ö‡•á‡§§ ‡§Ü‡§π‡•á.

‡§Ø‡§æ ‡§ò‡§ü‡§®‡•á‡§®‡•á ‡§Ö‡§®‡•á‡§ï‡§æ‡§Ç‡§®‡§æ ‡§Ü‡§∂‡•ç‡§ö‡§∞‡•ç‡§Ø‡§ö‡§ï‡§ø‡§§ ‡§ï‡•á‡§≤‡•á ‡§Ü‡§π‡•á.

‡§∏‡§∞‡•ç‡§µ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§®‡§æ‡§ó‡§∞‡§ø‡§ï‡§æ‡§Ç‡§µ‡§∞ ‡§Ø‡§æ‡§ö‡§æ ‡§•‡•á‡§ü ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ ‡§π‡•ã‡§£‡§æ‡§∞ ‡§Ü‡§π‡•á.

‡§™‡•ç‡§∞‡§∂‡§æ‡§∏‡§®‡§æ‡§®‡•á ‡§Ø‡§æ‡§¨‡§æ‡§¨‡§§ ‡§Ö‡§¶‡•ç‡§Ø‡§æ‡§™ ‡§Ö‡§ß‡§ø‡§ï‡•É‡§§ ‡§™‡•ç‡§∞‡§§‡§ø‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ ‡§¶‡§ø‡§≤‡•á‡§≤‡•Ä ‡§®‡§æ‡§π‡•Ä.

‡§µ‡§ø‡§∞‡•ã‡§ß‡§ï‡§æ‡§Ç‡§®‡•Ä ‡§Ø‡§æ ‡§®‡§ø‡§∞‡•ç‡§£‡§Ø‡§æ‡§µ‡§∞ ‡§ú‡•ã‡§∞‡§¶‡§æ‡§∞ ‡§ü‡•Ä‡§ï‡§æ ‡§ï‡•á‡§≤‡•Ä ‡§Ü‡§π‡•á.

‡§Ø‡•á‡§§‡•ç‡§Ø‡§æ ‡§ï‡§æ‡§π‡•Ä ‡§¶‡§ø‡§µ‡§∏‡§æ‡§Ç‡§§ ‡§Ø‡§æ‡§µ‡§∞ ‡§Æ‡•ã‡§†‡§æ ‡§®‡§ø‡§∞‡•ç‡§£‡§Ø ‡§π‡•ã‡§£‡•ç‡§Ø‡§æ‡§ö‡•Ä ‡§∂‡§ï‡•ç‡§Ø‡§§‡§æ ‡§Ü‡§π‡•á.

‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§≤‡§æ ‡§Ø‡§æ ‡§¨‡§æ‡§§‡§Æ‡•Ä‡§¨‡§¶‡•ç‡§¶‡§≤ ‡§ï‡§æ‡§Ø ‡§µ‡§æ‡§ü‡§§‡•á?

‡§Ø‡§æ ‡§™‡•ç‡§∞‡§ï‡§∞‡§£‡§æ‡§ï‡§°‡•á ‡§∏‡§∞‡•ç‡§µ‡§æ‡§Ç‡§ö‡•á ‡§≤‡§ï‡•ç‡§∑ ‡§≤‡§æ‡§ó‡§≤‡•á ‡§Ü‡§π‡•á.

‡§Ö‡§∂‡§æ ‡§Æ‡§π‡§§‡•ç‡§§‡•ç‡§µ‡§æ‡§ö‡•ç‡§Ø‡§æ ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ‡§Ç‡§∏‡§æ‡§†‡•Ä ‡§Ü‡§Æ‡§ö‡•á ‡§™‡•á‡§ú ‡§´‡•â‡§≤‡•ã ‡§ï‡§∞‡§æ.

‡§ú‡§¨‡§∞‡•Ä ‡§ñ‡§¨‡§∞‡•Ä ‡§∏‡•ã‡§¨‡§§ ‡§∞‡§æ‡§π‡§æ, ‡§∏‡§§‡•ç‡§Ø ‡§ú‡§æ‡§£‡•Ç‡§® ‡§ò‡•ç‡§Ø‡§æ.

‡§§‡•Å‡§Æ‡§ö‡§Ç ‡§ï‡§æ‡§Ø ‡§Æ‡§§ ‡§Ü‡§π‡•á? ‡§ï‡§Æ‡•á‡§Ç‡§ü ‡§ï‡§∞‡•Ç‡§® ‡§∏‡§æ‡§Ç‡§ó‡§æ ‡§Ü‡§£‡§ø ‡§´‡•â‡§≤‡•ã ‡§ï‡§∞‡§æ ‡§ú‡§¨‡§∞‡•Ä ‡§ñ‡§¨‡§∞‡•Ä."""


# ============================================================
# Main Pipeline
# ============================================================
async def main():
    global total_input_tokens, total_output_tokens, total_cost

    print("=" * 70)
    print("üöÄ JABARI KHABRI - SMART NEWS SCRAPER v5.0")
    print(f"üîç Analysis : {ANALYSIS_MODEL}")
    print(f"‚úçÔ∏è  Scripts  : {SCRIPT_MODEL}")
    print("=" * 70)

    # ‚úÖ STEP 0: CHECK CREDITS BEFORE ANYTHING
    credits_ok = await check_api_credits()
    if not credits_ok:
        print("\nüõë Stopping. Top up credits first.")
        print("üëâ https://www.perplexity.ai/settings/api")
        return

    start_time = datetime.now()

    # ‚îÄ‚îÄ STEP 1: SCRAPING ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    print("\n" + "=" * 70)
    print("STEP 1: SCRAPING 5 MARATHI NEWS SITES")
    print("=" * 70 + "\n")

    try:
        all_articles = await scrape_multiple_marathi_sources()
    except CreditExhaustedException:
        print("\nüõë Credits exhausted during scraping. Stopping.")
        return

    # Final deduplication
    unique_articles = []
    seen_hashes = set()
    for article in all_articles:
        h = article.get('hash', get_content_hash(
            article['title'], article.get('detailed_summary', '')
        ))
        if h not in seen_hashes:
            unique_articles.append(article)
            seen_hashes.add(h)

    print(f"\n‚úÖ Total unique articles: {len(unique_articles)}")

    # Category breakdown
    category_counts = {}
    for article in unique_articles:
        cat = article.get('category', 'general')
        category_counts[cat] = category_counts.get(cat, 0) + 1

    print("\nüìä Category Breakdown:")
    for cat, count in sorted(category_counts.items(), key=sort_by_count):
        bar = "‚ñà" * count
        print(f"   {cat.upper():<15} {bar} ({count})")

    # Sort by importance
    unique_articles.sort(key=sort_by_priority)
    selected_articles = unique_articles[:TARGET_SCRIPTS]

    print(f"\nüéØ Selected : {len(selected_articles)}/{TARGET_SCRIPTS}")
    print(f"‚è±Ô∏è  Scraping : {(datetime.now()-start_time).total_seconds():.0f}s\n")

    # ‚îÄ‚îÄ STEP 2: SCRIPTS + SHEETS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    print("=" * 70)
    print("STEP 2: GENERATING SCRIPTS ‚Üí GOOGLE SHEETS")
    print("=" * 70 + "\n")

    worksheet        = setup_google_sheets()
    successful_saves = 0
    failed_saves     = 0

    if worksheet and selected_articles:
        for idx, article in enumerate(selected_articles, 1):
            print(f"\n[{idx:02d}/{len(selected_articles)}] "
                f"{article.get('source','')[:12]} | "
                f"{article.get('category','').upper():<13} | "
                f"{article['title'][:40]}...")

            try:
                script = await create_reel_script_single(article)
            except CreditExhaustedException:
                print(f"\nüõë Credits exhausted at script {idx}/{len(selected_articles)}")
                print(f"   ‚úÖ Saved so far: {successful_saves} scripts")
                print(f"üëâ Top up: https://www.perplexity.ai/settings/api")
                break

            # Log Marathi %
            dev_chars   = len(re.findall(r'[\u0900-\u097F]', script))
            total_ch    = len(script.replace(' ', '').replace('\n', ''))
            marathi_pct = (dev_chars / max(total_ch, 1)) * 100
            lang_tag    = "üáÆüá≥" if marathi_pct > 35 else "‚ö†Ô∏è"
            print(f"   üìù {lang_tag} ({marathi_pct:.0f}%) | "
                f"üîó {article.get('link','')[:60]}...")

            success = save_to_google_sheets(
                worksheet,
                article.get('category', 'general'),
                article['title'],
                script,
                article.get('link', '')
            )
            if success:
                successful_saves += 1
            else:
                failed_saves += 1

            await asyncio.sleep(1)

    # ‚îÄ‚îÄ FINAL SUMMARY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    total_duration = (datetime.now() - start_time).total_seconds()
    total_tokens   = total_input_tokens + total_output_tokens

    print("\n" + "=" * 70)
    print("üìà FINAL SUMMARY")
    print("=" * 70)
    print(f"   üîç Analysis model     : {ANALYSIS_MODEL}")
    print(f"   ‚úçÔ∏è  Script model       : {SCRIPT_MODEL}")
    print(f"   üì∞ Articles scraped   : {len(unique_articles)}")
    print(f"   ‚úÖ Scripts saved      : {successful_saves}")
    print(f"   ‚ùå Failed             : {failed_saves}")
    print(f"   ‚è±Ô∏è  Total time         : {total_duration:.0f}s ({total_duration/60:.1f} mins)")
    print(f"   üì• Input tokens       : {total_input_tokens:,}")
    print(f"   üì§ Output tokens      : {total_output_tokens:,}")
    print(f"   üî¢ Total tokens       : {total_tokens:,}")
    print(f"   üí∞ Total cost         : ${total_cost:.4f} (~‚Çπ{total_cost*84:.2f})")
    print(f"   üíµ Cost per script    : ${total_cost/max(successful_saves,1):.4f}")
    if worksheet:
        print(f"   üìä Sheet URL          : https://docs.google.com/spreadsheets/d/"
            f"{worksheet.spreadsheet.id}")
    print("=" * 70 + "\n")


if __name__ == "__main__":
    asyncio.run(main())
