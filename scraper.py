
# import asyncio
# import json
# from datetime import datetime
# from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode
# from bs4 import BeautifulSoup
# from openai import OpenAI
# import re
# import gspread
# from google.oauth2.service_account import Credentials
# import os


# # Initialize Perplexity client from environment variable
# perplexity_client = OpenAI(
#     api_key=os.environ.get("PERPLEXITY_API_KEY"),  # Read from GitHub Secrets
#     base_url="https://api.perplexity.ai"
# )


# # Google Sheets Configuration
# GOOGLE_SHEETS_CREDENTIALS_FILE = "credentials.json"  # Created dynamically by GitHub Actions
# GOOGLE_SHEET_NAME = "Instagram Scripts"
# GOOGLE_WORKSHEET_NAME = "Scripts"


# # Track token usage (CORRECTED: $1 per 1M tokens)
# total_tokens_used = 0
# total_cost = 0.0


# def setup_google_sheets():
#     """
#     Initialize Google Sheets connection
#     """
#     try:
#         # Define the scope
#         scope = [
#             'https://spreadsheets.google.com/feeds',
#             'https://www.googleapis.com/auth/drive'
#         ]
        
#         # Load credentials
#         creds = Credentials.from_service_account_file(
#             GOOGLE_SHEETS_CREDENTIALS_FILE, 
#             scopes=scope
#         )
        
#         # Authorize and connect
#         client = gspread.authorize(creds)
        
#         # Open or create spreadsheet
#         try:
#             sheet = client.open(GOOGLE_SHEET_NAME)
#             print(f"‚úÖ Connected to existing sheet: '{GOOGLE_SHEET_NAME}'")
#         except gspread.SpreadsheetNotFound:
#             sheet = client.create(GOOGLE_SHEET_NAME)
#             print(f"‚úÖ Created new sheet: '{GOOGLE_SHEET_NAME}'")
        
#         # Open or create worksheet
#         try:
#             worksheet = sheet.worksheet(GOOGLE_WORKSHEET_NAME)
#             print(f"‚úÖ Using worksheet: '{GOOGLE_WORKSHEET_NAME}'")
#         except gspread.WorksheetNotFound:
#             worksheet = sheet.add_worksheet(
#                 title=GOOGLE_WORKSHEET_NAME,
#                 rows=1000,
#                 cols=10
#             )
#             # Add headers (only 4 columns now)
#             worksheet.update('A1:D1', [[
#                 'Timestamp',
#                 'Title',
#                 'Script',
#                 'Source Link'
#             ]])
            
#             # Format headers (bold, colored background, white text)
#             worksheet.format('A1:D1', {
#                 'textFormat': {
#                     'bold': True,
#                     'foregroundColor': {'red': 1.0, 'green': 1.0, 'blue': 1.0}  # White text
#                 },
#                 'backgroundColor': {'red': 0.2, 'green': 0.6, 'blue': 0.9},  # Blue background
#                 'horizontalAlignment': 'CENTER'
#             })
            
#             # Set column widths
#             worksheet.set_column_width('A', 180)  # Timestamp
#             worksheet.set_column_width('B', 400)  # Title
#             worksheet.set_column_width('C', 600)  # Script (wide)
#             worksheet.set_column_width('D', 400)  # Source Link
            
#             print(f"‚úÖ Created new worksheet with headers: '{GOOGLE_WORKSHEET_NAME}'")
        
#         return worksheet
        
#     except FileNotFoundError:
#         print(f"‚ùå Error: '{GOOGLE_SHEETS_CREDENTIALS_FILE}' not found!")
#         print("üí° This file is created automatically by GitHub Actions")
#         return None
#     except Exception as e:
#         print(f"‚ùå Google Sheets setup error: {e}")
#         import traceback
#         traceback.print_exc()
#         return None


# def save_to_google_sheets(worksheet, script, source_link, news_title):
#     """
#     Append script data to Google Sheets with proper formatting
#     Only saves: Timestamp, Title, Script, Source Link
#     """
#     try:
#         # Get current timestamp
#         timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
#         # Ensure all values are proper strings
#         if isinstance(script, list):
#             script = '\n'.join(str(item) for item in script)
#         else:
#             script = str(script).strip()
        
#         # Clean up any remaining brackets
#         script = script.replace('[', '').replace(']', '')
        
#         # Ensure other fields are strings
#         news_title = str(news_title).strip()
#         source_link = str(source_link).strip()
        
#         # Prepare row data (4 columns)
#         row_data = [
#             timestamp,
#             news_title,
#             script,
#             source_link
#         ]
        
#         # Get next row number
#         next_row = len(worksheet.get_all_values()) + 1
        
#         # Append to the sheet with RAW string values
#         worksheet.append_row(row_data, value_input_option='RAW')
        
#         # Format the newly added row (BLACK text, white background, wrap text)
#         row_range = f'A{next_row}:D{next_row}'
#         worksheet.format(row_range, {
#             'textFormat': {
#                 'foregroundColor': {'red': 0.0, 'green': 0.0, 'blue': 0.0},
#                 'fontSize': 10
#             },
#             'backgroundColor': {'red': 1.0, 'green': 1.0, 'blue': 1.0},
#             'wrapStrategy': 'WRAP',
#             'verticalAlignment': 'TOP'
#         })
        
#         # Format Script column (C) - wrap and left align
#         worksheet.format(f'C{next_row}', {
#             'textFormat': {
#                 'foregroundColor': {'red': 0.0, 'green': 0.0, 'blue': 0.0},
#                 'fontSize': 10
#             },
#             'wrapStrategy': 'WRAP',
#             'verticalAlignment': 'TOP',
#             'horizontalAlignment': 'LEFT'
#         })
        
#         # Format Title column (B) - left align
#         worksheet.format(f'B{next_row}', {
#             'textFormat': {
#                 'foregroundColor': {'red': 0.0, 'green': 0.0, 'blue': 0.0},
#                 'fontSize': 10
#             },
#             'wrapStrategy': 'WRAP',
#             'verticalAlignment': 'TOP',
#             'horizontalAlignment': 'LEFT'
#         })
        
#         # Format link column (D) - clickable blue
#         worksheet.format(f'D{next_row}', {
#             'textFormat': {
#                 'foregroundColor': {'red': 0.06, 'green': 0.27, 'blue': 0.8},
#                 'fontSize': 10,
#                 'underline': True
#             },
#             'wrapStrategy': 'WRAP',
#             'verticalAlignment': 'TOP'
#         })
        
#         print(f"‚úÖ Script saved to Google Sheets!")
#         print(f"   Row #{next_row} added with timestamp: {timestamp}")
        
#         return True
        
#     except Exception as e:
#         print(f"‚ùå Error saving to Google Sheets: {e}")
#         import traceback
#         traceback.print_exc()
#         return False


# async def scrape_marathi_news_final():
#     """
#     Scraper that collects articles from all three sites
#     """
    
#     news_sites = [
#         {
#             "name": "TV9 Marathi",
#             "url": "https://www.tv9marathi.com/latest-news",
#             "article_selector": "article, div.story-card, div.news-item",
#             "link_pattern": "tv9marathi.com"
#         },
#         {
#             "name": "ABP Majha",
#             "url": "https://marathi.abplive.com/news",
#             "article_selector": "article, div.story-box, div.news-card",
#             "link_pattern": "abplive.com"
#         },
#         {
#             "name": "Lokmat",
#             "url": "https://www.lokmat.com/latestnews/",
#             "article_selector": "article, div.story-card, div.card-body",
#             "link_pattern": "lokmat.com"
#         }
#     ]
    
#     all_news = []
    
#     async with AsyncWebCrawler(verbose=True) as crawler:
        
#         for site in news_sites:
#             print(f"\nüîç Scraping {site['name']}...")
            
#             try:
#                 # Fetch homepage with JavaScript rendering
#                 config = CrawlerRunConfig(
#                     cache_mode=CacheMode.BYPASS,
#                     wait_for="body",
#                     word_count_threshold=10,
#                     page_timeout=30000,
#                     js_code="""
#                     // Wait for content to load
#                     await new Promise(r => setTimeout(r, 2000));
#                     """
#                 )
                
#                 result = await crawler.arun(site['url'], config=config)
                
#                 if result.success:
#                     soup = BeautifulSoup(result.html, 'html.parser')
                    
#                     raw_articles = []
#                     all_links = soup.find_all('a', href=True)
                    
#                     for link_tag in all_links:
#                         href = link_tag.get('href', '')
#                         title = link_tag.get_text(strip=True)
                        
#                         # Filter valid news links
#                         if (len(title) > 15 and len(title) < 300 and
#                             site['link_pattern'] in href and
#                             not any(x in href for x in [
#                                 'javascript:', 'mailto:', '#', 
#                                 '/category/', '/tag/', '/author/',
#                                 'facebook.com', 'twitter.com', 'instagram.com',
#                                 'youtube.com', 'whatsapp.com', '/myaccount/',
#                                 '/install_app', '/advertisement', '/epaper',
#                                 'web-stories', 'photo-gallery', '/videos/',
#                                 '/sakhi/', '/astro/', '/bhakti/', '/games/',
#                                 '/jokes/', '/terms-and-conditions', '/utility-news',
#                                 '/spiritual-adhyatmik', '/rashi-bhavishya', 
#                                 '/topic/', '/elections/', '/career/'
#                             ])):
                            
#                             # Make absolute URL
#                             if href.startswith('/'):
#                                 base_url = site['url'].split('/')[0] + '//' + site['url'].split('/')[2]
#                                 href = base_url + href
                            
#                             if href.startswith('http'):
#                                 raw_articles.append({
#                                     'title': title,
#                                     'link': href
#                                 })
                    
#                     # Remove duplicates by link
#                     seen_links = set()
#                     unique_articles = []
#                     for article in raw_articles:
#                         if article['link'] not in seen_links:
#                             unique_articles.append(article)
#                             seen_links.add(article['link'])
                    
#                     print(f"üìã Found {len(unique_articles)} unique articles from {site['name']}")
                    
#                     if len(unique_articles) > 0:
#                         print(f"üìÑ Fetching detailed content from top {min(12, len(unique_articles))} articles...")
                        
#                         articles_with_content = []
#                         for article in unique_articles[:12]:
#                             try:
#                                 article_result = await crawler.arun(
#                                     article['link'],
#                                     config=CrawlerRunConfig(
#                                         cache_mode=CacheMode.BYPASS,
#                                         word_count_threshold=50,
#                                         page_timeout=15000
#                                     )
#                                 )
                                
#                                 if article_result.success and len(article_result.markdown) > 100:
#                                     articles_with_content.append({
#                                         'title': article['title'],
#                                         'link': article['link'],
#                                         'content': article_result.markdown[:2500]
#                                     })
#                                     print(f"   ‚úì {article['title'][:60]}...")
                                    
#                             except Exception as e:
#                                 continue
                        
#                         print(f"‚úÖ Fetched content for {len(articles_with_content)} articles")
                        
#                         # AI analysis
#                         if articles_with_content:
#                             filtered_news = await smart_analyze_with_detailed_summary(
#                                 articles_with_content, 
#                                 site['name']
#                             )
#                             all_news.extend(filtered_news)
#                             print(f"‚úÖ Extracted {len(filtered_news)} important articles")
#                     else:
#                         print(f"‚ö†Ô∏è No articles found from {site['name']}")
                
#                 else:
#                     print(f"‚ùå Failed to fetch {site['name']}: {result.error_message}")
                    
#             except Exception as e:
#                 print(f"‚ùå Error scraping {site['name']}: {e}")
#                 import traceback
#                 traceback.print_exc()
    
#     return all_news


# async def smart_analyze_with_detailed_summary(articles, source_name):
#     """
#     AI analysis with token tracking ($1 per 1M tokens)
#     """
#     global total_tokens_used, total_cost
    
#     print(f"\nüß† Using AI for detailed analysis of {source_name} articles...")
    
#     all_filtered = []
    
#     # Process in batches of 5
#     for i in range(0, len(articles), 5):
#         batch = articles[i:i+5]
        
#         articles_text = ""
#         for idx, article in enumerate(batch, i+1):
#             articles_text += f"""
# ‡§¨‡§æ‡§§‡§Æ‡•Ä #{idx}:
# ‡§∂‡•Ä‡§∞‡•ç‡§∑‡§ï: {article['title']}
# Link: {article['link']}
# Content: {article['content'][:1200]}

# ---
# """
        
#         prompt = f"""
# ‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä ‡§è‡§ï ‡§§‡§ú‡•ç‡§û ‡§Æ‡§∞‡§æ‡§†‡•Ä ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§ï ‡§Ü‡§π‡§æ‡§§. ‡§ñ‡§æ‡§≤‡•Ä‡§≤ ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ‡§Ç‡§ö‡•á ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£ ‡§ï‡§∞‡§æ.

# **‡§´‡§ï‡•ç‡§§ ‡§π‡•á ‡§™‡•ç‡§∞‡§ï‡§æ‡§∞ ‡§®‡§ø‡§µ‡§°‡§æ:**
# 1. ‡§ó‡•Å‡§®‡•ç‡§π‡•á‡§ó‡§æ‡§∞‡•Ä ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ (Crime) - ‡§π‡§§‡•ç‡§Ø‡§æ, ‡§¶‡§∞‡•ã‡§°‡§æ, ‡§Ö‡§™‡§ò‡§æ‡§§, ‡§Ö‡§ü‡§ï, ‡§≤‡§æ‡§ö
# 2. ‡§∞‡§æ‡§ú‡§ï‡•Ä‡§Ø ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ (Political) - ‡§®‡§ø‡§µ‡§°‡§£‡•Å‡§ï‡§æ, ‡§∏‡§∞‡§ï‡§æ‡§∞, ‡§Æ‡§π‡§æ‡§™‡§æ‡§≤‡§ø‡§ï‡§æ, ‡§∞‡§æ‡§ú‡§ï‡•Ä‡§Ø ‡§ò‡§°‡§æ‡§Æ‡•ã‡§°‡•Ä
# 3. ‡§Æ‡§π‡§§‡•ç‡§§‡•ç‡§µ‡§æ‡§ö‡•ç‡§Ø‡§æ ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ (Important General) - ‡§∂‡§æ‡§∏‡§ï‡•Ä‡§Ø ‡§®‡§ø‡§∞‡•ç‡§£‡§Ø, ‡§∏‡§æ‡§Æ‡§æ‡§ú‡§ø‡§ï ‡§Æ‡•Å‡§¶‡•ç‡§¶‡•á

# **‡§ü‡§æ‡§≥‡§æ‡§µ‡•á:** ‡§Æ‡§®‡•ã‡§∞‡§Ç‡§ú‡§® gossip, ‡§ú‡•ç‡§Ø‡•ã‡§§‡§ø‡§∑, ‡§´‡•Ö‡§∂‡§®, lifestyle, ‡§ñ‡•á‡§≥‡§æ‡§ö‡•Ä ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§¨‡§æ‡§§‡§Æ‡•Ä, job posts, ‡§ß‡§æ‡§∞‡•ç‡§Æ‡§ø‡§ï ‡§ï‡§•‡§æ, Bigg Boss, ‡§¨‡•â‡§≤‡•Ä‡§µ‡•Ç‡§° gossip

# **JSON format (‡§´‡§ï‡•ç‡§§ array ‡§™‡§∞‡§§ ‡§ï‡§∞‡§æ):**
# [
#   {{
#     "title": "‡§Æ‡•Ç‡§≥ ‡§∂‡•Ä‡§∞‡•ç‡§∑‡§ï",
#     "category": "crime/politics/general",
#     "detailed_summary": "‡§∏‡§Ç‡§™‡•Ç‡§∞‡•ç‡§£ ‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§ ‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂ 150-250 ‡§∂‡§¨‡•ç‡§¶‡§æ‡§Ç‡§§ ‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§",
#     "importance": "high/medium/low",
#     "link": "URL",
#     "article_number": number,
#     "key_points": ["‡§Æ‡•Å‡§¶‡•ç‡§¶‡§æ 1", "‡§Æ‡•Å‡§¶‡•ç‡§¶‡§æ 2", "‡§Æ‡•Å‡§¶‡•ç‡§¶‡§æ 3"]
#   }}
# ]

# {articles_text}
# """
        
#         try:
#             response = perplexity_client.chat.completions.create(
#                 model="sonar-pro",
#                 messages=[
#                     {
#                         "role": "system",
#                         "content": "You are an expert Marathi news analyst. Return ONLY valid JSON array."
#                     },
#                     {
#                         "role": "user",
#                         "content": prompt
#                     }
#                 ],
#                 temperature=0.3,
#                 max_tokens=4000
#             )
            
#             # Track tokens
#             if hasattr(response, 'usage'):
#                 batch_tokens = response.usage.total_tokens
#                 total_tokens_used += batch_tokens
#                 batch_cost = (batch_tokens / 1_000_000) * 1.0
#                 total_cost += batch_cost
#                 print(f"   üìä Batch tokens: {batch_tokens:,} | Cost: ${batch_cost:.4f}")
            
#             content = response.choices[0].message.content
#             json_match = re.search(r'\[.*\]', content, re.DOTALL)
            
#             if json_match:
#                 batch_articles = json.loads(json_match.group())
#                 all_filtered.extend(batch_articles)
#                 print(f"   ‚úÖ Extracted {len(batch_articles)} articles")
            
#         except Exception as e:
#             print(f"   ‚ùå AI analysis error: {e}")
    
#     # Add source and timestamp
#     for article in all_filtered:
#         article['source'] = source_name
#         article['scraped_at'] = datetime.now().isoformat()
    
#     return all_filtered


# def create_reel_script(news_articles):
#     """
#     Generate Instagram Reel script from news articles
#     Returns: (script, source_link, news_title)
#     """
#     global total_tokens_used, total_cost
    
#     # Prepare news context
#     news_context = ""
#     for idx, article in enumerate(news_articles[:5], 1):
#         news_context += f"""
# ‡§¨‡§æ‡§§‡§Æ‡•Ä #{idx}:
# ‡§∂‡•Ä‡§∞‡•ç‡§∑‡§ï: {article['title']}
# ‡§™‡•ç‡§∞‡§ï‡§æ‡§∞: {article['category']}
# ‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂: {article['detailed_summary']}
# ‡§Æ‡§π‡§§‡•ç‡§§‡•ç‡§µ: {article['importance']}
# ‡§≤‡§ø‡§Ç‡§ï: {article['link']}
# ---
# """
    
#     # System prompt with diverse hook examples
#     system_prompt = """
# ‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä "‡§ú‡§¨‡§∞‡•Ä ‡§ñ‡§¨‡§∞‡•Ä" Instagram Reels ‡§ö‡•á ‡§è‡§ï‡•ç‡§∏‡§™‡§∞‡•ç‡§ü ‡§∏‡•ç‡§ï‡•ç‡§∞‡§ø‡§™‡•ç‡§ü ‡§∞‡§æ‡§Ø‡§ü‡§∞ ‡§Ü‡§π‡§æ‡§§.

# **HOOK VARIETY (‡§™‡§π‡§ø‡§≤‡•ç‡§Ø‡§æ 2 ‡§ì‡§≥‡•Ä):**
# 1. Shock Statement: "‡§è‡§ï‡§æ ‡§Ö‡§™‡§ò‡§æ‡§§‡§æ‡§®‡•á ‡§∏‡§Ç‡§™‡•Ç‡§∞‡•ç‡§£ ‡§∞‡§æ‡§ú‡•ç‡§Ø ‡§π‡§æ‡§¶‡§∞‡§≤‡§Ç."
# 2. Direct Question: "‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§≤‡§æ ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä ‡§Ü‡§π‡•á ‡§ï‡§æ...?"
# 3. Breaking News: "‡§®‡•Å‡§ï‡§§‡•Ä‡§ö ‡§è‡§ï ‡§ß‡§ï‡•ç‡§ï‡§æ‡§¶‡§æ‡§Ø‡§ï ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä ‡§∏‡§Æ‡•ã‡§∞ ‡§Ü‡§≤‡•Ä."
# 4. Name Drop: "[‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡•Ä ‡§®‡§æ‡§µ] ‡§Ü‡§ú ‡§ö‡§∞‡•ç‡§ö‡•á‡§§ ‡§ï‡§æ ‡§Ü‡§π‡•á?"
# 5. Contrast/Twist: "‡§¶‡§ø‡§∏‡§§‡§Ç ‡§ï‡§æ‡§π‡•Ä‡§§‡§∞‡•Ä, ‡§™‡§£ ‡§µ‡§æ‡§∏‡•ç‡§§‡§µ ‡§µ‡•á‡§ó‡§≥‡§Ç‡§ö."

# **‡§∏‡•ç‡§ü‡•ç‡§∞‡§ï‡•ç‡§ö‡§∞ (40-60 seconds):**
# - ‡§™‡§π‡§ø‡§≤‡•á 2 ‡§ì‡§≥‡•Ä: Hook (‡§µ‡•á‡§ó‡§≥‡•Ä style)
# - 3-10 ‡§ì‡§≥‡•Ä: ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§ò‡§ü‡§®‡§æ
# - 11-14 ‡§ì‡§≥‡•Ä: ‡§ü‡•ç‡§µ‡§ø‡§∏‡•ç‡§ü/‡§™‡•ç‡§∞‡§∂‡•ç‡§®
# - ‡§∂‡•á‡§µ‡§ü‡§ö‡•ç‡§Ø‡§æ 2-3 ‡§ì‡§≥‡•Ä: CTA

# **ENDING:**
# "‡§§‡•Å‡§Æ‡§ö‡§Ç ‡§ï‡§æ‡§Ø ‡§Æ‡§§ ‡§Ü‡§π‡•á? ‡§ï‡§Æ‡•á‡§Ç‡§ü ‡§ï‡§∞‡•Ç‡§® ‡§∏‡§æ‡§Ç‡§ó‡§æ ‡§Ü‡§£‡§ø ‡§´‡•â‡§≤‡•ã ‡§ï‡§∞‡§æ ‡§ú‡§¨‡§∞‡•Ä ‡§ñ‡§¨‡§∞‡•Ä."

# **OUTPUT:** ‡§´‡§ï‡•ç‡§§ ‡§∏‡•ç‡§ï‡•ç‡§∞‡§ø‡§™‡•ç‡§ü. 15-18 ‡§ì‡§≥‡•Ä. ‡§∂‡•á‡§µ‡§ü‡•Ä: ---ARTICLE_NUMBER: X---
# """
    
#     user_prompt = f"""
# ‡§ñ‡§æ‡§≤‡•Ä‡§≤ ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ‡§Ç‡§™‡•à‡§ï‡•Ä ‡§∏‡§∞‡•ç‡§µ‡§æ‡§§ ENGAGING ‡§¨‡§æ‡§§‡§Æ‡•Ä ‡§®‡§ø‡§µ‡§°‡•Ç‡§® Instagram Reel script ‡§§‡§Ø‡§æ‡§∞ ‡§ï‡§∞‡§æ.

# **‡§Ü‡§ú‡§ö‡•ç‡§Ø‡§æ ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ:**
# {news_context}

# **‡§§‡•Å‡§Æ‡§ö‡§Ç ‡§ï‡§æ‡§Æ:**
# 1. ‡§∏‡§∞‡•ç‡§µ‡§æ‡§§ STRONG ‡§¨‡§æ‡§§‡§Æ‡•Ä ‡§®‡§ø‡§µ‡§°‡§æ
# 2. ‡§Ø‡•ã‡§ó‡•ç‡§Ø hook style ‡§µ‡§æ‡§™‡§∞‡§æ
# 3. 15-18 ‡§ì‡§≥‡•Ä‡§Ç‡§ö‡•Ä script ‡§≤‡§ø‡§π‡§æ
# 4. ‡§∂‡•á‡§µ‡§ü‡•Ä article number ‡§¶‡•ç‡§Ø‡§æ

# OUTPUT: [‡§∏‡•ç‡§ï‡•ç‡§∞‡§ø‡§™‡•ç‡§ü]\n\n---ARTICLE_NUMBER: X---
# """
    
#     try:
#         response = perplexity_client.chat.completions.create(
#             model="sonar-pro",
#             messages=[
#                 {"role": "system", "content": system_prompt},
#                 {"role": "user", "content": user_prompt}
#             ],
#             temperature=0.8,
#             max_tokens=1500
#         )
        
#         # Track tokens
#         if hasattr(response, 'usage'):
#             script_tokens = response.usage.total_tokens
#             total_tokens_used += script_tokens
#             total_cost += (script_tokens / 1_000_000) * 1.0
        
#         full_response = response.choices[0].message.content.strip()
        
#         # Extract script and article number
#         if "---ARTICLE_NUMBER:" in full_response:
#             parts = full_response.split("---ARTICLE_NUMBER:")
#             script = parts[0].strip()
#             article_num_str = parts[1].strip().replace("---", "").strip()
#             try:
#                 article_num = int(article_num_str) - 1
#             except:
#                 article_num = 0
#         else:
#             script = full_response
#             article_num = 0
        
#         # Clean up
#         script = script.replace('```', '').strip()
#         script = script.replace('---ARTICLE_NUMBER:', '').strip()
        
#         # Get source article
#         if article_num < len(news_articles):
#             source_article = news_articles[article_num]
#         else:
#             source_article = news_articles
        
#         source_link = source_article.get('link', 'N/A')
#         news_title = source_article.get('title', 'N/A')
        
#         return script, source_link, news_title
        
#     except Exception as e:
#         print(f"‚ùå Error generating script: {e}")
#         return None, None, None


# async def main():
#     global total_tokens_used, total_cost
    
#     print("üöÄ Starting Smart Marathi News Scraper + Script Generator")
#     print("üìç Focus: Criminal, Political & Important General News")
#     print("üìù Feature: Detailed summaries + Instagram Scripts")
#     print("üí∞ Token tracking enabled")
#     print("üìä Output: Direct to Google Sheets\n")
    
#     start_time = datetime.now()
    
#     # PART 1: SCRAPING
#     all_articles = await scrape_marathi_news_final()
    
#     # Remove duplicates
#     unique_articles = []
#     seen_titles = set()
    
#     for article in all_articles:
#         title_lower = article['title'].lower()
#         if title_lower not in seen_titles:
#             unique_articles.append(article)
#             seen_titles.add(title_lower)
    
#     # Sort by importance
#     priority_order = {'high': 1, 'medium': 2, 'low': 3}
#     unique_articles.sort(key=lambda x: priority_order.get(x.get('importance', 'medium'), 2))
    
#     top_10_articles = unique_articles[:10]
    
#     end_scrape = datetime.now()
#     scrape_duration = (end_scrape - start_time).total_seconds()
    
#     # Scraping summary
#     print("\n" + "="*80)
#     print("üìä SCRAPING SUMMARY")
#     print("="*80)
#     print(f"   Total articles: {len(unique_articles)}")
#     print(f"   High importance: {len([a for a in unique_articles if a.get('importance') == 'high'])}")
#     print(f"   Crime: {len([a for a in unique_articles if a.get('category') == 'crime'])}")
#     print(f"   Political: {len([a for a in unique_articles if a.get('category') == 'politics'])}")
#     print(f"   General: {len([a for a in unique_articles if a.get('category') == 'general'])}")
#     print(f"\n‚è±Ô∏è  Scraping time: {scrape_duration:.2f} seconds")
#     print("="*80 + "\n")
    
#     # PART 2: SCRIPT GENERATION
#     print("="*80)
#     print("üé¨ GENERATING INSTAGRAM SCRIPT")
#     print("="*80 + "\n")
    
#     worksheet = setup_google_sheets()
    
#     if worksheet and len(top_10_articles) > 0:
#         print(f"\nüéØ Generating script from TOP 10 articles...\n")
        
#         script, source_link, news_title = create_reel_script(top_10_articles)
        
#         if script:
#             print("\n" + "="*70)
#             print("üìù GENERATED SCRIPT:")
#             print("="*70)
#             print(script)
#             print("\n" + "="*70)
#             print(f"üì∞ Title: {news_title}")
#             print(f"üîó Source: {source_link}")
#             print("="*70 + "\n")
            
#             success = save_to_google_sheets(worksheet, script, source_link, news_title)
            
#             if success:
#                 print(f"üìà View sheet: https://docs.google.com/spreadsheets/d/{worksheet.spreadsheet.id}")
#         else:
#             print("‚ùå Failed to generate script")
#     else:
#         print("‚ö†Ô∏è No articles or Google Sheets unavailable")
    
#     end_time = datetime.now()
#     total_duration = (end_time - start_time).total_seconds()
    
#     # Final summary
#     print("\n" + "="*80)
#     print("‚úÖ COMPLETE!")
#     print("="*80)
#     print(f"   Total articles: {len(unique_articles)}")
#     print(f"   Scripts generated: 1")
#     print(f"   Saved to: {GOOGLE_SHEET_NAME}")
#     print(f"\n   ‚è±Ô∏è Total time: {total_duration:.2f} seconds")
#     print(f"   üî¢ Total tokens: {total_tokens_used:,}")
#     print(f"   üí∞ Total cost: ${total_cost:.4f}")
#     print("="*80 + "\n")


# if __name__ == "__main__":
#     asyncio.run(main())




import asyncio
import json
from datetime import datetime, date
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode
from bs4 import BeautifulSoup
from openai import OpenAI
import re
import gspread
from google.oauth2.service_account import Credentials
import os
from typing import List, Dict
import hashlib


# Initialize Perplexity client from environment variable
perplexity_client = OpenAI(
    api_key=os.environ.get("PERPLEXITY_API_KEY"),  # Read from GitHub Secrets
    base_url="https://api.perplexity.ai"
)


# Google Sheets Configuration
GOOGLE_SHEETS_CREDENTIALS_FILE = "credentials.json"  # Created by GitHub Actions
GOOGLE_SHEET_NAME = "Instagram Scripts"
GOOGLE_WORKSHEET_NAME = "Scripts"


# Categories
VALID_CATEGORIES = [
    "sports", "general", "crime", "politics", 
    "education", "economy", "entertainment", "horror"
]


# Track token usage and costs
total_tokens_used = 0
total_cost = 0.0
processed_hashes = set()  # To avoid duplicate news


def setup_google_sheets():
    """Initialize Google Sheets connection"""
    try:
        scope = [
            'https://spreadsheets.google.com/feeds',
            'https://www.googleapis.com/auth/drive'
        ]
        
        creds = Credentials.from_service_account_file(
            GOOGLE_SHEETS_CREDENTIALS_FILE, 
            scopes=scope
        )
        
        client = gspread.authorize(creds)
        
        try:
            sheet = client.open(GOOGLE_SHEET_NAME)
            print(f"‚úÖ Connected to existing sheet: '{GOOGLE_SHEET_NAME}'")
        except gspread.SpreadsheetNotFound:
            sheet = client.create(GOOGLE_SHEET_NAME)
            print(f"‚úÖ Created new sheet: '{GOOGLE_SHEET_NAME}'")
        
        try:
            worksheet = sheet.worksheet(GOOGLE_WORKSHEET_NAME)
            print(f"‚úÖ Using worksheet: '{GOOGLE_WORKSHEET_NAME}'")
        except gspread.WorksheetNotFound:
            worksheet = sheet.add_worksheet(
                title=GOOGLE_WORKSHEET_NAME,
                rows=2000,
                cols=10
            )
            # Add headers (5 columns: Timestamp, Category, Title, Script, Source Link)
            worksheet.update('A1:E1', [[
                'Timestamp',
                'Category',
                'Title',
                'Script',
                'Source Link'
            ]])
            
            # Format headers
            worksheet.format('A1:E1', {
                'textFormat': {
                    'bold': True,
                    'foregroundColor': {'red': 1.0, 'green': 1.0, 'blue': 1.0}
                },
                'backgroundColor': {'red': 0.2, 'green': 0.6, 'blue': 0.9},
                'horizontalAlignment': 'CENTER'
            })
            
            # Set column widths
            worksheet.set_column_width('A', 180)   # Timestamp
            worksheet.set_column_width('B', 150)   # Category
            worksheet.set_column_width('C', 400)   # Title
            worksheet.set_column_width('D', 600)   # Script
            worksheet.set_column_width('E', 400)   # Source Link
            
            print(f"‚úÖ Created new worksheet with headers")
        
        return worksheet
        
    except FileNotFoundError:
        print(f"‚ùå Error: '{GOOGLE_SHEETS_CREDENTIALS_FILE}' not found!")
        print("üí° This file is created automatically by GitHub Actions")
        return None
    except Exception as e:
        print(f"‚ùå Google Sheets setup error: {e}")
        import traceback
        traceback.print_exc()
        return None


def save_to_google_sheets(worksheet, category, title, script, source_link):
    """Save script to Google Sheets with category"""
    try:
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        # Clean data
        if isinstance(script, list):
            script = '\n'.join(str(item) for item in script)
        else:
            script = str(script).strip()
        
        script = script.replace('[', '').replace(']', '')
        title = str(title).strip()
        source_link = str(source_link).strip()
        category = str(category).strip().lower()
        
        # Validate category
        if category not in VALID_CATEGORIES:
            category = "general"
        
        row_data = [timestamp, category, title, script, source_link]
        
        next_row = len(worksheet.get_all_values()) + 1
        worksheet.append_row(row_data, value_input_option='RAW')
        
        # Format the row
        row_range = f'A{next_row}:E{next_row}'
        worksheet.format(row_range, {
            'textFormat': {
                'foregroundColor': {'red': 0.0, 'green': 0.0, 'blue': 0.0},
                'fontSize': 10
            },
            'backgroundColor': {'red': 1.0, 'green': 1.0, 'blue': 1.0},
            'wrapStrategy': 'WRAP',
            'verticalAlignment': 'TOP'
        })
        
        # Format category column with color coding
        category_colors = {
            'crime': {'red': 0.95, 'green': 0.8, 'blue': 0.8},
            'politics': {'red': 0.8, 'green': 0.9, 'blue': 1.0},
            'sports': {'red': 0.8, 'green': 1.0, 'blue': 0.8},
            'entertainment': {'red': 1.0, 'green': 0.9, 'blue': 0.8},
            'education': {'red': 0.9, 'green': 0.95, 'blue': 1.0},
            'economy': {'red': 0.95, 'green': 1.0, 'blue': 0.85},
            'horror': {'red': 0.7, 'green': 0.7, 'blue': 0.7},
            'general': {'red': 1.0, 'green': 1.0, 'blue': 0.9}
        }
        
        worksheet.format(f'B{next_row}', {
            'textFormat': {
                'bold': True,
                'foregroundColor': {'red': 0.0, 'green': 0.0, 'blue': 0.0},
                'fontSize': 10
            },
            'backgroundColor': category_colors.get(category, category_colors['general']),
            'horizontalAlignment': 'CENTER'
        })
        
        print(f"‚úÖ Saved [{category.upper()}] {title[:50]}...")
        return True
        
    except Exception as e:
        print(f"‚ùå Error saving to Google Sheets: {e}")
        return False


def get_content_hash(title: str, content: str) -> str:
    """Generate hash to detect duplicate news"""
    combined = f"{title.lower()}{content[:200].lower()}"
    return hashlib.md5(combined.encode()).hexdigest()


async def scrape_multiple_marathi_sources():
    """Scrape from multiple trusted Marathi news sources"""
    
    today = date.today()
    today_str = today.strftime('%Y-%m-%d')
    
    news_sites = [
        {
            "name": "TV9 Marathi",
            "url": "https://www.tv9marathi.com/latest-news",
            "article_selector": "article, div.story-card",
            "link_pattern": "tv9marathi.com",
            "target": 10
        },
        {
            "name": "ABP Majha",
            "url": "https://marathi.abplive.com/news",
            "article_selector": "article, div.story-box",
            "link_pattern": "abplive.com",
            "target": 10
        },
        {
            "name": "Lokmat",
            "url": "https://www.lokmat.com/latestnews/",
            "article_selector": "article, div.story-card",
            "link_pattern": "lokmat.com",
            "target": 10
        },
        {
            "name": "Maharashtra Times",
            "url": "https://maharashtratimes.com/",
            "article_selector": "article, div.brief-story",
            "link_pattern": "maharashtratimes.com",
            "target": 8
        },
        {
            "name": "NDTV Marathi",
            "url": "https://marathi.ndtv.com/",
            "article_selector": "article, div.news_Itm",
            "link_pattern": "marathi.ndtv.com",
            "target": 8
        },
        {
            "name": "Zee 24 Taas",
            "url": "https://zeenews.india.com/marathi/",
            "article_selector": "article, div.story",
            "link_pattern": "zeenews.india.com/marathi",
            "target": 8
        },
        {
            "name": "Loksatta",
            "url": "https://www.loksatta.com/",
            "article_selector": "article, div.item",
            "link_pattern": "loksatta.com",
            "target": 6
        }
    ]
    
    all_news = []
    
    async with AsyncWebCrawler(verbose=False) as crawler:
        
        for site in news_sites:
            print(f"\nüîç Scraping {site['name']}...")
            
            try:
                config = CrawlerRunConfig(
                    cache_mode=CacheMode.BYPASS,
                    wait_for="body",
                    word_count_threshold=10,
                    page_timeout=30000,
                    js_code="await new Promise(r => setTimeout(r, 2000));"
                )
                
                result = await crawler.arun(site['url'], config=config)
                
                if result.success:
                    soup = BeautifulSoup(result.html, 'html.parser')
                    
                    raw_articles = []
                    all_links = soup.find_all('a', href=True)
                    
                    for link_tag in all_links:
                        href = link_tag.get('href', '')
                        title = link_tag.get_text(strip=True)
                        
                        if (len(title) > 15 and len(title) < 300 and
                            site['link_pattern'] in href and
                            not any(x in href.lower() for x in [
                                'javascript:', 'mailto:', '#', 
                                '/category/', '/tag/', '/author/',
                                'facebook.com', 'twitter.com', 'instagram.com',
                                'youtube.com', 'whatsapp.com', '/myaccount/',
                                '/install_app', '/advertisement', '/epaper',
                                'web-stories', 'photo-gallery', '/videos/',
                                '/games/', '/jokes/', '/terms-and-conditions',
                                '/topic/', '/widget/'
                            ])):
                            
                            if href.startswith('/'):
                                base_url = site['url'].split('/')[0] + '//' + site['url'].split('/')[2]
                                href = base_url + href
                            
                            if href.startswith('http'):
                                raw_articles.append({
                                    'title': title,
                                    'link': href
                                })
                    
                    # Remove duplicates
                    seen_links = set()
                    unique_articles = []
                    for article in raw_articles:
                        if article['link'] not in seen_links:
                            unique_articles.append(article)
                            seen_links.add(article['link'])
                    
                    print(f"üìã Found {len(unique_articles)} unique articles")
                    
                    if len(unique_articles) > 0:
                        articles_with_content = []
                        
                        for article in unique_articles[:site['target']]:
                            try:
                                article_result = await crawler.arun(
                                    article['link'],
                                    config=CrawlerRunConfig(
                                        cache_mode=CacheMode.BYPASS,
                                        word_count_threshold=50,
                                        page_timeout=15000
                                    )
                                )
                                
                                if article_result.success and len(article_result.markdown) > 100:
                                    content_hash = get_content_hash(article['title'], article_result.markdown)
                                    
                                    if content_hash not in processed_hashes:
                                        articles_with_content.append({
                                            'title': article['title'],
                                            'link': article['link'],
                                            'content': article_result.markdown[:2500],
                                            'hash': content_hash
                                        })
                                        processed_hashes.add(content_hash)
                                        print(f"   ‚úì {article['title'][:60]}...")
                                    
                            except Exception as e:
                                continue
                        
                        print(f"‚úÖ Fetched {len(articles_with_content)} articles")
                        
                        if articles_with_content:
                            filtered_news = await smart_analyze_with_category(
                                articles_with_content, 
                                site['name']
                            )
                            all_news.extend(filtered_news)
                
                else:
                    print(f"‚ùå Failed to fetch {site['name']}")
                    
            except Exception as e:
                print(f"‚ùå Error scraping {site['name']}: {e}")
            
            await asyncio.sleep(2)
    
    return all_news


async def smart_analyze_with_category(articles: List[Dict], source_name: str):
    """AI analysis with proper categorization"""
    global total_tokens_used, total_cost
    
    print(f"\nüß† Analyzing {source_name} articles...")
    
    all_filtered = []
    
    for i in range(0, len(articles), 3):
        batch = articles[i:i+3]
        
        articles_text = ""
        for idx, article in enumerate(batch, i+1):
            articles_text += f"""
‡§¨‡§æ‡§§‡§Æ‡•Ä #{idx}:
‡§∂‡•Ä‡§∞‡•ç‡§∑‡§ï: {article['title']}
Link: {article['link']}
Content: {article['content'][:1200]}

---
"""
        
        prompt = f"""
‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä ‡§è‡§ï ‡§§‡§ú‡•ç‡§û ‡§Æ‡§∞‡§æ‡§†‡•Ä ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§ï ‡§Ü‡§π‡§æ‡§§. ‡§ñ‡§æ‡§≤‡•Ä‡§≤ ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ‡§Ç‡§ö‡•á ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£ ‡§ï‡§∞‡§æ ‡§Ü‡§£‡§ø ‡§™‡•ç‡§∞‡§§‡•ç‡§Ø‡•á‡§ï ‡§¨‡§æ‡§§‡§Æ‡•Ä‡§≤‡§æ ‡§Ø‡•ã‡§ó‡•ç‡§Ø category ‡§¶‡•ç‡§Ø‡§æ.

**Categories (‡§´‡§ï‡•ç‡§§ ‡§Ø‡§æ‡§™‡•à‡§ï‡•Ä ‡§è‡§ï ‡§®‡§ø‡§µ‡§°‡§æ):**
1. sports - ‡§ï‡•ç‡§∞‡•Ä‡§°‡§æ ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ
2. general - ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§Æ‡§π‡§§‡•ç‡§§‡•ç‡§µ‡§æ‡§ö‡•ç‡§Ø‡§æ ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ
3. crime - ‡§ó‡•Å‡§®‡•ç‡§π‡•á‡§ó‡§æ‡§∞‡•Ä ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ
4. politics - ‡§∞‡§æ‡§ú‡§ï‡•Ä‡§Ø ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ
5. education - ‡§∂‡•à‡§ï‡•ç‡§∑‡§£‡§ø‡§ï ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ
6. economy - ‡§Ü‡§∞‡•ç‡§•‡§ø‡§ï/‡§µ‡•ç‡§Ø‡§µ‡§∏‡§æ‡§Ø ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ
7. entertainment - ‡§Æ‡§®‡•ã‡§∞‡§Ç‡§ú‡§® ‡§¨‡§æ‡§§‡§Æ‡•ç‡§Ø‡§æ
8. horror - ‡§≠‡§Ø‡§æ‡§®‡§ï/‡§¶‡•Å‡§É‡§ñ‡§¶ ‡§ò‡§ü‡§®‡§æ

**JSON format:**
[
  {{
    "title": "‡§Æ‡•Ç‡§≥ ‡§∂‡•Ä‡§∞‡•ç‡§∑‡§ï",
    "category": "category name",
    "detailed_summary": "‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§ ‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂ 150-250 ‡§∂‡§¨‡•ç‡§¶‡§æ‡§Ç‡§§",
    "importance": "high/medium/low",
    "link": "URL",
    "article_number": number,
    "key_points": ["‡§Æ‡•Å‡§¶‡•ç‡§¶‡§æ 1", "‡§Æ‡•Å‡§¶‡•ç‡§¶‡§æ 2", "‡§Æ‡•Å‡§¶‡•ç‡§¶‡§æ 3"]
  }}
]

{articles_text}
"""
        
        try:
            response = perplexity_client.chat.completions.create(
                model="sonar-pro",
                messages=[
                    {
                        "role": "system",
                        "content": "You are an expert Marathi news analyst. Return ONLY valid JSON array."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.3,
                max_tokens=3000
            )
            
            if hasattr(response, 'usage'):
                batch_tokens = response.usage.total_tokens
                total_tokens_used += batch_tokens
                batch_cost = (batch_tokens / 1_000_000) * 1.0
                total_cost += batch_cost
            
            content = response.choices[0].message.content
            json_match = re.search(r'\[.*\]', content, re.DOTALL)
            
            if json_match:
                batch_articles = json.loads(json_match.group())
                all_filtered.extend(batch_articles)
                print(f"   ‚úÖ Extracted {len(batch_articles)} articles")
            
        except Exception as e:
            print(f"   ‚ùå AI analysis error: {e}")
        
        await asyncio.sleep(1)
    
    for article in all_filtered:
        article['source'] = source_name
        article['scraped_at'] = datetime.now().isoformat()
    
    return all_filtered


async def create_reel_script_single(news_article: Dict):
    """Generate Instagram Reel script for a SINGLE news article"""
    global total_tokens_used, total_cost
    
    category = news_article.get('category', 'general')
    
    system_prompt = """
‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä "‡§ú‡§¨‡§∞‡•Ä ‡§ñ‡§¨‡§∞‡•Ä" Instagram Reels ‡§ö‡•á ‡§è‡§ï‡•ç‡§∏‡§™‡§∞‡•ç‡§ü ‡§∏‡•ç‡§ï‡•ç‡§∞‡§ø‡§™‡•ç‡§ü ‡§∞‡§æ‡§Ø‡§ü‡§∞ ‡§Ü‡§π‡§æ‡§§.

**HOOK VARIETY (‡§™‡§π‡§ø‡§≤‡•ç‡§Ø‡§æ 2 ‡§ì‡§≥‡•Ä):**
1. Shock Statement: "‡§è‡§ï‡§æ ‡§Ö‡§™‡§ò‡§æ‡§§‡§æ‡§®‡•á ‡§∏‡§Ç‡§™‡•Ç‡§∞‡•ç‡§£ ‡§∞‡§æ‡§ú‡•ç‡§Ø ‡§π‡§æ‡§¶‡§∞‡§≤‡§Ç."
2. Direct Question: "‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§≤‡§æ ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä ‡§Ü‡§π‡•á ‡§ï‡§æ...?"
3. Breaking News: "‡§®‡•Å‡§ï‡§§‡•Ä‡§ö ‡§è‡§ï ‡§ß‡§ï‡•ç‡§ï‡§æ‡§¶‡§æ‡§Ø‡§ï ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä ‡§∏‡§Æ‡•ã‡§∞ ‡§Ü‡§≤‡•Ä."
4. Name Drop: "[‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡•Ä ‡§®‡§æ‡§µ] ‡§Ü‡§ú ‡§ö‡§∞‡•ç‡§ö‡•á‡§§ ‡§ï‡§æ ‡§Ü‡§π‡•á?"
5. Contrast/Twist: "‡§¶‡§ø‡§∏‡§§‡§Ç ‡§ï‡§æ‡§π‡•Ä‡§§‡§∞‡•Ä, ‡§™‡§£ ‡§µ‡§æ‡§∏‡•ç‡§§‡§µ ‡§µ‡•á‡§ó‡§≥‡§Ç‡§ö."

**‡§∏‡•ç‡§ü‡•ç‡§∞‡§ï‡•ç‡§ö‡§∞ (15-18 ‡§ì‡§≥‡•Ä):**
- ‡§™‡§π‡§ø‡§≤‡•á 2 ‡§ì‡§≥‡•Ä: Hook
- 3-10 ‡§ì‡§≥‡•Ä: ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§ò‡§ü‡§®‡§æ
- 11-14 ‡§ì‡§≥‡•Ä: ‡§ü‡•ç‡§µ‡§ø‡§∏‡•ç‡§ü/‡§™‡•ç‡§∞‡§∂‡•ç‡§®
- ‡§∂‡•á‡§µ‡§ü‡§ö‡•ç‡§Ø‡§æ 2-3 ‡§ì‡§≥‡•Ä: CTA

**ENDING:**
"‡§§‡•Å‡§Æ‡§ö‡§Ç ‡§ï‡§æ‡§Ø ‡§Æ‡§§ ‡§Ü‡§π‡•á? ‡§ï‡§Æ‡•á‡§Ç‡§ü ‡§ï‡§∞‡•Ç‡§® ‡§∏‡§æ‡§Ç‡§ó‡§æ ‡§Ü‡§£‡§ø ‡§´‡•â‡§≤‡•ã ‡§ï‡§∞‡§æ ‡§ú‡§¨‡§∞‡•Ä ‡§ñ‡§¨‡§∞‡•Ä."

OUTPUT: ‡§´‡§ï‡•ç‡§§ ‡§∏‡•ç‡§ï‡•ç‡§∞‡§ø‡§™‡•ç‡§ü, 15-18 ‡§ì‡§≥‡•Ä
"""
    
    user_prompt = f"""
‡§ñ‡§æ‡§≤‡•Ä‡§≤ ‡§¨‡§æ‡§§‡§Æ‡•Ä‡§µ‡§∞ Instagram Reel script ‡§§‡§Ø‡§æ‡§∞ ‡§ï‡§∞‡§æ.

**‡§¨‡§æ‡§§‡§Æ‡•Ä:**
‡§∂‡•Ä‡§∞‡•ç‡§∑‡§ï: {news_article['title']}
Category: {category}
‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂: {news_article['detailed_summary']}
‡§Æ‡•Å‡§¶‡•ç‡§¶‡•á: {', '.join(news_article.get('key_points', []))}

‡§´‡§ï‡•ç‡§§ ‡§∏‡•ç‡§ï‡•ç‡§∞‡§ø‡§™‡•ç‡§ü ‡§¶‡•ç‡§Ø‡§æ, 15-18 ‡§ì‡§≥‡•Ä.
"""
    
    try:
        response = perplexity_client.chat.completions.create(
            model="sonar-pro",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.8,
            max_tokens=1500
        )
        
        if hasattr(response, 'usage'):
            script_tokens = response.usage.total_tokens
            total_tokens_used += script_tokens
            total_cost += (script_tokens / 1_000_000) * 1.0
        
        script = response.choices[0].message.content.strip()
        script = script.replace('```', '').strip()
        
        return script
        
    except Exception as e:
        print(f"‚ùå Error generating script: {e}")
        return None


async def main():
    global total_tokens_used, total_cost
    
    print("="*80)
    print("üöÄ SMART MARATHI NEWS SCRAPER + SCRIPT GENERATOR v2.0")
    print("="*80)
    print("üìç Target: 50+ different news articles")
    print("üìã Categories: Sports, General, Crime, Politics, Education, Economy, Entertainment, Horror")
    print("üé¨ Output: Individual scripts for each news")
    print("üíæ Storage: Google Sheets with category column")
    print("="*80 + "\n")
    
    start_time = datetime.now()
    
    # STEP 1: SCRAPING
    print("\n" + "="*80)
    print("STEP 1: SCRAPING NEWS FROM MULTIPLE SOURCES")
    print("="*80 + "\n")
    
    all_articles = await scrape_multiple_marathi_sources()
    
    # Remove duplicates
    unique_articles = []
    seen_hashes = set()
    
    for article in all_articles:
        article_hash = article.get('hash', get_content_hash(article['title'], article.get('detailed_summary', '')))
        if article_hash not in seen_hashes:
            unique_articles.append(article)
            seen_hashes.add(article_hash)
    
    print(f"\n‚úÖ Total unique articles: {len(unique_articles)}")
    
    # Category breakdown
    category_counts = {}
    for article in unique_articles:
        cat = article.get('category', 'general')
        category_counts[cat] = category_counts.get(cat, 0) + 1
    
    print("\nüìä Category Breakdown:")
    for cat, count in sorted(category_counts.items()):
        print(f"   {cat.upper()}: {count}")
    
    # Select top 50 articles
    priority_order = {'high': 1, 'medium': 2, 'low': 3}
    unique_articles.sort(key=lambda x: priority_order.get(x.get('importance', 'medium'), 2))
    
    selected_articles = unique_articles[:50]
    
    print(f"\nüéØ Selected {len(selected_articles)} articles for scripts")
    
    end_scrape = datetime.now()
    scrape_duration = (end_scrape - start_time).total_seconds()
    print(f"‚è±Ô∏è  Scraping: {scrape_duration:.2f} seconds\n")
    
    # STEP 2: SCRIPT GENERATION
    print("="*80)
    print("STEP 2: GENERATING SCRIPTS & SAVING TO GOOGLE SHEETS")
    print("="*80 + "\n")
    
    worksheet = setup_google_sheets()
    
    if worksheet and len(selected_articles) > 0:
        successful_saves = 0
        failed_saves = 0
        
        for idx, article in enumerate(selected_articles, 1):
            print(f"\n[{idx}/{len(selected_articles)}] {article['title'][:60]}...")
            
            script = await create_reel_script_single(article)
            
            if script:
                success = save_to_google_sheets(
                    worksheet,
                    article.get('category', 'general'),
                    article['title'],
                    script,
                    article['link']
                )
                
                if success:
                    successful_saves += 1
                else:
                    failed_saves += 1
            else:
                failed_saves += 1
            
            await asyncio.sleep(1.5)
        
        print("\n" + "="*80)
        print("‚úÖ COMPLETE!")
        print("="*80)
        print(f"   Successfully saved: {successful_saves}")
        print(f"   Failed: {failed_saves}")
        print(f"   üìä View: https://docs.google.com/spreadsheets/d/{worksheet.spreadsheet.id}")
    else:
        print("‚ö†Ô∏è No articles or Google Sheets unavailable")
    
    end_time = datetime.now()
    total_duration = (end_time - start_time).total_seconds()
    
    print("\n" + "="*80)
    print("üìà SUMMARY")
    print("="*80)
    print(f"   Articles scraped: {len(unique_articles)}")
    print(f"   Scripts generated: {successful_saves}")
    print(f"   Time: {total_duration:.2f} seconds")
    print(f"   Tokens: {total_tokens_used:,}")
    print(f"   Cost: ${total_cost:.4f}")
    print("="*80 + "\n")


if __name__ == "__main__":
    asyncio.run(main())
