import asyncio
import json
from datetime import datetime
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode
from bs4 import BeautifulSoup
from openai import OpenAI
import re
import os

# Initialize Perplexity client with environment variable
perplexity_client = OpenAI(
    api_key=os.environ.get("PERPLEXITY_API_KEY"),  # Read from GitHub Secrets
    base_url="https://api.perplexity.ai"
)

# Track token usage (CORRECTED: $1 per 1M tokens)
total_tokens_used = 0
total_cost = 0.0

async def scrape_marathi_news_final():
    """
    Scraper that collects articles from all three sites
    """
    
    news_sites = [
        {
            "name": "TV9 Marathi",
            "url": "https://www.tv9marathi.com/latest-news",
            "article_selector": "article, div.story-card, div.news-item",
            "link_pattern": "tv9marathi.com"
        },
        {
            "name": "ABP Majha",
            "url": "https://marathi.abplive.com/news",
            "article_selector": "article, div.story-box, div.news-card",
            "link_pattern": "abplive.com"
        },
        {
            "name": "Lokmat",
            "url": "https://www.lokmat.com/latestnews/",
            "article_selector": "article, div.story-card, div.card-body",
            "link_pattern": "lokmat.com"
        }
    ]
    
    all_news = []
    
    async with AsyncWebCrawler(verbose=True) as crawler:
        
        for site in news_sites:
            print(f"\nЁЯФН Scraping {site['name']}...")
            
            try:
                # Step 1: Fetch homepage with JavaScript rendering
                config = CrawlerRunConfig(
                    cache_mode=CacheMode.BYPASS,
                    wait_for="body",
                    word_count_threshold=10,
                    page_timeout=30000,
                    js_code="""
                    // Wait for content to load
                    await new Promise(r => setTimeout(r, 2000));
                    """
                )
                
                result = await crawler.arun(site['url'], config=config)
                
                if result.success:
                    soup = BeautifulSoup(result.html, 'html.parser')
                    
                    raw_articles = []
                    
                    # Strategy: Find all links with Marathi text
                    all_links = soup.find_all('a', href=True)
                    
                    for link_tag in all_links:
                        href = link_tag.get('href', '')
                        title = link_tag.get_text(strip=True)
                        
                        # Filter valid news links
                        if (len(title) > 15 and len(title) < 300 and
                            site['link_pattern'] in href and
                            not any(x in href for x in [
                                'javascript:', 'mailto:', '#', 
                                '/category/', '/tag/', '/author/',
                                'facebook.com', 'twitter.com', 'instagram.com',
                                'youtube.com', 'whatsapp.com', '/myaccount/',
                                '/install_app', '/advertisement', '/epaper',
                                'web-stories', 'photo-gallery', '/videos/',
                                '/sakhi/', '/astro/', '/bhakti/', '/games/',
                                '/jokes/', '/terms-and-conditions', '/utility-news',
                                '/spiritual-adhyatmik', '/rashi-bhavishya', 
                                '/topic/', '/elections/', '/career/'
                            ])):
                            
                            # Make absolute URL
                            if href.startswith('/'):
                                base_url = site['url'].split('/')[0] + '//' + site['url'].split('/')[2]
                                href = base_url + href
                            
                            if href.startswith('http'):
                                raw_articles.append({
                                    'title': title,
                                    'link': href
                                })
                    
                    # Remove duplicates by link
                    seen_links = set()
                    unique_articles = []
                    for article in raw_articles:
                        if article['link'] not in seen_links:
                            unique_articles.append(article)
                            seen_links.add(article['link'])
                    
                    print(f"ЁЯУЛ Found {len(unique_articles)} unique articles from {site['name']}")
                    
                    if len(unique_articles) > 0:
                        # Get top 12 articles per site
                        print(f"ЁЯУД Fetching detailed content from top {min(12, len(unique_articles))} articles...")
                        
                        articles_with_content = []
                        for article in unique_articles[:12]:  # Top 12 per site
                            try:
                                article_result = await crawler.arun(
                                    article['link'],
                                    config=CrawlerRunConfig(
                                        cache_mode=CacheMode.BYPASS,
                                        word_count_threshold=50,
                                        page_timeout=15000
                                    )
                                )
                                
                                if article_result.success and len(article_result.markdown) > 100:
                                    articles_with_content.append({
                                        'title': article['title'],
                                        'link': article['link'],
                                        'content': article_result.markdown[:2500]
                                    })
                                    print(f"   тЬУ {article['title'][:60]}...")
                                    
                            except Exception as e:
                                continue
                        
                        print(f"тЬЕ Fetched content for {len(articles_with_content)} articles")
                        
                        # Step 3: AI analysis
                        if articles_with_content:
                            filtered_news = await smart_analyze_with_detailed_summary(
                                articles_with_content, 
                                site['name']
                            )
                            all_news.extend(filtered_news)
                            print(f"тЬЕ Extracted {len(filtered_news)} important articles with detailed summaries")
                    else:
                        print(f"тЪая╕П No articles found from {site['name']}")
                
                else:
                    print(f"тЭМ Failed to fetch {site['name']}: {result.error_message}")
                    
            except Exception as e:
                print(f"тЭМ Error scraping {site['name']}: {e}")
                import traceback
                traceback.print_exc()
    
    return all_news


async def smart_analyze_with_detailed_summary(articles, source_name):
    """
    AI analysis with CORRECTED token tracking ($1 per 1M tokens)
    """
    global total_tokens_used, total_cost
    
    print(f"\nЁЯза Using AI for detailed analysis of {source_name} articles...")
    
    all_filtered = []
    
    # Process in batches of 5
    for i in range(0, len(articles), 5):
        batch = articles[i:i+5]
        
        articles_text = ""
        for idx, article in enumerate(batch, i+1):
            articles_text += f"""
рдмрд╛рддрдореА #{idx}:
рд╢реАрд░реНрд╖рдХ: {article['title']}
Link: {article['link']}
Content: {article['content'][:1200]}

---
"""
        
        prompt = f"""
рддреБрдореНрд╣реА рдПрдХ рддрдЬреНрдЮ рдорд░рд╛рдареА рдмрд╛рддрдореНрдпрд╛ рд╡рд┐рд╢реНрд▓реЗрд╖рдХ рдЖрд╣рд╛рдд. рдЦрд╛рд▓реАрд▓ рдмрд╛рддрдореНрдпрд╛рдВрдЪреЗ рд╡рд┐рд╢реНрд▓реЗрд╖рдг рдХрд░рд╛.

**рдлрдХреНрдд рд╣реЗ рдкреНрд░рдХрд╛рд░ рдирд┐рд╡рдбрд╛:**
1. рдЧреБрдиреНрд╣реЗрдЧрд╛рд░реА рдмрд╛рддрдореНрдпрд╛ (Crime) - рд╣рддреНрдпрд╛, рджрд░реЛрдбрд╛, рдЕрдкрдШрд╛рдд, рдЕрдЯрдХ, рд▓рд╛рдЪ
2. рд░рд╛рдЬрдХреАрдп рдмрд╛рддрдореНрдпрд╛ (Political) - рдирд┐рд╡рдбрдгреБрдХрд╛, рд╕рд░рдХрд╛рд░, рдорд╣рд╛рдкрд╛рд▓рд┐рдХрд╛, рд░рд╛рдЬрдХреАрдп рдШрдбрд╛рдореЛрдбреА
3. рдорд╣рддреНрддреНрд╡рд╛рдЪреНрдпрд╛ рд╕рд╛рдорд╛рдиреНрдп рдмрд╛рддрдореНрдпрд╛ (Important General) - рд╢рд╛рд╕рдХреАрдп рдирд┐рд░реНрдгрдп, рд╕рд╛рдорд╛рдЬрд┐рдХ рдореБрджреНрджреЗ

**рдЯрд╛рд│рд╛рд╡реЗ:** рдордиреЛрд░рдВрдЬрди gossip, рдЬреНрдпреЛрддрд┐рд╖, рдлреЕрд╢рди, lifestyle, рдЦреЗрд│рд╛рдЪреА рд╕рд╛рдорд╛рдиреНрдп рдмрд╛рддрдореА, job posts, рдзрд╛рд░реНрдорд┐рдХ рдХрдерд╛, Bigg Boss, рдмреЙрд▓реАрд╡реВрдб gossip

**JSON format (рдлрдХреНрдд array рдкрд░рдд рдХрд░рд╛, рдЗрддрд░ рдХрд╛рд╣реА рдирд╛рд╣реА):**
[
  {{
    "title": "рдореВрд│ рд╢реАрд░реНрд╖рдХ",
    "category": "crime/politics/general",
    "detailed_summary": "рд╕рдВрдкреВрд░реНрдг рд╡рд┐рд╕реНрддреГрдд рд╕рд╛рд░рд╛рдВрд╢ 150-250 рд╢рдмреНрджрд╛рдВрдд рдорд░рд╛рдареАрдд. рдХрд╛рдп рдШрдбрд▓рдВ? рдХреБрдареЗ? рдХрдзреА? рдХреЛрдг рдЖрд╣реЗрдд? рдХреЛрдгрддреА рдХрд╛рд░рд╡рд╛рдИ? рдХрд╛рдп рдкрд░рд┐рдгрд╛рдо? рдЗрддрд░ рддрдкрд╢реАрд▓ рд╕рдорд╛рд╡рд┐рд╖реНрдЯ рдХрд░рд╛",
    "importance": "high/medium/low",
    "link": "URL",
    "article_number": number,
    "key_points": ["рдореБрджреНрджрд╛ 1", "рдореБрджреНрджрд╛ 2", "рдореБрджреНрджрд╛ 3"]
  }}
]

{articles_text}
"""
        
        try:
            response = perplexity_client.chat.completions.create(
                model="sonar-pro",
                messages=[
                    {
                        "role": "system",
                        "content": "You are an expert Marathi news analyst. Return ONLY valid JSON array. No markdown, no explanation, no extra text."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.3,
                max_tokens=4000
            )
            
            # Track tokens (CORRECTED: $1 per 1M tokens)
            if hasattr(response, 'usage'):
                batch_tokens = response.usage.total_tokens
                total_tokens_used += batch_tokens
                
                # CORRECT pricing: $1 per 1M tokens
                batch_cost = (batch_tokens / 1_000_000) * 1.0
                total_cost += batch_cost
                
                print(f"   ЁЯУК Batch tokens: {batch_tokens:,} | Cost: ${batch_cost:.4f}")
            
            content = response.choices[0].message.content
            
            # Extract JSON
            json_match = re.search(r'\[.*\]', content, re.DOTALL)
            
            if json_match:
                batch_articles = json.loads(json_match.group())
                all_filtered.extend(batch_articles)
                print(f"   тЬЕ Extracted {len(batch_articles)} articles from this batch")
            else:
                print(f"   тЪая╕П No valid JSON in AI response")
            
        except json.JSONDecodeError as e:
            print(f"   тЭМ JSON parsing error: {e}")
        except Exception as e:
            print(f"   тЭМ AI analysis error: {e}")
    
    # Add source and timestamp
    for article in all_filtered:
        article['source'] = source_name
        article['scraped_at'] = datetime.now().isoformat()
    
    return all_filtered


async def main():
    global total_tokens_used, total_cost
    
    print("ЁЯЪА Starting Smart Marathi News Scraper with DETAILED Summaries")
    print("ЁЯУН Focus: Criminal, Political & Important General News")
    print("ЁЯУЭ Feature: Detailed 150-250 word summaries")
    print("ЁЯТ░ Token tracking enabled (Correct pricing: $1/1M tokens)")
    print("ЁЯОп Strategy: Get top 10 news from ALL THREE SITES COMBINED\n")
    
    start_time = datetime.now()
    
    # Scrape all sites
    all_articles = await scrape_marathi_news_final()
    
    # Remove duplicates by title
    unique_articles = []
    seen_titles = set()
    
    for article in all_articles:
        title_lower = article['title'].lower()
        if title_lower not in seen_titles:
            unique_articles.append(article)
            seen_titles.add(title_lower)
    
    # Sort ALL articles by importance FIRST
    priority_order = {'high': 1, 'medium': 2, 'low': 3}
    unique_articles.sort(key=lambda x: priority_order.get(x.get('importance', 'medium'), 2))
    
    # Save ALL articles to JSON (fixed filename)
    output_file = "latest_news.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(unique_articles, f, ensure_ascii=False, indent=2)
    
    # Save TOP 10 to separate file (fixed filename)
    top_10_articles = unique_articles[:10]
    top_10_file = "top_10_latest.json"
    with open(top_10_file, 'w', encoding='utf-8') as f:
        json.dump(top_10_articles, f, ensure_ascii=False, indent=2)
    
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()
    
    # Final summary
    print("\n" + "="*80)
    print("ЁЯУК SCRAPING SUMMARY")
    print("="*80)
    print(f"   Total articles scraped: {len(unique_articles)}")
    print(f"   High importance: {len([a for a in unique_articles if a.get('importance') == 'high'])}")
    print(f"   Crime news: {len([a for a in unique_articles if a.get('category') == 'crime'])}")
    print(f"   Political news: {len([a for a in unique_articles if a.get('category') == 'politics'])}")
    print(f"   General news: {len([a for a in unique_articles if a.get('category') == 'general'])}")
    print(f"\n   By source:")
    for source in ['TV9 Marathi', 'ABP Majha', 'Lokmat']:
        count = len([a for a in unique_articles if a['source'] == source])
        count_top10 = len([a for a in top_10_articles if a['source'] == source])
        print(f"      тАв {source}: {count} total articles | {count_top10} in TOP 10")
    print(f"\nЁЯТ╛ All articles saved to: {output_file}")
    print(f"ЁЯПЖ TOP 10 articles saved to: {top_10_file}")
    print(f"\nтП▒я╕П  Total time: {duration:.2f} seconds")
    print(f"ЁЯФв Total tokens used: {total_tokens_used:,}")
    print(f"ЁЯТ░ Estimated cost: ${total_cost:.4f} (@ $1.00 per 1M tokens)")
    if len(unique_articles) > 0:
        print(f"ЁЯУИ Average tokens per article: {total_tokens_used // len(unique_articles):,}")
    print("="*80 + "\n")
    
    # Display TOP 10 from ALL sites combined
    if len(top_10_articles) > 0:
        print("ЁЯПЖ TOP 10 IMPORTANT NEWS FROM ALL THREE SITES COMBINED")
        print("="*80 + "\n")
        
        for i, article in enumerate(top_10_articles, 1):
            importance_emoji = "ЁЯФе" if article.get('importance') == 'high' else "ЁЯУМ"
            category_emoji = {
                'crime': 'ЁЯЪи',
                'politics': 'ЁЯПЫя╕П',
                'general': 'ЁЯУ░'
            }.get(article.get('category', 'general'), 'ЁЯУ░')
            
            print(f"{i}. {importance_emoji} {category_emoji} [{article['source']}]")
            print(f"\n   ЁЯУЛ рд╢реАрд░реНрд╖рдХ: {article['title']}")
            print(f"\n   ЁЯУЭ рд╡рд┐рд╕реНрддреГрдд рд╕рд╛рд░рд╛рдВрд╢:")
            print(f"   {article.get('detailed_summary', 'N/A')}")
            
            if article.get('key_points'):
                print(f"\n   ЁЯФС рдореБрдЦреНрдп рдореБрджреНрджреЗ:")
                for point in article['key_points']:
                    print(f"      тАв {point}")
            
            print(f"\n   ЁЯФЧ {article['link']}")
            print(f"   тЪб рдорд╣рддреНрддреНрд╡: {article.get('importance', 'N/A').upper()}")
            print("\n" + "-"*80 + "\n")
    
    print("тЬЕ Smart scraping complete! Top 10 news from all sites extracted.\n")


if __name__ == "__main__":
    asyncio.run(main())
